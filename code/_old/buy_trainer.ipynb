{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd9d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "\n",
    "# from agents.buy_agent_trainer import BuyAgentTrainer\n",
    "# from agents.multi_process.multi_process_trainer import MultiProcessTrainer\n",
    "# from agents.multi_process.handler import EnvHandler, AgentHandler\n",
    "\n",
    "# def train_buy_agent_mp():\n",
    "#     # 1) Build trainer with your usual settings\n",
    "#     buy_trainer = BuyAgentTrainer(\n",
    "#         ticker=\"AAPL\",\n",
    "#         window_size=30,\n",
    "#         horizon=20,\n",
    "#         transaction_cost=0.001,\n",
    "#         lambda_dd=0.05,\n",
    "#         lambda_vol=0.01,\n",
    "#         hold_penalty_long=0.0,\n",
    "#         device=\"cpu\",\n",
    "#     )\n",
    "\n",
    "#     # 2) Ensure dataset + env + agent are built\n",
    "#     if buy_trainer.env is None or buy_trainer.agent is None:\n",
    "#         buy_trainer._build_dataset_and_env()\n",
    "\n",
    "#     print(f\"[BuyTrainer-MP] state_df shape: {buy_trainer.state_df.shape}\")\n",
    "#     print(f\"[BuyTrainer-MP] prices shape: {buy_trainer.prices.shape}\")\n",
    "\n",
    "#     # 3) Create EnvFactory and AgentFactory for MP\n",
    "#     EnvClass = buy_trainer.env.__class__\n",
    "\n",
    "#     env_fn = EnvHandler(\n",
    "#         state_df=buy_trainer.state_df,\n",
    "#         prices=buy_trainer.prices,\n",
    "#         EnvClass=EnvClass,\n",
    "#         horizon=buy_trainer.horizon,\n",
    "#         transaction_cost=buy_trainer.transaction_cost,\n",
    "#         # NOTE: Sell-specific kwargs are ignored for BuyEnv by EnvFactory\n",
    "#     )\n",
    "\n",
    "#     agent_fn = AgentHandler(buy_trainer.agent)\n",
    "\n",
    "#     # 4) Multi-process trainer\n",
    "#     mp_trainer = MultiProcessTrainer(\n",
    "#         agent=buy_trainer.agent,\n",
    "#         env_fn=env_fn,\n",
    "#         agent_fn=agent_fn,\n",
    "#         n_workers=4,          # you can go higher if CPU allows\n",
    "#         steps_per_batch=300,  # per worker per batch\n",
    "#     )\n",
    "\n",
    "#     print(\"[BuyTrainer-MP] Starting MP training...\")\n",
    "\n",
    "#     # Recommended: 600â€“800 batches for a decent run\n",
    "#     mp_trainer.train(\n",
    "#         n_batches=600,\n",
    "#         updates_per_batch=50,\n",
    "#     )\n",
    "\n",
    "#     print(\n",
    "#         f\"[BuyTrainer-MP] Done. \"\n",
    "#         f\"Replay buffer size={len(buy_trainer.agent.replay_buffer)} | \"\n",
    "#         f\"Loss entries={len(buy_trainer.agent.loss_history)}\"\n",
    "#     )\n",
    "\n",
    "#     # 5) Greedy evaluation on full BuyEnv\n",
    "#     reward, steps = evaluate_greedy_buy(buy_trainer)\n",
    "#     print(\"\\n[BuyTrainer-MP] Greedy evaluation:\")\n",
    "#     print(f\"Reward: {reward:.4f} | Steps: {steps}\")\n",
    "\n",
    "#     # 6) Inspect BUY confidence distribution\n",
    "#     avg_buy, max_buy, min_buy = inspect_buy_confidence(buy_trainer)\n",
    "#     print(\"\\n[BuyTrainer-MP] BUY confidence stats:\")\n",
    "#     print(f\"Average BUY confidence: {avg_buy:.6f}\")\n",
    "#     print(f\"Max BUY confidence:     {max_buy:.6f}\")\n",
    "#     print(f\"Min BUY confidence:     {min_buy:.6f}\")\n",
    "\n",
    "#     # 7) Plot loss curve\n",
    "#     plot_loss_history(buy_trainer.agent)\n",
    "\n",
    "#     # Optionally: return trainer to reuse in TradeManager\n",
    "#     return buy_trainer\n",
    "\n",
    "\n",
    "# def evaluate_greedy_buy(buy_trainer):\n",
    "#     \"\"\"\n",
    "#     Simple greedy rollout using the existing BuyEnv.\n",
    "#     \"\"\"\n",
    "#     env = buy_trainer.env\n",
    "#     agent = buy_trainer.agent\n",
    "#     assert env is not None and agent is not None\n",
    "\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0.0\n",
    "#     steps = 0\n",
    "\n",
    "#     while not done:\n",
    "#         action = agent.select_action(state, greedy=True)\n",
    "#         next_state, reward, done, info = env.step(action)\n",
    "#         total_reward += reward\n",
    "#         state = next_state\n",
    "#         steps += 1\n",
    "\n",
    "#     return total_reward, steps\n",
    "\n",
    "\n",
    "# def inspect_buy_confidence(buy_trainer):\n",
    "#     \"\"\"\n",
    "#     Run the trained BuyAgent over the entire state_df and\n",
    "#     compute BUY probabilities (softmax over Q-values).\n",
    "#     \"\"\"\n",
    "#     agent = buy_trainer.agent\n",
    "#     state_df = buy_trainer.state_df\n",
    "#     assert agent is not None and state_df is not None\n",
    "\n",
    "#     confs = []\n",
    "#     for i in range(len(state_df)):\n",
    "#         state = state_df.iloc[i].values.astype(np.float32)\n",
    "#         with torch.no_grad():\n",
    "#             s = torch.from_numpy(state).unsqueeze(0).to(agent.device)\n",
    "#             q = agent.q_net(s)[0].cpu().numpy()\n",
    "#             exps = np.exp(q - np.max(q))\n",
    "#             probs = exps / np.sum(exps)\n",
    "#             confs.append(probs[1])  # index 1 = BUY\n",
    "\n",
    "#     confs = np.array(confs)\n",
    "#     return float(confs.mean()), float(confs.max()), float(confs.min())\n",
    "\n",
    "\n",
    "# def plot_loss_history(agent):\n",
    "#     \"\"\"\n",
    "#     Plot the DDQN loss history.\n",
    "#     \"\"\"\n",
    "#     if not agent.loss_history:\n",
    "#         print(\"[BuyTrainer-MP] No loss history to plot.\")\n",
    "#         return\n",
    "\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.plot(agent.loss_history)\n",
    "#     plt.xlabel(\"Update step\")\n",
    "#     plt.ylabel(\"Loss\")\n",
    "#     plt.title(\"BuyAgent DDQN Loss (MP training)\")\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     buy_trainer = train_buy_agent_mp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from agents.buy_agent_trainer import BuyAgentTrainer\n",
    "\n",
    "# buy_trainer = BuyAgentTrainer(\n",
    "#     ticker=\"AAPL\",\n",
    "#     window_size=30,\n",
    "#     horizon=20,\n",
    "#     transaction_cost=0.001,\n",
    "#     lambda_dd=0.05,\n",
    "#     lambda_vol=0.01,\n",
    "#     hold_penalty_long=0.0,\n",
    "#     device=\"cpu\",\n",
    "# )\n",
    "\n",
    "# buy_trend_history = buy_trainer.train_trend_filtered(\n",
    "#     n_episodes=200,\n",
    "#     verbose=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22583b1b",
   "metadata": {},
   "source": [
    "# New test after refacoring GA - 16 December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2390f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BuyTrainer] Raw dataset: (1224, 10)\n",
      "[BuyTrainer] After dropna: (1224, 10)\n",
      "[BuyTrainer] Rolling state_df shape: (1194, 270)\n",
      "[BuyTrainer] state_dim=270, actions=2\n",
      "[BuyTrainer] Warmup set to: 238\n",
      "[Buy Ep 1/20] Reward=1.1072 | Eps=0.773 | Steps=1193 | Buffer=1193 | Avg10=1.1072\n",
      "[Buy Ep 10/20] Reward=0.4692 | Eps=0.050 | Steps=1193 | Buffer=11930 | Avg10=0.8626\n",
      "[Buy Ep 20/20] Reward=0.5786 | Eps=0.050 | Steps=1193 | Buffer=23860 | Avg10=1.0089\n",
      "\n",
      "Training complete.\n",
      "Final reward: 0.5786427583559783\n",
      "Replay buffer size: 23860\n"
     ]
    }
   ],
   "source": [
    "from config.loader import load_config\n",
    "from envs.buy_env import BuyEnv\n",
    "from agents.buy_agent_trainer import BuyAgentTrainer\n",
    "\n",
    "from dataclasses import replace\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load config\n",
    "# --------------------------------------------------\n",
    "config = load_config(\"config/config.yaml\")\n",
    "\n",
    "# override ONLY what you need for this experiment\n",
    "trade_cfg_override = replace(\n",
    "    config.trade_manager,\n",
    "    use_trend_filter=False,\n",
    "    buy_min_confidence=0.0,\n",
    ")\n",
    "\n",
    "config = replace(\n",
    "    config,\n",
    "    trade_manager=trade_cfg_override,\n",
    ")\n",
    "\n",
    "# # Disable advanced filters for test\n",
    "# config.trade_manager.use_trend_filter = False\n",
    "# config.trade_manager.buy_min_confidence = 0.0\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Train Buy Agent (single process)\n",
    "# --------------------------------------------------\n",
    "trainer = BuyAgentTrainer(\n",
    "    ticker=\"AAPL\",\n",
    "    config=config,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "history = trainer.train(\n",
    "    n_episodes=20,\n",
    "    warmup_dynamic=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(\"Final reward:\", history[\"episode_rewards\"][-1])\n",
    "print(\"Replay buffer size:\", len(trainer.agent.replay_buffer))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
