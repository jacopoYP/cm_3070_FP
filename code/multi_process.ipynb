{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2236019b",
   "metadata": {},
   "source": [
    "# Multi Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd553bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from config.loader import load_config\n",
    "from agents.buy_agent_trainer import BuyAgentTrainer\n",
    "from agents.multi_process.multi_process_trainer import MultiProcessTrainer\n",
    "from agents.multi_process.handler import EnvHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf74121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_buy_agent_mp():\n",
    "    config = load_config(\"config/config.yaml\")\n",
    "\n",
    "    buy_trainer = BuyAgentTrainer(\n",
    "        ticker=\"AAPL\",\n",
    "        config=config,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "\n",
    "    assert buy_trainer.env is not None\n",
    "    assert buy_trainer.agent is not None\n",
    "    assert buy_trainer.state_df is not None\n",
    "    assert buy_trainer.prices is not None\n",
    "\n",
    "    print(f\"[BuyTrainer-MP] state_df shape: {buy_trainer.state_df.shape}\")\n",
    "    print(f\"[BuyTrainer-MP] prices shape: {buy_trainer.prices.shape}\")\n",
    "\n",
    "    env_fn = EnvHandler(\n",
    "        env_type=\"buy\",\n",
    "        features=buy_trainer.state_df.values.astype(np.float32),\n",
    "        prices=np.asarray(buy_trainer.prices, dtype=np.float32),\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # âœ… NO AgentHandler, NO agent_fn\n",
    "    mp_trainer = MultiProcessTrainer(\n",
    "        agent=buy_trainer.agent,\n",
    "        env_fn=env_fn,\n",
    "        n_workers=4,\n",
    "        steps_per_batch=300,\n",
    "        worker_epsilon=0.05,\n",
    "        sync_every=10,\n",
    "        log_every=10,\n",
    "    )\n",
    "\n",
    "    print(\"[BuyTrainer-MP] Starting MP training...\")\n",
    "    mp_trainer.train(n_batches=600, updates_per_batch=50, log_every=10)\n",
    "\n",
    "    return buy_trainer\n",
    "\n",
    "\n",
    "def evaluate_greedy_buy(buy_trainer):\n",
    "    env = buy_trainer.env\n",
    "    agent = buy_trainer.agent\n",
    "    assert env is not None and agent is not None\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state, greedy=True)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += float(reward)\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "    return total_reward, steps\n",
    "\n",
    "\n",
    "def inspect_buy_confidence(buy_trainer):\n",
    "    agent = buy_trainer.agent\n",
    "    state_df = buy_trainer.state_df\n",
    "    env = buy_trainer.env\n",
    "\n",
    "    buy_index = env.BUY\n",
    "\n",
    "    confs = []\n",
    "    agent.q_net.eval()\n",
    "\n",
    "    for i in range(len(state_df)):\n",
    "        state = state_df.iloc[i].values.astype(np.float32)\n",
    "        with torch.no_grad():\n",
    "            s = torch.from_numpy(state).unsqueeze(0).to(agent.device)\n",
    "            q = agent.q_net(s)[0].detach().cpu().numpy()\n",
    "            exps = np.exp(q - np.max(q))\n",
    "            probs = exps / (np.sum(exps) + 1e-12)\n",
    "            confs.append(float(probs[buy_index]))\n",
    "\n",
    "    confs = np.asarray(confs, dtype=np.float32)\n",
    "    return float(confs.mean()), float(confs.max()), float(confs.min())\n",
    "\n",
    "\n",
    "\n",
    "def plot_loss_history(agent):\n",
    "    if not agent.loss_history:\n",
    "        print(\"[BuyTrainer-MP] No loss history to plot.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(agent.loss_history)\n",
    "    plt.xlabel(\"Update step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"BuyAgent DDQN Loss (MP training)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def greedy_buy_rate(buy_trainer):\n",
    "    agent = buy_trainer.agent\n",
    "    state_df = buy_trainer.state_df\n",
    "    env = buy_trainer.env\n",
    "\n",
    "    assert agent is not None and state_df is not None\n",
    "    assert hasattr(env, \"BUY\"), \"BuyEnv must define BUY action index\"\n",
    "\n",
    "    buy_index = env.BUY\n",
    "\n",
    "    buys = 0\n",
    "    for i in range(len(state_df)):\n",
    "        state = state_df.iloc[i].values.astype(np.float32)\n",
    "        a = agent.select_action(state, greedy=True)\n",
    "        buys += int(a == buy_index)\n",
    "\n",
    "    return buys / len(state_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11dc426b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BuyTrainer] Raw dataset: (1224, 10)\n",
      "[BuyTrainer] After dropna: (1224, 10)\n",
      "[BuyTrainer] Rolling state_df shape: (1194, 270)\n",
      "[BuyTrainer] state_dim=270, actions=2\n",
      "[BuyTrainer-MP] state_df shape: (1194, 270)\n",
      "[BuyTrainer-MP] prices shape: (1194,)\n",
      "[BuyTrainer-MP] Starting MP training...\n",
      "[MP Trainer] Starting workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10/600] Buffer size=3000 | eps=1.000\n",
      "[Batch 20/600] Buffer size=6000 | eps=1.000\n",
      "[Batch 30/600] Buffer size=9000 | eps=1.000\n",
      "[Batch 40/600] Buffer size=12000 | eps=1.000\n",
      "[Batch 50/600] Buffer size=15000 | eps=1.000\n",
      "[Probe @ batch 50] BUY_conf mean=0.5005 max=0.5054 min=0.4943 | greedy_buy_rate=0.656\n",
      "[Batch 60/600] Buffer size=18000 | eps=1.000\n",
      "[Batch 70/600] Buffer size=21000 | eps=1.000\n",
      "[Batch 80/600] Buffer size=24000 | eps=1.000\n",
      "[Batch 90/600] Buffer size=27000 | eps=1.000\n",
      "[Batch 100/600] Buffer size=30000 | eps=1.000\n",
      "[Probe @ batch 100] BUY_conf mean=0.5008 max=0.5033 min=0.4979 | greedy_buy_rate=0.906\n",
      "[Batch 110/600] Buffer size=33000 | eps=1.000\n",
      "[Batch 120/600] Buffer size=36000 | eps=1.000\n",
      "[Batch 130/600] Buffer size=39000 | eps=1.000\n",
      "[Batch 140/600] Buffer size=42000 | eps=1.000\n",
      "[Batch 150/600] Buffer size=45000 | eps=1.000\n",
      "[Probe @ batch 150] BUY_conf mean=0.5006 max=0.5027 min=0.4993 | greedy_buy_rate=0.922\n",
      "[Batch 160/600] Buffer size=48000 | eps=1.000\n",
      "[Batch 170/600] Buffer size=51000 | eps=1.000\n",
      "[Batch 180/600] Buffer size=54000 | eps=1.000\n",
      "[Batch 190/600] Buffer size=57000 | eps=1.000\n",
      "[Batch 200/600] Buffer size=60000 | eps=1.000\n",
      "[Probe @ batch 200] BUY_conf mean=0.5006 max=0.5025 min=0.4994 | greedy_buy_rate=0.703\n",
      "[Batch 210/600] Buffer size=63000 | eps=1.000\n",
      "[Batch 220/600] Buffer size=66000 | eps=1.000\n",
      "[Batch 230/600] Buffer size=69000 | eps=1.000\n",
      "[Batch 240/600] Buffer size=72000 | eps=1.000\n",
      "[Batch 250/600] Buffer size=75000 | eps=1.000\n",
      "[Probe @ batch 250] BUY_conf mean=0.4993 max=0.4998 min=0.4989 | greedy_buy_rate=0.000\n",
      "[Batch 260/600] Buffer size=78000 | eps=1.000\n",
      "[Batch 270/600] Buffer size=81000 | eps=1.000\n",
      "[Batch 280/600] Buffer size=84000 | eps=1.000\n",
      "[Batch 290/600] Buffer size=87000 | eps=1.000\n",
      "[Batch 300/600] Buffer size=90000 | eps=1.000\n",
      "[Probe @ batch 300] BUY_conf mean=0.5008 max=0.5049 min=0.4999 | greedy_buy_rate=0.953\n",
      "[Batch 310/600] Buffer size=93000 | eps=1.000\n",
      "[Batch 320/600] Buffer size=96000 | eps=1.000\n",
      "[Batch 330/600] Buffer size=99000 | eps=1.000\n",
      "[Batch 340/600] Buffer size=102000 | eps=1.000\n",
      "[Batch 350/600] Buffer size=105000 | eps=1.000\n",
      "[Probe @ batch 350] BUY_conf mean=0.5010 max=0.5015 min=0.5001 | greedy_buy_rate=1.000\n",
      "[Batch 360/600] Buffer size=108000 | eps=1.000\n",
      "[Batch 370/600] Buffer size=111000 | eps=1.000\n",
      "[Batch 380/600] Buffer size=114000 | eps=1.000\n",
      "[Batch 390/600] Buffer size=117000 | eps=1.000\n",
      "[Batch 400/600] Buffer size=120000 | eps=1.000\n",
      "[Probe @ batch 400] BUY_conf mean=0.5008 max=0.5035 min=0.4982 | greedy_buy_rate=0.781\n",
      "[Batch 410/600] Buffer size=123000 | eps=1.000\n",
      "[Batch 420/600] Buffer size=126000 | eps=1.000\n",
      "[Batch 430/600] Buffer size=129000 | eps=1.000\n",
      "[Batch 440/600] Buffer size=132000 | eps=1.000\n",
      "[Batch 450/600] Buffer size=135000 | eps=1.000\n",
      "[Probe @ batch 450] BUY_conf mean=0.4995 max=0.4998 min=0.4979 | greedy_buy_rate=0.000\n",
      "[Batch 460/600] Buffer size=138000 | eps=1.000\n",
      "[Batch 470/600] Buffer size=141000 | eps=1.000\n",
      "[Batch 480/600] Buffer size=144000 | eps=1.000\n",
      "[Batch 490/600] Buffer size=147000 | eps=1.000\n",
      "[Batch 500/600] Buffer size=150000 | eps=1.000\n",
      "[Probe @ batch 500] BUY_conf mean=0.4996 max=0.5014 min=0.4986 | greedy_buy_rate=0.031\n",
      "[Batch 510/600] Buffer size=153000 | eps=1.000\n",
      "[Batch 520/600] Buffer size=156000 | eps=1.000\n",
      "[Batch 530/600] Buffer size=159000 | eps=1.000\n",
      "[Batch 540/600] Buffer size=162000 | eps=1.000\n",
      "[Batch 550/600] Buffer size=165000 | eps=1.000\n",
      "[Probe @ batch 550] BUY_conf mean=0.5002 max=0.5018 min=0.4990 | greedy_buy_rate=0.906\n",
      "[Batch 560/600] Buffer size=168000 | eps=1.000\n",
      "[Batch 570/600] Buffer size=171000 | eps=1.000\n",
      "[Batch 580/600] Buffer size=174000 | eps=1.000\n",
      "[Batch 590/600] Buffer size=177000 | eps=1.000\n",
      "[Batch 600/600] Buffer size=180000 | eps=1.000\n",
      "[Probe @ batch 600] BUY_conf mean=0.5000 max=0.5010 min=0.4996 | greedy_buy_rate=0.359\n",
      "[MP Trainer] Training complete.\n",
      "Greedy eval reward=0.9620 steps=1193\n",
      "BUY confidence mean=0.499916 max=0.501307 min=0.494677\n",
      "Greedy BUY rate=0.3208\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "    buy_trainer = train_buy_agent_mp()\n",
    "\n",
    "    reward, steps = evaluate_greedy_buy(buy_trainer)\n",
    "    avg_buy, max_buy, min_buy = inspect_buy_confidence(buy_trainer)\n",
    "    buy_rate = greedy_buy_rate(buy_trainer)\n",
    "\n",
    "    print(f\"Greedy eval reward={reward:.4f} steps={steps}\")\n",
    "    print(f\"BUY confidence mean={avg_buy:.6f} max={max_buy:.6f} min={min_buy:.6f}\")\n",
    "    print(f\"Greedy BUY rate={buy_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4199dabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUY index: 1\n",
      "HOLD index: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"BUY index:\", buy_trainer.env.BUY)\n",
    "print(\"HOLD index:\", buy_trainer.env.HOLD)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
