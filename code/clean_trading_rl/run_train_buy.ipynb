{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Buy DDQN training (notebook version)\n",
        "\n",
        "Converted from your `run_train_buy.py`-style script.\n",
        "\n",
        "1. Edit paths in **Parameters**.\n",
        "2. Run cells top-to-bottom.\n",
        "3. Model + diagnostics saved under `out_dir/<run_id>/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters (edit these)\n",
        "config_path = \"config.yaml\"                 # path to your YAML config\n",
        "features_npy = \"data/features.npy\"      # (n_steps, state_dim)\n",
        "prices_npy = \"data/prices.npy\"          # (n_steps,)\n",
        "out_dir = \"runs\"                            # output root folder\n",
        "run_id = None                               # set to a string to override, or leave None for timestamp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# If you run this notebook from outside your project root, you may need:\n",
        "# import sys\n",
        "# sys.path.append(\"/absolute/path/to/clean_trading_rl\")\n",
        "\n",
        "from core.config import load_config\n",
        "from agents.ddqn_agent import DDQNAgent\n",
        "from envs.buy_env import BuyEnv\n",
        "from diagnostics.q_gap import compute_q_gap, plot_q_gap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features: (6195, 10) prices: (6195,)\n",
            "state_dim: 10 n_actions: 2\n"
          ]
        }
      ],
      "source": [
        "# Load config + data\n",
        "cfg = load_config(config_path)\n",
        "\n",
        "features = np.load(features_npy)\n",
        "prices = np.load(prices_npy)\n",
        "\n",
        "cfg.agent.state_dim = int(features.shape[1])\n",
        "cfg.agent.n_actions = 2\n",
        "\n",
        "env = BuyEnv(features, prices, cfg.reward, cfg.trade_manager)\n",
        "agent = DDQNAgent(cfg.agent)\n",
        "\n",
        "print(\"features:\", features.shape, \"prices:\", prices.shape)\n",
        "print(\"state_dim:\", cfg.agent.state_dim, \"n_actions:\", cfg.agent.n_actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_path: runs/20260113-232303\n"
          ]
        }
      ],
      "source": [
        "# Output folder\n",
        "if run_id is None:\n",
        "    run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "run_path = os.path.join(out_dir, run_id)\n",
        "os.makedirs(run_path, exist_ok=True)\n",
        "\n",
        "print(\"run_path:\", run_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ced7936b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.is_inference_mode_enabled(): False\n",
            "torch.is_grad_enabled(): True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"torch.is_inference_mode_enabled():\", torch.is_inference_mode_enabled())\n",
        "print(\"torch.is_grad_enabled():\", torch.is_grad_enabled())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BUY] ep=1/200 reward=0.8143 eps=0.976 total_steps=500 learn_steps=263 loss=0.0028921966440975666\n",
            "target sync @ 500 loss 0.002374432049691677\n",
            "[BUY] ep=2/200 reward=1.3121 eps=0.953 total_steps=1000 learn_steps=763 loss=0.00036167068174108863\n",
            "target sync @ 1000 loss 0.00027116044657304883\n",
            "[BUY] ep=3/200 reward=1.2410 eps=0.929 total_steps=1500 learn_steps=1263 loss=0.0001651451748330146\n",
            "target sync @ 1500 loss 9.78595344349742e-05\n",
            "[BUY] ep=4/200 reward=0.8409 eps=0.905 total_steps=2000 learn_steps=1763 loss=0.00036094122333452106\n",
            "target sync @ 2000 loss 0.0002143800666090101\n",
            "[BUY] ep=5/200 reward=1.1366 eps=0.881 total_steps=2500 learn_steps=2263 loss=0.00014927815936971456\n",
            "target sync @ 2500 loss 0.00035478401696309447\n",
            "[BUY] ep=6/200 reward=0.8667 eps=0.858 total_steps=3000 learn_steps=2763 loss=0.000264271191554144\n",
            "target sync @ 3000 loss 0.00017317270976491272\n",
            "[BUY] ep=7/200 reward=0.7860 eps=0.834 total_steps=3500 learn_steps=3263 loss=0.00010935858881566674\n",
            "target sync @ 3500 loss 0.0004065746907144785\n",
            "[BUY] ep=8/200 reward=0.7751 eps=0.810 total_steps=4000 learn_steps=3763 loss=9.434058301849291e-05\n",
            "target sync @ 4000 loss 0.0002827669959515333\n",
            "[BUY] ep=9/200 reward=0.7101 eps=0.786 total_steps=4500 learn_steps=4263 loss=0.00034977056202478707\n",
            "target sync @ 4500 loss 0.00023676373530179262\n",
            "[BUY] ep=10/200 reward=0.6070 eps=0.762 total_steps=5000 learn_steps=4763 loss=0.0007650175830349326\n",
            "target sync @ 5000 loss 0.0001396621228195727\n",
            "[BUY] ep=11/200 reward=1.1968 eps=0.739 total_steps=5500 learn_steps=5263 loss=0.00012297691137064248\n",
            "target sync @ 5500 loss 0.0005931165651418269\n",
            "[BUY] ep=12/200 reward=0.7068 eps=0.715 total_steps=6000 learn_steps=5763 loss=0.0001355635467916727\n",
            "target sync @ 6000 loss 0.00017841154476627707\n",
            "[BUY] ep=13/200 reward=1.1475 eps=0.691 total_steps=6500 learn_steps=6263 loss=0.0001576592039782554\n",
            "target sync @ 6500 loss 5.708808021154255e-05\n",
            "[BUY] ep=14/200 reward=1.2960 eps=0.667 total_steps=7000 learn_steps=6763 loss=0.00026505746063776314\n",
            "target sync @ 7000 loss 0.0006100976606830955\n",
            "[BUY] ep=15/200 reward=0.7343 eps=0.644 total_steps=7500 learn_steps=7263 loss=0.0003207873087376356\n",
            "target sync @ 7500 loss 0.00016487849643453956\n",
            "[BUY] ep=16/200 reward=0.8729 eps=0.620 total_steps=8000 learn_steps=7763 loss=0.000399635115172714\n",
            "target sync @ 8000 loss 5.874102498637512e-05\n",
            "[BUY] ep=17/200 reward=0.9824 eps=0.596 total_steps=8500 learn_steps=8263 loss=0.0002857849467545748\n",
            "target sync @ 8500 loss 4.789613376487978e-05\n",
            "[BUY] ep=18/200 reward=1.0871 eps=0.573 total_steps=9000 learn_steps=8763 loss=0.00015595117292832583\n",
            "target sync @ 9000 loss 0.00041376854642294347\n",
            "[BUY] ep=19/200 reward=0.9451 eps=0.549 total_steps=9500 learn_steps=9263 loss=0.00019910540140699595\n",
            "target sync @ 9500 loss 0.0001379908644594252\n",
            "[BUY] ep=20/200 reward=0.7025 eps=0.525 total_steps=10000 learn_steps=9763 loss=0.00013565194967668504\n",
            "target sync @ 10000 loss 0.00013591980678029358\n",
            "[BUY] ep=21/200 reward=0.8292 eps=0.501 total_steps=10500 learn_steps=10263 loss=0.00018264836398884654\n",
            "target sync @ 10500 loss 0.00031573447631672025\n",
            "[BUY] ep=22/200 reward=0.7645 eps=0.478 total_steps=11000 learn_steps=10763 loss=0.0002283856156282127\n",
            "target sync @ 11000 loss 0.00017219108121935278\n",
            "[BUY] ep=23/200 reward=0.7861 eps=0.454 total_steps=11500 learn_steps=11263 loss=0.0003891863161697984\n",
            "target sync @ 11500 loss 0.0003197882615495473\n",
            "[BUY] ep=24/200 reward=1.1142 eps=0.430 total_steps=12000 learn_steps=11763 loss=0.00013907640823163092\n",
            "target sync @ 12000 loss 0.00031994114397093654\n",
            "[BUY] ep=25/200 reward=0.6510 eps=0.406 total_steps=12500 learn_steps=12263 loss=0.00022074677690397948\n",
            "target sync @ 12500 loss 0.00015401392010971904\n",
            "[BUY] ep=26/200 reward=1.0839 eps=0.383 total_steps=13000 learn_steps=12763 loss=0.0003736635553650558\n",
            "target sync @ 13000 loss 0.00027621709159575403\n",
            "[BUY] ep=27/200 reward=0.6169 eps=0.359 total_steps=13500 learn_steps=13263 loss=0.00037778992555104196\n",
            "target sync @ 13500 loss 5.9498866903595626e-05\n",
            "[BUY] ep=28/200 reward=1.0483 eps=0.335 total_steps=14000 learn_steps=13763 loss=0.00012690512812696397\n",
            "target sync @ 14000 loss 0.0001857579918578267\n",
            "[BUY] ep=29/200 reward=0.9889 eps=0.311 total_steps=14500 learn_steps=14263 loss=0.00042560993460938334\n",
            "target sync @ 14500 loss 7.585895946249366e-05\n",
            "[BUY] ep=30/200 reward=0.7448 eps=0.288 total_steps=15000 learn_steps=14763 loss=0.0010774002876132727\n",
            "target sync @ 15000 loss 0.0002984924358315766\n",
            "[BUY] ep=31/200 reward=0.8241 eps=0.264 total_steps=15500 learn_steps=15263 loss=0.00011752332648029551\n",
            "target sync @ 15500 loss 0.000108949257992208\n",
            "[BUY] ep=32/200 reward=0.7730 eps=0.240 total_steps=16000 learn_steps=15763 loss=7.432425627484918e-05\n",
            "target sync @ 16000 loss 0.000232039499678649\n",
            "[BUY] ep=33/200 reward=0.5917 eps=0.216 total_steps=16500 learn_steps=16263 loss=0.00023416154726874083\n",
            "target sync @ 16500 loss 0.0005541712162084877\n",
            "[BUY] ep=34/200 reward=0.8434 eps=0.193 total_steps=17000 learn_steps=16763 loss=0.00014700714382342994\n",
            "target sync @ 17000 loss 0.0002737859031185508\n",
            "[BUY] ep=35/200 reward=0.6378 eps=0.169 total_steps=17500 learn_steps=17263 loss=0.00024323651450686157\n",
            "target sync @ 17500 loss 0.00021036239922977984\n",
            "[BUY] ep=36/200 reward=0.9251 eps=0.145 total_steps=18000 learn_steps=17763 loss=5.5307336879195645e-05\n",
            "target sync @ 18000 loss 0.00015206701937131584\n",
            "[BUY] ep=37/200 reward=0.7207 eps=0.121 total_steps=18500 learn_steps=18263 loss=0.0001553364854771644\n",
            "target sync @ 18500 loss 0.0002596603590063751\n",
            "[BUY] ep=38/200 reward=1.2248 eps=0.098 total_steps=19000 learn_steps=18763 loss=0.00011003079998772591\n",
            "target sync @ 19000 loss 7.733252277830616e-05\n",
            "[BUY] ep=39/200 reward=0.6688 eps=0.074 total_steps=19500 learn_steps=19263 loss=0.0001122601970564574\n",
            "target sync @ 19500 loss 4.9951340770348907e-05\n",
            "[BUY] ep=40/200 reward=1.0655 eps=0.050 total_steps=20000 learn_steps=19763 loss=9.3484552053269e-05\n",
            "target sync @ 20000 loss 0.00017954308714251965\n",
            "[BUY] ep=41/200 reward=0.7973 eps=0.050 total_steps=20500 learn_steps=20263 loss=0.0003971757832914591\n",
            "target sync @ 20500 loss 0.00026976235676556826\n",
            "[BUY] ep=42/200 reward=0.7228 eps=0.050 total_steps=21000 learn_steps=20763 loss=0.00020391256839502603\n",
            "target sync @ 21000 loss 4.550154699245468e-05\n",
            "[BUY] ep=43/200 reward=0.9271 eps=0.050 total_steps=21500 learn_steps=21263 loss=8.765675011090934e-05\n",
            "target sync @ 21500 loss 0.00011234173143748194\n",
            "[BUY] ep=44/200 reward=1.0103 eps=0.050 total_steps=22000 learn_steps=21763 loss=3.657022534753196e-05\n",
            "target sync @ 22000 loss 0.00038551053148694336\n",
            "[BUY] ep=45/200 reward=1.1157 eps=0.050 total_steps=22500 learn_steps=22263 loss=0.00015471357619389892\n",
            "target sync @ 22500 loss 0.000417016155552119\n",
            "[BUY] ep=46/200 reward=0.6149 eps=0.050 total_steps=23000 learn_steps=22763 loss=0.00022713460202794522\n",
            "target sync @ 23000 loss 0.00019098262418992817\n",
            "[BUY] ep=47/200 reward=0.7402 eps=0.050 total_steps=23500 learn_steps=23263 loss=8.278795576188713e-05\n",
            "target sync @ 23500 loss 0.00021055543038528413\n",
            "[BUY] ep=48/200 reward=0.7400 eps=0.050 total_steps=24000 learn_steps=23763 loss=0.00011605363397393376\n",
            "target sync @ 24000 loss 3.6392666515894234e-05\n",
            "[BUY] ep=49/200 reward=0.7699 eps=0.050 total_steps=24500 learn_steps=24263 loss=9.626669634599239e-05\n",
            "target sync @ 24500 loss 0.00019887373491656035\n",
            "[BUY] ep=50/200 reward=0.5893 eps=0.050 total_steps=25000 learn_steps=24763 loss=4.765875928569585e-05\n",
            "target sync @ 25000 loss 2.6479952794034034e-05\n",
            "[BUY] ep=51/200 reward=0.4363 eps=0.050 total_steps=25500 learn_steps=25263 loss=2.2689124307362363e-05\n",
            "target sync @ 25500 loss 0.0009024189785122871\n",
            "[BUY] ep=52/200 reward=0.8681 eps=0.050 total_steps=26000 learn_steps=25763 loss=0.00011195428669452667\n",
            "target sync @ 26000 loss 3.6785146221518517e-05\n",
            "[BUY] ep=53/200 reward=0.8536 eps=0.050 total_steps=26500 learn_steps=26263 loss=5.098749898024835e-05\n",
            "target sync @ 26500 loss 5.4701125918654725e-05\n",
            "[BUY] ep=54/200 reward=0.7134 eps=0.050 total_steps=27000 learn_steps=26763 loss=0.0001980060915229842\n",
            "target sync @ 27000 loss 9.829372720560059e-05\n",
            "[BUY] ep=55/200 reward=1.0947 eps=0.050 total_steps=27500 learn_steps=27263 loss=0.00021358646336011589\n",
            "target sync @ 27500 loss 0.0004786195349879563\n",
            "[BUY] ep=56/200 reward=0.8577 eps=0.050 total_steps=28000 learn_steps=27763 loss=0.00014575492241419852\n",
            "target sync @ 28000 loss 0.0002165946934837848\n",
            "[BUY] ep=57/200 reward=0.6611 eps=0.050 total_steps=28500 learn_steps=28263 loss=0.00016412290278822184\n",
            "target sync @ 28500 loss 4.575941420625895e-05\n",
            "[BUY] ep=58/200 reward=0.3635 eps=0.050 total_steps=29000 learn_steps=28763 loss=5.4953357903286815e-05\n",
            "target sync @ 29000 loss 6.87182619003579e-05\n",
            "[BUY] ep=59/200 reward=0.9874 eps=0.050 total_steps=29500 learn_steps=29263 loss=0.00026760619948618114\n",
            "target sync @ 29500 loss 0.0001094974868465215\n",
            "[BUY] ep=60/200 reward=0.6336 eps=0.050 total_steps=30000 learn_steps=29763 loss=0.0001252691145054996\n",
            "target sync @ 30000 loss 2.863202644221019e-05\n",
            "[BUY] ep=61/200 reward=0.6891 eps=0.050 total_steps=30500 learn_steps=30263 loss=0.0002487101883161813\n",
            "target sync @ 30500 loss 6.094710261095315e-05\n",
            "[BUY] ep=62/200 reward=0.8591 eps=0.050 total_steps=31000 learn_steps=30763 loss=0.00010344504698878154\n",
            "target sync @ 31000 loss 0.0003215695032849908\n",
            "[BUY] ep=63/200 reward=0.4950 eps=0.050 total_steps=31500 learn_steps=31263 loss=7.22986733308062e-05\n",
            "target sync @ 31500 loss 0.00012276058259885758\n",
            "[BUY] ep=64/200 reward=0.3317 eps=0.050 total_steps=32000 learn_steps=31763 loss=0.00041847748798318207\n",
            "target sync @ 32000 loss 5.0482602091506124e-05\n",
            "[BUY] ep=65/200 reward=0.4001 eps=0.050 total_steps=32500 learn_steps=32263 loss=0.00017757421301212162\n",
            "target sync @ 32500 loss 0.00010232919885311276\n",
            "[BUY] ep=66/200 reward=0.9940 eps=0.050 total_steps=33000 learn_steps=32763 loss=0.0004137546056881547\n",
            "target sync @ 33000 loss 0.0003705896087922156\n",
            "[BUY] ep=67/200 reward=0.5571 eps=0.050 total_steps=33500 learn_steps=33263 loss=0.00017717655282467604\n",
            "target sync @ 33500 loss 0.00011738248576875776\n",
            "[BUY] ep=68/200 reward=0.6163 eps=0.050 total_steps=34000 learn_steps=33763 loss=0.00023895026242826134\n",
            "target sync @ 34000 loss 0.000293152523227036\n",
            "[BUY] ep=69/200 reward=0.6028 eps=0.050 total_steps=34500 learn_steps=34263 loss=0.00010730965004768223\n",
            "target sync @ 34500 loss 0.00019511618302203715\n",
            "[BUY] ep=70/200 reward=0.9733 eps=0.050 total_steps=35000 learn_steps=34763 loss=0.0007432593265548348\n",
            "target sync @ 35000 loss 0.0001892851578304544\n",
            "[BUY] ep=71/200 reward=0.4433 eps=0.050 total_steps=35500 learn_steps=35263 loss=7.594157796120271e-05\n",
            "target sync @ 35500 loss 0.00020043589756824076\n",
            "[BUY] ep=72/200 reward=0.7521 eps=0.050 total_steps=36000 learn_steps=35763 loss=0.0004098464851267636\n",
            "target sync @ 36000 loss 0.00011797777551691979\n",
            "[BUY] ep=73/200 reward=0.5969 eps=0.050 total_steps=36500 learn_steps=36263 loss=8.399378566537052e-05\n",
            "target sync @ 36500 loss 0.00013692992797587067\n",
            "[BUY] ep=74/200 reward=0.5288 eps=0.050 total_steps=37000 learn_steps=36763 loss=0.00015376898227259517\n",
            "target sync @ 37000 loss 3.483068576315418e-05\n",
            "[BUY] ep=75/200 reward=0.9156 eps=0.050 total_steps=37500 learn_steps=37263 loss=7.184829883044586e-05\n",
            "target sync @ 37500 loss 5.314900408848189e-05\n",
            "[BUY] ep=76/200 reward=1.1049 eps=0.050 total_steps=38000 learn_steps=37763 loss=0.00018441674183122814\n",
            "target sync @ 38000 loss 8.694440475665033e-05\n",
            "[BUY] ep=77/200 reward=0.9377 eps=0.050 total_steps=38500 learn_steps=38263 loss=0.00013600783131550997\n",
            "target sync @ 38500 loss 7.600021490361542e-05\n",
            "[BUY] ep=78/200 reward=0.6191 eps=0.050 total_steps=39000 learn_steps=38763 loss=0.0013296975521370769\n",
            "target sync @ 39000 loss 0.0001894761371659115\n",
            "[BUY] ep=79/200 reward=0.4466 eps=0.050 total_steps=39500 learn_steps=39263 loss=0.0005735314334742725\n",
            "target sync @ 39500 loss 0.0003459278086666018\n",
            "[BUY] ep=80/200 reward=0.8980 eps=0.050 total_steps=40000 learn_steps=39763 loss=2.9326345611480065e-05\n",
            "target sync @ 40000 loss 0.0001559227384859696\n",
            "[BUY] ep=81/200 reward=0.7937 eps=0.050 total_steps=40500 learn_steps=40263 loss=0.0004098114150110632\n",
            "target sync @ 40500 loss 0.00014235814160201699\n",
            "[BUY] ep=82/200 reward=0.4827 eps=0.050 total_steps=41000 learn_steps=40763 loss=0.0010719387792050838\n",
            "target sync @ 41000 loss 0.00011699344031512737\n",
            "[BUY] ep=83/200 reward=0.7031 eps=0.050 total_steps=41500 learn_steps=41263 loss=1.8673645172384568e-05\n",
            "target sync @ 41500 loss 1.5826144590391777e-05\n",
            "[BUY] ep=84/200 reward=0.5707 eps=0.050 total_steps=42000 learn_steps=41763 loss=0.00017697116709314287\n",
            "target sync @ 42000 loss 7.439262117259204e-05\n",
            "[BUY] ep=85/200 reward=0.5328 eps=0.050 total_steps=42500 learn_steps=42263 loss=2.3960237740539014e-05\n",
            "target sync @ 42500 loss 0.00036601710598915815\n",
            "[BUY] ep=86/200 reward=0.6591 eps=0.050 total_steps=43000 learn_steps=42763 loss=0.00012565209181047976\n",
            "target sync @ 43000 loss 0.0001552217872813344\n",
            "[BUY] ep=87/200 reward=0.9150 eps=0.050 total_steps=43500 learn_steps=43263 loss=3.0682916985824704e-05\n",
            "target sync @ 43500 loss 0.00012439244892448187\n",
            "[BUY] ep=88/200 reward=1.0680 eps=0.050 total_steps=44000 learn_steps=43763 loss=0.0002481657429598272\n",
            "target sync @ 44000 loss 0.00015725853154435754\n",
            "[BUY] ep=89/200 reward=0.4978 eps=0.050 total_steps=44500 learn_steps=44263 loss=4.297542182030156e-05\n",
            "target sync @ 44500 loss 0.00029303043265827\n",
            "[BUY] ep=90/200 reward=0.6202 eps=0.050 total_steps=45000 learn_steps=44763 loss=5.2813229558523744e-05\n",
            "target sync @ 45000 loss 8.234695997089148e-05\n",
            "[BUY] ep=91/200 reward=0.6746 eps=0.050 total_steps=45500 learn_steps=45263 loss=0.0002707377716433257\n",
            "target sync @ 45500 loss 0.0003775894292630255\n",
            "[BUY] ep=92/200 reward=0.9749 eps=0.050 total_steps=46000 learn_steps=45763 loss=0.00015297527716029435\n",
            "target sync @ 46000 loss 0.0001629279722692445\n",
            "[BUY] ep=93/200 reward=0.7367 eps=0.050 total_steps=46500 learn_steps=46263 loss=4.733512832899578e-05\n",
            "target sync @ 46500 loss 0.00011984364391537383\n",
            "[BUY] ep=94/200 reward=0.8329 eps=0.050 total_steps=47000 learn_steps=46763 loss=1.767544927133713e-05\n",
            "target sync @ 47000 loss 0.00020165066234767437\n",
            "[BUY] ep=95/200 reward=0.6607 eps=0.050 total_steps=47500 learn_steps=47263 loss=0.0001224276056746021\n",
            "target sync @ 47500 loss 3.906893834937364e-05\n",
            "[BUY] ep=96/200 reward=0.9381 eps=0.050 total_steps=48000 learn_steps=47763 loss=3.957631270168349e-05\n",
            "target sync @ 48000 loss 0.0001540902885608375\n",
            "[BUY] ep=97/200 reward=0.6756 eps=0.050 total_steps=48500 learn_steps=48263 loss=7.970088336151093e-05\n",
            "target sync @ 48500 loss 0.00018159390310756862\n",
            "[BUY] ep=98/200 reward=0.4555 eps=0.050 total_steps=49000 learn_steps=48763 loss=2.169982144550886e-05\n",
            "target sync @ 49000 loss 0.00015811496996320784\n",
            "[BUY] ep=99/200 reward=0.8045 eps=0.050 total_steps=49500 learn_steps=49263 loss=0.00013526930706575513\n",
            "target sync @ 49500 loss 6.595478771487251e-05\n",
            "[BUY] ep=100/200 reward=0.6234 eps=0.050 total_steps=50000 learn_steps=49763 loss=0.00011200614972040057\n",
            "target sync @ 50000 loss 0.0005903092678636312\n",
            "[BUY] ep=101/200 reward=0.7834 eps=0.050 total_steps=50500 learn_steps=50263 loss=0.00020351151761133224\n",
            "target sync @ 50500 loss 2.263491478515789e-05\n",
            "[BUY] ep=102/200 reward=0.6516 eps=0.050 total_steps=51000 learn_steps=50763 loss=9.09903465071693e-05\n",
            "target sync @ 51000 loss 0.00043656432535499334\n",
            "[BUY] ep=103/200 reward=0.8799 eps=0.050 total_steps=51500 learn_steps=51263 loss=0.00048636377323418856\n",
            "target sync @ 51500 loss 7.853574788896367e-05\n",
            "[BUY] ep=104/200 reward=0.9336 eps=0.050 total_steps=52000 learn_steps=51763 loss=5.9868158132303506e-05\n",
            "target sync @ 52000 loss 2.741395655903034e-05\n",
            "[BUY] ep=105/200 reward=0.8207 eps=0.050 total_steps=52500 learn_steps=52263 loss=0.0005938659305684268\n",
            "target sync @ 52500 loss 0.00015475857071578503\n",
            "[BUY] ep=106/200 reward=0.8837 eps=0.050 total_steps=53000 learn_steps=52763 loss=4.80082344438415e-05\n",
            "target sync @ 53000 loss 0.0008037264342419803\n",
            "[BUY] ep=107/200 reward=0.6180 eps=0.050 total_steps=53500 learn_steps=53263 loss=0.0003271335444878787\n",
            "target sync @ 53500 loss 1.1708889360306785e-05\n",
            "[BUY] ep=108/200 reward=0.7694 eps=0.050 total_steps=54000 learn_steps=53763 loss=0.0001546308776596561\n",
            "target sync @ 54000 loss 7.062536315061152e-05\n",
            "[BUY] ep=109/200 reward=0.9086 eps=0.050 total_steps=54500 learn_steps=54263 loss=0.00011575505050132051\n",
            "target sync @ 54500 loss 0.0001561827666591853\n",
            "[BUY] ep=110/200 reward=1.1881 eps=0.050 total_steps=55000 learn_steps=54763 loss=0.0001463366497773677\n",
            "target sync @ 55000 loss 0.000254415615927428\n",
            "[BUY] ep=111/200 reward=0.4944 eps=0.050 total_steps=55500 learn_steps=55263 loss=0.0001507848792243749\n",
            "target sync @ 55500 loss 0.00036454445216804743\n",
            "[BUY] ep=112/200 reward=0.6307 eps=0.050 total_steps=56000 learn_steps=55763 loss=2.1255964384181425e-05\n",
            "target sync @ 56000 loss 3.877536437357776e-05\n",
            "[BUY] ep=113/200 reward=0.6369 eps=0.050 total_steps=56500 learn_steps=56263 loss=0.00019418130978010595\n",
            "target sync @ 56500 loss 4.724712925963104e-05\n",
            "[BUY] ep=114/200 reward=0.7839 eps=0.050 total_steps=57000 learn_steps=56763 loss=0.0007763032335788012\n",
            "target sync @ 57000 loss 0.0001840139739215374\n",
            "[BUY] ep=115/200 reward=0.9337 eps=0.050 total_steps=57500 learn_steps=57263 loss=0.00014654052210971713\n",
            "target sync @ 57500 loss 2.3404802050208673e-05\n",
            "[BUY] ep=116/200 reward=0.7898 eps=0.050 total_steps=58000 learn_steps=57763 loss=3.947937875636853e-05\n",
            "target sync @ 58000 loss 3.3381234970875084e-05\n",
            "[BUY] ep=117/200 reward=0.5377 eps=0.050 total_steps=58500 learn_steps=58263 loss=8.797972259344533e-05\n",
            "target sync @ 58500 loss 0.0001588594459462911\n",
            "[BUY] ep=118/200 reward=0.9107 eps=0.050 total_steps=59000 learn_steps=58763 loss=1.9914983568014577e-05\n",
            "target sync @ 59000 loss 4.161383549217135e-05\n",
            "[BUY] ep=119/200 reward=0.4392 eps=0.050 total_steps=59500 learn_steps=59263 loss=6.969091919017956e-05\n",
            "target sync @ 59500 loss 0.00014624159666709602\n",
            "[BUY] ep=120/200 reward=0.8308 eps=0.050 total_steps=60000 learn_steps=59763 loss=4.7216512029990554e-05\n",
            "target sync @ 60000 loss 1.201931263494771e-05\n",
            "[BUY] ep=121/200 reward=0.7250 eps=0.050 total_steps=60500 learn_steps=60263 loss=0.0001882346987258643\n",
            "target sync @ 60500 loss 2.2471505872090347e-05\n",
            "[BUY] ep=122/200 reward=0.8846 eps=0.050 total_steps=61000 learn_steps=60763 loss=0.00022526492830365896\n",
            "target sync @ 61000 loss 6.456708797486499e-05\n",
            "[BUY] ep=123/200 reward=0.7605 eps=0.050 total_steps=61500 learn_steps=61263 loss=0.0007973822066560388\n",
            "target sync @ 61500 loss 2.3687192879151553e-05\n",
            "[BUY] ep=124/200 reward=0.6646 eps=0.050 total_steps=62000 learn_steps=61763 loss=7.829140668036416e-05\n",
            "target sync @ 62000 loss 0.00014385457325261086\n",
            "[BUY] ep=125/200 reward=0.8238 eps=0.050 total_steps=62500 learn_steps=62263 loss=0.00015830084157641977\n",
            "target sync @ 62500 loss 1.7461810784880072e-05\n",
            "[BUY] ep=126/200 reward=0.7938 eps=0.050 total_steps=63000 learn_steps=62763 loss=8.924138091970235e-05\n",
            "target sync @ 63000 loss 0.00025118893245235085\n",
            "[BUY] ep=127/200 reward=0.9350 eps=0.050 total_steps=63500 learn_steps=63263 loss=0.0002381293597863987\n",
            "target sync @ 63500 loss 2.820059671648778e-05\n",
            "[BUY] ep=128/200 reward=1.0452 eps=0.050 total_steps=64000 learn_steps=63763 loss=0.0002255726867588237\n",
            "target sync @ 64000 loss 0.00011775418533943594\n",
            "[BUY] ep=129/200 reward=0.9070 eps=0.050 total_steps=64500 learn_steps=64263 loss=0.00015609261754434556\n",
            "target sync @ 64500 loss 5.17191510880366e-05\n",
            "[BUY] ep=130/200 reward=0.6951 eps=0.050 total_steps=65000 learn_steps=64763 loss=0.0007791788084432483\n",
            "target sync @ 65000 loss 0.00017575047968421131\n",
            "[BUY] ep=131/200 reward=0.8541 eps=0.050 total_steps=65500 learn_steps=65263 loss=0.0001396231964463368\n",
            "target sync @ 65500 loss 1.4596216715290211e-05\n",
            "[BUY] ep=132/200 reward=0.8697 eps=0.050 total_steps=66000 learn_steps=65763 loss=0.00030516728293150663\n",
            "target sync @ 66000 loss 3.69855624740012e-05\n",
            "[BUY] ep=133/200 reward=0.5997 eps=0.050 total_steps=66500 learn_steps=66263 loss=0.0001885708188638091\n",
            "target sync @ 66500 loss 1.4613777238992043e-05\n",
            "[BUY] ep=134/200 reward=0.6267 eps=0.050 total_steps=67000 learn_steps=66763 loss=0.0003626156540121883\n",
            "target sync @ 67000 loss 0.0004073964082635939\n",
            "[BUY] ep=135/200 reward=0.8236 eps=0.050 total_steps=67500 learn_steps=67263 loss=0.00010470952838659286\n",
            "target sync @ 67500 loss 0.00024403570569120347\n",
            "[BUY] ep=136/200 reward=0.7817 eps=0.050 total_steps=68000 learn_steps=67763 loss=0.0001359723974019289\n",
            "target sync @ 68000 loss 0.0006347305607050657\n",
            "[BUY] ep=137/200 reward=0.4512 eps=0.050 total_steps=68500 learn_steps=68263 loss=0.0009932593675330281\n",
            "target sync @ 68500 loss 0.00032423061202280223\n",
            "[BUY] ep=138/200 reward=1.0252 eps=0.050 total_steps=69000 learn_steps=68763 loss=0.0003538727469276637\n",
            "target sync @ 69000 loss 0.00024599162861704826\n",
            "[BUY] ep=139/200 reward=0.8250 eps=0.050 total_steps=69500 learn_steps=69263 loss=0.00019708120089489967\n",
            "target sync @ 69500 loss 0.0005658160662278533\n",
            "[BUY] ep=140/200 reward=0.9151 eps=0.050 total_steps=70000 learn_steps=69763 loss=8.8508240878582e-05\n",
            "target sync @ 70000 loss 0.00012205730308778584\n",
            "[BUY] ep=141/200 reward=0.9001 eps=0.050 total_steps=70500 learn_steps=70263 loss=0.00015112292021512985\n",
            "target sync @ 70500 loss 1.9956298274337314e-05\n",
            "[BUY] ep=142/200 reward=0.9293 eps=0.050 total_steps=71000 learn_steps=70763 loss=2.883941851905547e-05\n",
            "target sync @ 71000 loss 0.0003287990693934262\n",
            "[BUY] ep=143/200 reward=1.0980 eps=0.050 total_steps=71500 learn_steps=71263 loss=0.00011532053031260148\n",
            "target sync @ 71500 loss 4.3086900404887274e-05\n",
            "[BUY] ep=144/200 reward=0.7842 eps=0.050 total_steps=72000 learn_steps=71763 loss=0.0003442063753027469\n",
            "target sync @ 72000 loss 0.00011249549424974248\n",
            "[BUY] ep=145/200 reward=0.6775 eps=0.050 total_steps=72500 learn_steps=72263 loss=0.00011304200597805902\n",
            "target sync @ 72500 loss 0.0006600585184060037\n",
            "[BUY] ep=146/200 reward=0.8972 eps=0.050 total_steps=73000 learn_steps=72763 loss=1.936453190864995e-05\n",
            "target sync @ 73000 loss 4.406492371344939e-05\n",
            "[BUY] ep=147/200 reward=0.7530 eps=0.050 total_steps=73500 learn_steps=73263 loss=0.00020791447605006397\n",
            "target sync @ 73500 loss 0.0007862327038310468\n",
            "[BUY] ep=148/200 reward=0.6966 eps=0.050 total_steps=74000 learn_steps=73763 loss=0.00016131388838402927\n",
            "target sync @ 74000 loss 7.843279308872297e-05\n",
            "[BUY] ep=149/200 reward=0.4282 eps=0.050 total_steps=74500 learn_steps=74263 loss=0.00010605301213217899\n",
            "target sync @ 74500 loss 0.0005118260160088539\n",
            "[BUY] ep=150/200 reward=0.8556 eps=0.050 total_steps=75000 learn_steps=74763 loss=0.0004145599959883839\n",
            "target sync @ 75000 loss 0.0004752459062729031\n",
            "[BUY] ep=151/200 reward=0.9937 eps=0.050 total_steps=75500 learn_steps=75263 loss=0.0003797508543357253\n",
            "target sync @ 75500 loss 0.0003966132935602218\n",
            "[BUY] ep=152/200 reward=0.9088 eps=0.050 total_steps=76000 learn_steps=75763 loss=2.44097118411446e-05\n",
            "target sync @ 76000 loss 1.3775333172816318e-05\n",
            "[BUY] ep=153/200 reward=1.0646 eps=0.050 total_steps=76500 learn_steps=76263 loss=0.0006680567166768014\n",
            "target sync @ 76500 loss 6.327670416794717e-05\n",
            "[BUY] ep=154/200 reward=0.4991 eps=0.050 total_steps=77000 learn_steps=76763 loss=0.00022918717877473682\n",
            "target sync @ 77000 loss 4.501789226196706e-05\n",
            "[BUY] ep=155/200 reward=0.9954 eps=0.050 total_steps=77500 learn_steps=77263 loss=0.00022214290220290422\n",
            "target sync @ 77500 loss 0.00011061550321755931\n",
            "[BUY] ep=156/200 reward=0.6321 eps=0.050 total_steps=78000 learn_steps=77763 loss=2.3933363991091028e-05\n",
            "target sync @ 78000 loss 0.0001679611305007711\n",
            "[BUY] ep=157/200 reward=0.9395 eps=0.050 total_steps=78500 learn_steps=78263 loss=0.00016247939493041486\n",
            "target sync @ 78500 loss 0.00031373684760183096\n",
            "[BUY] ep=158/200 reward=0.6302 eps=0.050 total_steps=79000 learn_steps=78763 loss=8.998912380775437e-05\n",
            "target sync @ 79000 loss 0.00010239531547995284\n",
            "[BUY] ep=159/200 reward=1.2129 eps=0.050 total_steps=79500 learn_steps=79263 loss=0.00011411101877456531\n",
            "target sync @ 79500 loss 0.0004730514483526349\n",
            "[BUY] ep=160/200 reward=0.7430 eps=0.050 total_steps=80000 learn_steps=79763 loss=4.303981768316589e-05\n",
            "target sync @ 80000 loss 0.0003318069502711296\n",
            "[BUY] ep=161/200 reward=0.9007 eps=0.050 total_steps=80500 learn_steps=80263 loss=0.00016039104957599193\n",
            "target sync @ 80500 loss 0.0002483010175637901\n",
            "[BUY] ep=162/200 reward=0.6026 eps=0.050 total_steps=81000 learn_steps=80763 loss=5.893857451155782e-05\n",
            "target sync @ 81000 loss 0.00017023053078446537\n",
            "[BUY] ep=163/200 reward=0.6387 eps=0.050 total_steps=81500 learn_steps=81263 loss=0.00019742247241083533\n",
            "target sync @ 81500 loss 0.000300584506476298\n",
            "[BUY] ep=164/200 reward=0.7435 eps=0.050 total_steps=82000 learn_steps=81763 loss=0.0002564800961408764\n",
            "target sync @ 82000 loss 8.58811690704897e-05\n",
            "[BUY] ep=165/200 reward=0.5068 eps=0.050 total_steps=82500 learn_steps=82263 loss=0.00027025482268072665\n",
            "target sync @ 82500 loss 1.5440939023392275e-05\n",
            "[BUY] ep=166/200 reward=0.6899 eps=0.050 total_steps=83000 learn_steps=82763 loss=1.9392882677493617e-05\n",
            "target sync @ 83000 loss 0.00019698645337484777\n",
            "[BUY] ep=167/200 reward=1.0406 eps=0.050 total_steps=83500 learn_steps=83263 loss=0.00011184414324816316\n",
            "target sync @ 83500 loss 0.0002097808028338477\n",
            "[BUY] ep=168/200 reward=0.4648 eps=0.050 total_steps=84000 learn_steps=83763 loss=0.0001274014648515731\n",
            "target sync @ 84000 loss 0.00014524914149660617\n",
            "[BUY] ep=169/200 reward=1.0821 eps=0.050 total_steps=84500 learn_steps=84263 loss=0.0001948861317941919\n",
            "target sync @ 84500 loss 3.685839692479931e-05\n",
            "[BUY] ep=170/200 reward=0.9925 eps=0.050 total_steps=85000 learn_steps=84763 loss=6.818062684033066e-05\n",
            "target sync @ 85000 loss 0.00019782476010732353\n",
            "[BUY] ep=171/200 reward=0.7121 eps=0.050 total_steps=85500 learn_steps=85263 loss=0.00042602443136274815\n",
            "target sync @ 85500 loss 0.0002106024039676413\n",
            "[BUY] ep=172/200 reward=0.8099 eps=0.050 total_steps=86000 learn_steps=85763 loss=0.00019816191343124956\n",
            "target sync @ 86000 loss 2.4801009203656577e-05\n",
            "[BUY] ep=173/200 reward=0.8357 eps=0.050 total_steps=86500 learn_steps=86263 loss=0.00019729042833205312\n",
            "target sync @ 86500 loss 8.517813694197685e-05\n",
            "[BUY] ep=174/200 reward=0.9115 eps=0.050 total_steps=87000 learn_steps=86763 loss=0.0007371210376732051\n",
            "target sync @ 87000 loss 0.0003142388304695487\n",
            "[BUY] ep=175/200 reward=0.5754 eps=0.050 total_steps=87500 learn_steps=87263 loss=0.00014818804629612714\n",
            "target sync @ 87500 loss 5.939289621892385e-05\n",
            "[BUY] ep=176/200 reward=0.6325 eps=0.050 total_steps=88000 learn_steps=87763 loss=8.921013795770705e-05\n",
            "target sync @ 88000 loss 3.350463884999044e-05\n",
            "[BUY] ep=177/200 reward=0.8420 eps=0.050 total_steps=88500 learn_steps=88263 loss=0.00029910923331044614\n",
            "target sync @ 88500 loss 0.00016772907110862434\n",
            "[BUY] ep=178/200 reward=0.8783 eps=0.050 total_steps=89000 learn_steps=88763 loss=0.00010708800982683897\n",
            "target sync @ 89000 loss 2.0460169253055938e-05\n",
            "[BUY] ep=179/200 reward=0.5666 eps=0.050 total_steps=89500 learn_steps=89263 loss=0.0001889672566903755\n",
            "target sync @ 89500 loss 0.00021406756422948092\n",
            "[BUY] ep=180/200 reward=0.9255 eps=0.050 total_steps=90000 learn_steps=89763 loss=0.00031911898986436427\n",
            "target sync @ 90000 loss 7.381845352938399e-05\n",
            "[BUY] ep=181/200 reward=0.7851 eps=0.050 total_steps=90500 learn_steps=90263 loss=0.0006321782129816711\n",
            "target sync @ 90500 loss 0.00012937902647536248\n",
            "[BUY] ep=182/200 reward=0.5706 eps=0.050 total_steps=91000 learn_steps=90763 loss=8.030372555367649e-05\n",
            "target sync @ 91000 loss 0.0010512208100408316\n",
            "[BUY] ep=183/200 reward=0.8000 eps=0.050 total_steps=91500 learn_steps=91263 loss=0.00032549723982810974\n",
            "target sync @ 91500 loss 1.9212038750993088e-05\n",
            "[BUY] ep=184/200 reward=0.6371 eps=0.050 total_steps=92000 learn_steps=91763 loss=1.2498705473262817e-05\n",
            "target sync @ 92000 loss 3.0068404157646e-05\n",
            "[BUY] ep=185/200 reward=0.5736 eps=0.050 total_steps=92500 learn_steps=92263 loss=0.0001022459618980065\n",
            "target sync @ 92500 loss 0.0002273049612995237\n",
            "[BUY] ep=186/200 reward=0.6844 eps=0.050 total_steps=93000 learn_steps=92763 loss=0.0003307716397102922\n",
            "target sync @ 93000 loss 2.0877345377812162e-05\n",
            "[BUY] ep=187/200 reward=0.8400 eps=0.050 total_steps=93500 learn_steps=93263 loss=0.0002030572504736483\n",
            "target sync @ 93500 loss 6.202530494192615e-05\n",
            "[BUY] ep=188/200 reward=1.0861 eps=0.050 total_steps=94000 learn_steps=93763 loss=0.0001772327086655423\n",
            "target sync @ 94000 loss 4.5894470531493425e-05\n",
            "[BUY] ep=189/200 reward=0.8890 eps=0.050 total_steps=94500 learn_steps=94263 loss=0.00035291945096105337\n",
            "target sync @ 94500 loss 0.00032903830287978053\n",
            "[BUY] ep=190/200 reward=0.7365 eps=0.050 total_steps=95000 learn_steps=94763 loss=0.0005276965093798935\n",
            "target sync @ 95000 loss 0.0006018135463818908\n",
            "[BUY] ep=191/200 reward=0.8279 eps=0.050 total_steps=95500 learn_steps=95263 loss=4.2181422031717375e-05\n",
            "target sync @ 95500 loss 0.0002794445608742535\n",
            "[BUY] ep=192/200 reward=0.9265 eps=0.050 total_steps=96000 learn_steps=95763 loss=0.00024881615536287427\n",
            "target sync @ 96000 loss 8.717938180780038e-05\n",
            "[BUY] ep=193/200 reward=0.6029 eps=0.050 total_steps=96500 learn_steps=96263 loss=4.290192009648308e-05\n",
            "target sync @ 96500 loss 2.656703509273939e-05\n",
            "[BUY] ep=194/200 reward=0.8143 eps=0.050 total_steps=97000 learn_steps=96763 loss=0.00033829055610112846\n",
            "target sync @ 97000 loss 0.00010361488966736943\n",
            "[BUY] ep=195/200 reward=0.8881 eps=0.050 total_steps=97500 learn_steps=97263 loss=0.0006161680794321001\n",
            "target sync @ 97500 loss 6.0796490288339555e-05\n",
            "[BUY] ep=196/200 reward=0.7350 eps=0.050 total_steps=98000 learn_steps=97763 loss=3.122690031887032e-05\n",
            "target sync @ 98000 loss 1.877188333310187e-05\n",
            "[BUY] ep=197/200 reward=0.7278 eps=0.050 total_steps=98500 learn_steps=98263 loss=0.0005949863698333502\n",
            "target sync @ 98500 loss 4.37845301348716e-05\n",
            "[BUY] ep=198/200 reward=1.0662 eps=0.050 total_steps=99000 learn_steps=98763 loss=0.0001036655085044913\n",
            "target sync @ 99000 loss 0.00038687195046804845\n",
            "[BUY] ep=199/200 reward=1.0213 eps=0.050 total_steps=99500 learn_steps=99263 loss=1.612064261280466e-05\n",
            "target sync @ 99500 loss 0.0002286601229570806\n",
            "[BUY] ep=200/200 reward=0.6838 eps=0.050 total_steps=100000 learn_steps=99763 loss=0.00014935700164642185\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for ep in range(int(cfg.training.episodes)):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    ep_reward = 0.0\n",
        "    steps = 0\n",
        "    loss = None\n",
        "\n",
        "    while not done:\n",
        "        a = agent.select_action(s, greedy=False)\n",
        "        ns, r, done, info = env.step(a)\n",
        "\n",
        "        agent.push(s, a, r, ns, done)\n",
        "\n",
        "        # âœ… warmup based on env steps (or buffer size), then train\n",
        "        if agent.total_steps >= int(cfg.training.warmup_steps):\n",
        "            loss = agent.update()\n",
        "\n",
        "        s = ns\n",
        "        ep_reward += float(r)\n",
        "        steps += 1\n",
        "\n",
        "        if cfg.training.steps_per_episode is not None and steps >= int(cfg.training.steps_per_episode):\n",
        "            break\n",
        "\n",
        "    if (ep + 1) % int(cfg.training.log_every) == 0:\n",
        "        last_loss = agent.loss_history[-1] if agent.loss_history else None\n",
        "        print(\n",
        "            f\"[BUY] ep={ep+1}/{cfg.training.episodes} \"\n",
        "            f\"reward={ep_reward:.4f} eps={agent.eps:.3f} \"\n",
        "            f\"total_steps={agent.total_steps} learn_steps={agent.learn_steps} \"\n",
        "            f\"loss={last_loss}\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af6dfdf7",
      "metadata": {},
      "source": [
        "# Greedy true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b9c55c12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# def eval_buy_agent(env, agent, n_episodes=10):\n",
        "#     rewards = []\n",
        "#     opens = []\n",
        "#     closes = []\n",
        "#     buy_actions = []\n",
        "#     steps_counts = []\n",
        "\n",
        "#     for ep in range(n_episodes):\n",
        "#         s = env.reset()\n",
        "#         done = False\n",
        "#         ep_reward = 0.0\n",
        "#         steps = 0\n",
        "\n",
        "#         n_open = 0\n",
        "#         n_close = 0\n",
        "#         n_buy_action = 0\n",
        "\n",
        "#         while not done:\n",
        "#             a = agent.select_action(s, greedy=True)\n",
        "#             if int(a) == 1:\n",
        "#                 n_buy_action += 1\n",
        "\n",
        "#             ns, r, done, info = env.step(a)\n",
        "\n",
        "#             if info.get(\"opened\", False):\n",
        "#                 n_open += 1\n",
        "#             if info.get(\"closed\", False):\n",
        "#                 n_close += 1\n",
        "\n",
        "#             s = ns\n",
        "#             ep_reward += float(r)\n",
        "#             steps += 1\n",
        "\n",
        "#             if hasattr(cfg.training, \"steps_per_episode\") and cfg.training.steps_per_episode is not None:\n",
        "#                 if steps >= int(cfg.training.steps_per_episode):\n",
        "#                     break\n",
        "\n",
        "#         rewards.append(ep_reward)\n",
        "#         opens.append(n_open)\n",
        "#         closes.append(n_close)\n",
        "#         buy_actions.append(n_buy_action)\n",
        "#         steps_counts.append(steps)\n",
        "\n",
        "#     rewards = np.array(rewards, dtype=float)\n",
        "\n",
        "#     print(\"=== BUY GREEDY EVAL (FIXED METRICS) ===\")\n",
        "#     print(\"episodes:\", n_episodes)\n",
        "#     print(\"reward mean:\", rewards.mean(), \"median:\", np.median(rewards), \"min/max:\", rewards.min(), rewards.max())\n",
        "#     print(\"avg opened trades/episode:\", np.mean(opens), \"min/max:\", np.min(opens), np.max(opens))\n",
        "#     print(\"avg closed trades/episode:\", np.mean(closes), \"min/max:\", np.min(closes), np.max(closes))\n",
        "#     print(\"avg BUY actions/episode:\", np.mean(buy_actions), \"min/max:\", np.min(buy_actions), np.max(buy_actions))\n",
        "#     print(\"avg steps/episode:\", np.mean(steps_counts))\n",
        "\n",
        "#     return rewards, np.array(opens), np.array(closes), np.array(buy_actions)\n",
        "\n",
        "# _ = eval_buy_agent(env, agent, n_episodes=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "243572d3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/jacopo/Downloads/clean_trading_rl/agents/ddqn_agent.py\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "from agents.ddqn_agent import DDQNAgent\n",
        "print(inspect.getfile(DDQNAgent))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model: runs/20260113-232303/buy_agent.pt\n",
            "Diagnostics: {'line': 'runs/20260113-232303/q_gap_buy.png', 'hist': 'runs/20260113-232303/q_gap_buy_hist.png'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Save model + diagnostics\n",
        "model_path = os.path.join(run_path, \"buy_agent.pt\")\n",
        "agent.save(model_path)\n",
        "\n",
        "gaps = compute_q_gap(agent, features, max_points=2000)\n",
        "paths = plot_q_gap(gaps, run_path, tag=\"buy\")\n",
        "\n",
        "print(\"Saved model:\", model_path)\n",
        "print(\"Diagnostics:\", paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "87c7a72d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mean_delta(trades, prices, horizon, tc):\n",
        "    deltas = []\n",
        "\n",
        "    for tr in trades:\n",
        "        if tr[\"meta\"].get(\"reason\") != \"sell_agent\":\n",
        "            continue\n",
        "\n",
        "        entry = tr[\"entry_idx\"]\n",
        "        exit_sell = tr[\"exit_idx\"]\n",
        "\n",
        "        horizon_exit = min(entry + horizon, len(prices) - 1)\n",
        "\n",
        "        entry_price = prices[entry]\n",
        "        horizon_price = prices[horizon_exit]\n",
        "\n",
        "        gross_horizon = (horizon_price - entry_price) / (entry_price + 1e-12)\n",
        "        net_horizon = ((1 - tc) ** 2) * (1 + gross_horizon) - 1\n",
        "\n",
        "        delta = tr[\"net_return\"] - net_horizon\n",
        "        deltas.append(delta)\n",
        "\n",
        "    return {\n",
        "        \"count\": len(deltas),\n",
        "        \"mean_delta\": float(np.mean(deltas)) if deltas else 0.0,\n",
        "        \"median_delta\": float(np.median(deltas)) if deltas else 0.0,\n",
        "        \"win_rate\": float(np.mean(np.array(deltas) > 0)) if deltas else 0.0,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f948f3",
      "metadata": {},
      "source": [
        "# TradeManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "63d0a92c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA SPLIT ===\n",
            "features: (6195, 10) prices: (6195,)\n",
            "SEG_LEN: 1239 N_SEGS: 5 TRAIN_FRAC: 0.7\n",
            "train_len per seg: 867 test_len per seg: 372\n",
            "X_train: (4335, 10) p_train: (4335,)\n",
            "X_test : (1860, 10) p_test : (1860,)\n",
            "Exit reasons: {'time': 142}\n",
            "Non-time exits: []\n",
            "ENTRY DEBUG: {'checked': 927, 'blocked_trend': 759, 'blocked_latest_entry': 11, 'blocked_conf': 15, 'opened': 142, 'conf_min': 0.34276009760483894, 'conf_max': 0.6326474680829068}\n",
            "SELL DEBUG: {'seen': 0, 'sell_actions': 0}\n",
            "EXIT REASONS: {'time': 142}\n",
            "\n",
            "=== TRADE MANAGER (TRAIN) ===\n",
            "n_steps: 4335\n",
            "segment_len: 867\n",
            "n_trades: 142\n",
            "final_equity: 17.20197363230212\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.024444396835919986\n",
            "win rate: 0.6197183098591549\n",
            "min/median/max net: -0.2827455067578124 0.0289863758166109 0.39307612553170723\n",
            "median hold bars: 20.0\n",
            "top 5 net: [0.21719871 0.22808477 0.24166434 0.29425715 0.39307613]\n",
            "bottom 5 net: [-0.28274551 -0.21338848 -0.19925274 -0.16360307 -0.14082351]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 49, 'entry_price': 43.37913513183594, 'exit_price': 47.89346694946289, 'gross_return': 0.10406689307905961, 'net_return': 0.10185986335979447, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4434249997152017, 'reason': 'time'}}\n",
            "{'entry_idx': 54, 'exit_idx': 74, 'entry_price': 48.49269104003906, 'exit_price': 45.552024841308594, 'gross_return': -0.06064143143349957, 'net_return': -0.062519209212064, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.42059662428734407, 'reason': 'time'}}\n",
            "{'entry_idx': 95, 'exit_idx': 115, 'entry_price': 46.465084075927734, 'exit_price': 49.00773620605469, 'gross_return': 0.05472178046577937, 'net_return': 0.05261339162662826, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.485496671678499, 'reason': 'time'}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 4206, 'exit_idx': 4226, 'entry_price': 143.90420532226562, 'exit_price': 136.61178588867188, 'gross_return': -0.05067551304190676, 'net_return': -0.05257321269133597, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.47810427643921516, 'reason': 'time'}}\n",
            "{'entry_idx': 4232, 'exit_idx': 4252, 'entry_price': 140.53018188476562, 'exit_price': 131.43641662597656, 'gross_return': -0.06471040695190917, 'net_return': -0.06658005084841234, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4886996732506806, 'reason': 'time'}}\n",
            "{'entry_idx': 4261, 'exit_idx': 4281, 'entry_price': 137.2385711669922, 'exit_price': 123.87850952148438, 'gross_return': -0.09734917473930244, 'net_return': -0.09915357373899858, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4819703375121182, 'reason': 'time'}}\n",
            "Exit reasons: {'time': 58}\n",
            "Non-time exits: []\n",
            "ENTRY DEBUG: {'checked': 468, 'blocked_trend': 376, 'blocked_latest_entry': 26, 'blocked_conf': 8, 'opened': 58, 'conf_min': 0.34916556097223117, 'conf_max': 0.5720961939810026}\n",
            "SELL DEBUG: {'seen': 0, 'sell_actions': 0}\n",
            "EXIT REASONS: {'time': 58}\n",
            "\n",
            "=== TRADE MANAGER (TEST) ===\n",
            "n_steps: 1860\n",
            "segment_len: 372\n",
            "n_trades: 58\n",
            "final_equity: 1.7408945638991993\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.01516869620831786\n",
            "win rate: 0.5862068965517241\n",
            "min/median/max net: -0.2515424823319645 0.01615260210626468 0.32334161900060887\n",
            "median hold bars: 20.0\n",
            "top 5 net: [0.1778045  0.18209916 0.24502303 0.30975361 0.32334162]\n",
            "bottom 5 net: [-0.25154248 -0.14124071 -0.13996183 -0.13894734 -0.12592149]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 49, 'entry_price': 168.69102478027344, 'exit_price': 151.9320831298828, 'gross_return': -0.09934696687165007, 'net_return': -0.10114737228487358, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.45140584156165353, 'reason': 'time'}}\n",
            "{'entry_idx': 78, 'exit_idx': 98, 'entry_price': 153.17129516601562, 'exit_price': 142.07659912109375, 'gross_return': -0.07243325867876713, 'net_return': -0.07428746459466828, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4694508576365352, 'reason': 'time'}}\n",
            "{'entry_idx': 103, 'exit_idx': 123, 'entry_price': 144.45079040527344, 'exit_price': 124.4820327758789, 'gross_return': -0.13823917178555914, 'net_return': -0.13996183168115972, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4886996732506806, 'reason': 'time'}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 1773, 'exit_idx': 1793, 'entry_price': 130.0275421142578, 'exit_price': 127.60585021972656, 'gross_return': -0.018624453366989296, 'net_return': -0.020586223084708588, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5026480978488047, 'reason': 'time'}}\n",
            "{'entry_idx': 1804, 'exit_idx': 1824, 'entry_price': 139.49600219726562, 'exit_price': 130.85133361816406, 'gross_return': -0.061970726350112935, 'net_return': -0.06384584686813899, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4965437307131889, 'reason': 'time'}}\n",
            "{'entry_idx': 1832, 'exit_idx': 1852, 'entry_price': 135.22824096679688, 'exit_price': 135.62527465820312, 'gross_return': 0.0029360264436459863, 'net_return': 0.0009311573267851703, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4527240995917555, 'reason': 'time'}}\n",
            "\n",
            "=== ENTRY HARVEST (TRAIN) ===\n",
            "topk_per_segment: 80 min_gap: None use_conf_score: False\n",
            "n_entries: 246\n",
            "horizon: 20 segment_len: 867 feasible_in_segment: True\n",
            "first 10: [34, 48, 62, 72, 96, 108, 118, 132, 145, 163]\n",
            "last 10 : [4109, 4125, 4137, 4157, 4168, 4178, 4208, 4232, 4261, 4271]\n",
            "\n",
            "=== ENTRY HARVEST (TEST) ===\n",
            "topk_per_segment: 40 min_gap: None use_conf_score: False\n",
            "n_entries: 94\n",
            "horizon: 20 segment_len: 372 feasible_in_segment: True\n",
            "first 10: [34, 83, 95, 135, 145, 155, 172, 190, 200, 219]\n",
            "last 10 : [1699, 1712, 1722, 1748, 1759, 1769, 1780, 1794, 1811, 1838]\n",
            "\n",
            "=== SAVED ===\n",
            " - runs/entry_indices_train.npy (HARVESTED)\n",
            " - runs/entry_indices_test.npy (HARVESTED)\n",
            " - runs/trades_buy_only_train.json\n",
            " - runs/trades_buy_only_test.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from trade.trade_manager import TradeManager\n",
        "\n",
        "# -----------------------\n",
        "# SETTINGS\n",
        "# -----------------------\n",
        "SEG_LEN = 1239          # rows per ticker segment from your build_features validation\n",
        "N_SEGS  = 5             # AAPL, MSFT, NVDA, AMZN, GOOGL\n",
        "TRAIN_FRAC = 0.70       # time-based split within each segment\n",
        "\n",
        "# NEW: entry harvesting (for SellAgent training)\n",
        "TOPK_PER_SEG_TRAIN = 80     # try 50â€“150\n",
        "TOPK_PER_SEG_TEST  = 40     # fewer is fine for eval\n",
        "MIN_GAP_TRAIN = None        # None => defaults inside TradeManager\n",
        "MIN_GAP_TEST  = None\n",
        "USE_CONF_SCORE = False      # False => uses q1-q0 margin (recommended)\n",
        "\n",
        "# -----------------------\n",
        "# BUILD TRAIN/TEST INDEX (per segment, no leakage)\n",
        "# -----------------------\n",
        "train_len = int(SEG_LEN * TRAIN_FRAC)\n",
        "\n",
        "train_idx = []\n",
        "test_idx = []\n",
        "\n",
        "for seg in range(N_SEGS):\n",
        "    start = seg * SEG_LEN\n",
        "    train_idx.extend(range(start, start + train_len))\n",
        "    test_idx.extend(range(start + train_len, start + SEG_LEN))\n",
        "\n",
        "train_idx = np.array(train_idx, dtype=np.int32)\n",
        "test_idx  = np.array(test_idx, dtype=np.int32)\n",
        "\n",
        "X_train = features[train_idx]\n",
        "p_train = prices[train_idx]\n",
        "X_test  = features[test_idx]\n",
        "p_test  = prices[test_idx]\n",
        "\n",
        "# Segment length inside each split subset (since we concatenated segments in order)\n",
        "SEG_TRAIN = train_len\n",
        "SEG_TEST  = SEG_LEN - train_len\n",
        "\n",
        "print(\"=== DATA SPLIT ===\")\n",
        "print(\"features:\", features.shape, \"prices:\", prices.shape)\n",
        "print(\"SEG_LEN:\", SEG_LEN, \"N_SEGS:\", N_SEGS, \"TRAIN_FRAC:\", TRAIN_FRAC)\n",
        "print(\"train_len per seg:\", SEG_TRAIN, \"test_len per seg:\", SEG_TEST)\n",
        "print(\"X_train:\", X_train.shape, \"p_train:\", p_train.shape)\n",
        "print(\"X_test :\", X_test.shape,  \"p_test :\", p_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# HELPER: run TM + debug logs (unchanged backtest)\n",
        "# -----------------------\n",
        "def run_tm(name: str, X: np.ndarray, p: np.ndarray, seg_len: int, sell_agent=None):\n",
        "    tm = TradeManager(\n",
        "        buy_agent=agent,            # trained buy agent\n",
        "        sell_agent=None,      # optional\n",
        "        state=X,\n",
        "        prices=p,\n",
        "        reward=cfg.reward,\n",
        "        trade=cfg.trade_manager,\n",
        "        segment_len=seg_len,        # IMPORTANT for boundary correctness\n",
        "    )\n",
        "\n",
        "    res = tm.run()\n",
        "    trades = res[\"trades\"]\n",
        "\n",
        "    from collections import Counter\n",
        "    reasons = Counter([t[\"meta\"].get(\"reason\", \"none\") for t in res[\"trades\"]])\n",
        "    print(\"Exit reasons:\", dict(reasons))\n",
        "    print(\"Non-time exits:\", [t for t in res[\"trades\"] if t[\"meta\"].get(\"reason\") != \"time\"][:3])\n",
        "\n",
        "    print(\"ENTRY DEBUG:\", res[\"entry_debug\"])\n",
        "    print(\"SELL DEBUG:\", res[\"sell_debug\"])\n",
        "    print(\"EXIT REASONS:\", res.get(\"exit_reasons\"))\n",
        "\n",
        "\n",
        "    print(f\"\\n=== TRADE MANAGER ({name}) ===\")\n",
        "    print(\"n_steps:\", len(p))\n",
        "    print(\"segment_len:\", seg_len)\n",
        "    print(\"n_trades:\", res[\"n_trades\"])\n",
        "    print(\"final_equity:\", res[\"final_equity\"])\n",
        "\n",
        "    # Boundary-crossing check (must be 0)\n",
        "    if trades:\n",
        "        cross = sum((t[\"entry_idx\"] // seg_len) != (t[\"exit_idx\"] // seg_len) for t in trades)\n",
        "    else:\n",
        "        cross = 0\n",
        "    print(\"Trades crossing segment boundary:\", cross)\n",
        "\n",
        "    # Return stats\n",
        "    if trades:\n",
        "        net = np.array([t[\"net_return\"] for t in trades], dtype=float)\n",
        "        hold = np.array([t[\"hold_bars\"] for t in trades], dtype=float)\n",
        "\n",
        "        print(\"avg net return:\", float(net.mean()))\n",
        "        print(\"win rate:\", float((net > 0).mean()))\n",
        "        print(\"min/median/max net:\", float(net.min()), float(np.median(net)), float(net.max()))\n",
        "        print(\"median hold bars:\", float(np.median(hold)))\n",
        "        print(\"top 5 net:\", np.sort(net)[-5:])\n",
        "        print(\"bottom 5 net:\", np.sort(net)[:5])\n",
        "\n",
        "        # A few sample trades (head + tail)\n",
        "        print(\"\\nSample trades (first 3):\")\n",
        "        for t in trades[:3]:\n",
        "            print(t)\n",
        "        print(\"\\nSample trades (last 3):\")\n",
        "        for t in trades[-3:]:\n",
        "            print(t)\n",
        "    else:\n",
        "        print(\"No trades produced. Try lowering buy_min_confidence or disabling trend filter.\")\n",
        "\n",
        "    return tm, res\n",
        "\n",
        "# -----------------------\n",
        "# NEW: Harvest entry indices for SellAgent training (no trade execution)\n",
        "# -----------------------\n",
        "def harvest_entries(name: str, tm: TradeManager, topk_per_seg: int, min_gap=None, use_confidence_score=False):\n",
        "    entries = tm.collect_entry_indices_topk(\n",
        "        topk_per_segment=topk_per_seg,\n",
        "        min_gap=min_gap,\n",
        "        use_confidence_score=use_confidence_score,\n",
        "    )\n",
        "    entries = np.array(entries, dtype=np.int32)\n",
        "\n",
        "    # Quick sanity: segment boundary + horizon feasibility check (should hold by construction)\n",
        "    horizon = int(cfg.trade_manager.sell_horizon)\n",
        "    if len(entries) > 0:\n",
        "        seg_ok = np.all((entries % tm.segment_len) <= (tm.segment_len - 1 - horizon))\n",
        "    else:\n",
        "        seg_ok = True\n",
        "\n",
        "    print(f\"\\n=== ENTRY HARVEST ({name}) ===\")\n",
        "    print(\"topk_per_segment:\", topk_per_seg, \"min_gap:\", min_gap, \"use_conf_score:\", use_confidence_score)\n",
        "    print(\"n_entries:\", len(entries))\n",
        "    print(\"horizon:\", horizon, \"segment_len:\", tm.segment_len, \"feasible_in_segment:\", bool(seg_ok))\n",
        "    if len(entries) > 0:\n",
        "        print(\"first 10:\", entries[:10].tolist())\n",
        "        print(\"last 10 :\", entries[-10:].tolist())\n",
        "\n",
        "    return entries\n",
        "\n",
        "# -----------------------\n",
        "# RUN TRAIN + TEST (backtest as before)\n",
        "# -----------------------\n",
        "tm_train, res_train = run_tm(\"TRAIN\", X_train, p_train, seg_len=SEG_TRAIN, sell_agent=None)\n",
        "tm_test,  res_test  = run_tm(\"TEST\",  X_test,  p_test,  seg_len=SEG_TEST,  sell_agent=None)\n",
        "\n",
        "# -----------------------\n",
        "# HARVEST ENTRIES (NEW LOGIC) â€” use these for SellEnv training\n",
        "# -----------------------\n",
        "train_entries = harvest_entries(\n",
        "    \"TRAIN\",\n",
        "    tm_train,\n",
        "    topk_per_seg=TOPK_PER_SEG_TRAIN,\n",
        "    min_gap=MIN_GAP_TRAIN,\n",
        "    use_confidence_score=USE_CONF_SCORE,\n",
        ")\n",
        "\n",
        "test_entries = harvest_entries(\n",
        "    \"TEST\",\n",
        "    tm_test,\n",
        "    topk_per_seg=TOPK_PER_SEG_TEST,\n",
        "    min_gap=MIN_GAP_TEST,\n",
        "    use_confidence_score=USE_CONF_SCORE,\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# SAVE ARTIFACTS (into out_dir)\n",
        "# -----------------------\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "train_entries_path = os.path.join(out_dir, \"entry_indices_train.npy\")\n",
        "test_entries_path  = os.path.join(out_dir, \"entry_indices_test.npy\")\n",
        "\n",
        "# NEW: save harvested entries (not trade entries)\n",
        "np.save(train_entries_path, train_entries)\n",
        "np.save(test_entries_path,  test_entries)\n",
        "\n",
        "train_trades_json = os.path.join(out_dir, \"trades_buy_only_train.json\")\n",
        "test_trades_json  = os.path.join(out_dir, \"trades_buy_only_test.json\")\n",
        "\n",
        "with open(train_trades_json, \"w\") as f:\n",
        "    json.dump(res_train[\"trades\"], f, indent=2)\n",
        "\n",
        "with open(test_trades_json, \"w\") as f:\n",
        "    json.dump(res_test[\"trades\"], f, indent=2)\n",
        "\n",
        "print(\"\\n=== SAVED ===\")\n",
        "print(\" -\", train_entries_path, \"(HARVESTED)\")\n",
        "print(\" -\", test_entries_path,  \"(HARVESTED)\")\n",
        "print(\" -\", train_trades_json)\n",
        "print(\" -\", test_trades_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d5aa5cee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST final_equity: 1.7408945638991993\n",
            "TEST n_trades: 58\n",
            "TEST avg net: 0.01516869620831786\n",
            "TEST win rate: 0.5862068965517241\n",
            "TEST min/median/max: (np.float64(-0.2515424823319645), np.float64(0.01615260210626468), np.float64(0.32334161900060887))\n"
          ]
        }
      ],
      "source": [
        "print(\"TEST final_equity:\", res_test[\"final_equity\"])\n",
        "print(\"TEST n_trades:\", res_test[\"n_trades\"])\n",
        "\n",
        "trades = res_test[\"trades\"]\n",
        "net = np.array([t[\"net_return\"] for t in trades], dtype=float) if trades else np.array([])\n",
        "print(\"TEST avg net:\", net.mean() if len(net) else None)\n",
        "print(\"TEST win rate:\", (net > 0).mean() if len(net) else None)\n",
        "print(\"TEST min/median/max:\", (net.min(), np.median(net), net.max()) if len(net) else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a420c7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def filter_entries(entries, seg_len, horizon, n):\n",
        "#     kept = []\n",
        "#     drop_oob = 0\n",
        "#     drop_seg = 0\n",
        "#     drop_horizon = 0\n",
        "\n",
        "#     for e in entries:\n",
        "#         e = int(e)\n",
        "#         if e < 0 or e >= n:\n",
        "#             drop_oob += 1\n",
        "#             continue\n",
        "\n",
        "#         # segment end (inclusive)\n",
        "#         seg_end = min(((e // seg_len) + 1) * seg_len - 1, n - 1)\n",
        "\n",
        "#         # need room for at least horizon bars INSIDE segment\n",
        "#         last_allowed = min(e + horizon, seg_end, n - 1)\n",
        "\n",
        "#         # if episode would immediately be at/over last_allowed, it's useless\n",
        "#         # (or you can make this stricter: require at least 1 step)\n",
        "#         if last_allowed <= e:\n",
        "#             drop_horizon += 1\n",
        "#             continue\n",
        "\n",
        "#         kept.append(e)\n",
        "\n",
        "#     kept = np.array(sorted(set(kept)), dtype=np.int32)\n",
        "\n",
        "#     print(\"entries raw:\", len(entries))\n",
        "#     print(\"entries kept:\", len(kept))\n",
        "#     print(\"dropped oob:\", drop_oob)\n",
        "#     print(\"dropped horizon/seg:\", drop_horizon)\n",
        "#     return kept\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3058fa38",
      "metadata": {},
      "source": [
        "# Sell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ec8c30cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "entries_train: (246,) entries_test: (94,)\n",
            "sell include_pos: True\n",
            "sell feat_dim: 10 state_dim: 13\n",
            "Sell Decay Steps: 60000 / Est Total: 75000\n",
            "SELL state_dim: 13 n_actions: 2\n",
            "[SELL] ep=10/4000 reward=0.0214 eps=0.998 loss=None\n",
            "[SELL] ep=20/4000 reward=0.0034 eps=0.996 loss=None\n",
            "[SELL] ep=30/4000 reward=-0.0287 eps=0.994 loss=0.001245988765731454\n",
            "[SELL] ep=40/4000 reward=-0.1032 eps=0.992 loss=0.0013697852846235037\n",
            "[SELL] ep=50/4000 reward=0.1679 eps=0.990 loss=0.00129480822943151\n",
            "[SELL] ep=60/4000 reward=-0.0588 eps=0.989 loss=0.0021641019266098738\n",
            "target sync @ 500 loss 0.0017683457117527723\n",
            "[SELL] ep=70/4000 reward=-0.1673 eps=0.987 loss=0.0003001574950758368\n",
            "[SELL] ep=80/4000 reward=0.1228 eps=0.985 loss=0.0002629755763337016\n",
            "[SELL] ep=90/4000 reward=-0.0421 eps=0.983 loss=0.00031343541922979057\n",
            "[SELL] ep=100/4000 reward=-0.0691 eps=0.981 loss=0.0005182377062737942\n",
            "target sync @ 1000 loss 0.0002673504641279578\n",
            "[SELL] ep=110/4000 reward=0.0842 eps=0.979 loss=0.0001980536908376962\n",
            "[SELL] ep=120/4000 reward=0.0326 eps=0.977 loss=0.00029150943737477064\n",
            "[SELL] ep=130/4000 reward=0.0514 eps=0.975 loss=0.0005583980237133801\n",
            "[SELL] ep=140/4000 reward=0.0137 eps=0.973 loss=0.00034878498991020024\n",
            "target sync @ 1500 loss 0.00027927360497415066\n",
            "[SELL] ep=150/4000 reward=-0.0173 eps=0.972 loss=0.0003035353438463062\n",
            "[SELL] ep=160/4000 reward=0.0739 eps=0.970 loss=0.0002582843299023807\n",
            "[SELL] ep=170/4000 reward=-0.0070 eps=0.968 loss=0.00019853396224789321\n",
            "[SELL] ep=180/4000 reward=-0.0920 eps=0.966 loss=0.00023018167121335864\n",
            "target sync @ 2000 loss 0.0005953413783572614\n",
            "[SELL] ep=190/4000 reward=-0.0411 eps=0.964 loss=0.0002252112899441272\n",
            "[SELL] ep=200/4000 reward=0.0155 eps=0.962 loss=0.00022920849733054638\n",
            "[SELL] ep=210/4000 reward=0.1679 eps=0.960 loss=0.00024242422659881413\n",
            "[SELL] ep=220/4000 reward=0.0554 eps=0.958 loss=0.00023138028336688876\n",
            "target sync @ 2500 loss 0.00026222842279821634\n",
            "[SELL] ep=230/4000 reward=-0.0079 eps=0.956 loss=0.00034255447098985314\n",
            "[SELL] ep=240/4000 reward=0.0441 eps=0.955 loss=0.0006334861973300576\n",
            "[SELL] ep=250/4000 reward=-0.0937 eps=0.953 loss=0.0001627626334084198\n",
            "[SELL] ep=260/4000 reward=0.0627 eps=0.951 loss=0.00038506544660776854\n",
            "target sync @ 3000 loss 0.00041216262616217136\n",
            "[SELL] ep=270/4000 reward=-0.2272 eps=0.949 loss=0.0005199505831114948\n",
            "[SELL] ep=280/4000 reward=-0.0602 eps=0.947 loss=0.00015577506565023214\n",
            "[SELL] ep=290/4000 reward=-0.0634 eps=0.945 loss=0.00012943471665494144\n",
            "[SELL] ep=300/4000 reward=-0.0070 eps=0.943 loss=0.0005000059027224779\n",
            "[SELL] ep=310/4000 reward=0.0231 eps=0.941 loss=0.0003274705377407372\n",
            "target sync @ 3500 loss 0.0002341310027986765\n",
            "[SELL] ep=320/4000 reward=0.0301 eps=0.939 loss=0.00032050011213868856\n",
            "[SELL] ep=330/4000 reward=0.0914 eps=0.937 loss=0.00013682538701687008\n",
            "[SELL] ep=340/4000 reward=-0.0215 eps=0.936 loss=0.00015966784849297255\n",
            "[SELL] ep=350/4000 reward=-0.0074 eps=0.934 loss=0.00014227523934096098\n",
            "target sync @ 4000 loss 0.00017693289555609226\n",
            "[SELL] ep=360/4000 reward=0.0715 eps=0.932 loss=0.00013572408352047205\n",
            "[SELL] ep=370/4000 reward=0.0378 eps=0.930 loss=0.0001949666766449809\n",
            "[SELL] ep=380/4000 reward=-0.0303 eps=0.928 loss=0.00018398559768684208\n",
            "[SELL] ep=390/4000 reward=-0.0111 eps=0.926 loss=0.00021250234567560256\n",
            "target sync @ 4500 loss 0.00028107501566410065\n",
            "[SELL] ep=400/4000 reward=-0.0006 eps=0.924 loss=0.000310658710077405\n",
            "[SELL] ep=410/4000 reward=-0.0889 eps=0.922 loss=0.00014589820057153702\n",
            "[SELL] ep=420/4000 reward=0.0153 eps=0.920 loss=0.00020674627739936113\n",
            "[SELL] ep=430/4000 reward=0.0055 eps=0.918 loss=0.00011027598520740867\n",
            "target sync @ 5000 loss 0.00019138689094688743\n",
            "[SELL] ep=440/4000 reward=0.0387 eps=0.916 loss=0.0001405902876285836\n",
            "[SELL] ep=450/4000 reward=0.0040 eps=0.914 loss=0.0002546018804423511\n",
            "[SELL] ep=460/4000 reward=0.0243 eps=0.912 loss=0.0001252235670108348\n",
            "[SELL] ep=470/4000 reward=-0.0473 eps=0.910 loss=0.00019158146460540593\n",
            "target sync @ 5500 loss 0.00015672252629883587\n",
            "[SELL] ep=480/4000 reward=0.0231 eps=0.908 loss=0.00017098983516916633\n",
            "[SELL] ep=490/4000 reward=-0.0372 eps=0.906 loss=0.0001801867038011551\n",
            "[SELL] ep=500/4000 reward=-0.0667 eps=0.904 loss=0.00015394175716210157\n",
            "[SELL] ep=510/4000 reward=-0.0333 eps=0.902 loss=9.642997611081228e-05\n",
            "target sync @ 6000 loss 0.0001161361433332786\n",
            "[SELL] ep=520/4000 reward=0.1567 eps=0.900 loss=0.00011999774142168462\n",
            "[SELL] ep=530/4000 reward=-0.0025 eps=0.899 loss=0.00017637161363381892\n",
            "[SELL] ep=540/4000 reward=-0.0070 eps=0.897 loss=0.00014967132301535457\n",
            "[SELL] ep=550/4000 reward=-0.0230 eps=0.895 loss=0.0003228713176213205\n",
            "target sync @ 6500 loss 7.301328150788322e-05\n",
            "[SELL] ep=560/4000 reward=-0.0091 eps=0.893 loss=9.370350744575262e-05\n",
            "[SELL] ep=570/4000 reward=-0.0462 eps=0.891 loss=8.076499943854287e-05\n",
            "[SELL] ep=580/4000 reward=-0.0339 eps=0.889 loss=0.0001194897122331895\n",
            "[SELL] ep=590/4000 reward=-0.0560 eps=0.887 loss=0.00013383755867835134\n",
            "target sync @ 7000 loss 0.00010812312393682078\n",
            "[SELL] ep=600/4000 reward=-0.0356 eps=0.885 loss=0.00011708639067364857\n",
            "[SELL] ep=610/4000 reward=0.1195 eps=0.883 loss=0.0001249820925295353\n",
            "[SELL] ep=620/4000 reward=-0.0138 eps=0.881 loss=0.00014799923519603908\n",
            "[SELL] ep=630/4000 reward=-0.0133 eps=0.879 loss=0.0002626521745696664\n",
            "target sync @ 7500 loss 8.481789700454101e-05\n",
            "[SELL] ep=640/4000 reward=-0.0582 eps=0.877 loss=0.00022701422858517617\n",
            "[SELL] ep=650/4000 reward=-0.0194 eps=0.875 loss=0.00012660049833357334\n",
            "[SELL] ep=660/4000 reward=0.0040 eps=0.873 loss=7.09425876266323e-05\n",
            "[SELL] ep=670/4000 reward=0.0322 eps=0.872 loss=0.00020000248332507908\n",
            "[SELL] ep=680/4000 reward=-0.0163 eps=0.870 loss=0.00011634254042292014\n",
            "target sync @ 8000 loss 7.509520946769044e-05\n",
            "[SELL] ep=690/4000 reward=0.0753 eps=0.868 loss=0.00011241054744459689\n",
            "[SELL] ep=700/4000 reward=0.1816 eps=0.866 loss=6.108144589234143e-05\n",
            "[SELL] ep=710/4000 reward=-0.0891 eps=0.864 loss=0.0001183214844786562\n",
            "[SELL] ep=720/4000 reward=0.0384 eps=0.862 loss=0.0001236479147337377\n",
            "target sync @ 8500 loss 3.815640229731798e-05\n",
            "[SELL] ep=730/4000 reward=-0.0132 eps=0.860 loss=0.0001251446665264666\n",
            "[SELL] ep=740/4000 reward=-0.0130 eps=0.858 loss=0.00011424272088333964\n",
            "[SELL] ep=750/4000 reward=0.0254 eps=0.856 loss=0.00010580936213955283\n",
            "[SELL] ep=760/4000 reward=-0.0228 eps=0.854 loss=7.216159428935498e-05\n",
            "target sync @ 9000 loss 4.803338379133493e-05\n",
            "[SELL] ep=770/4000 reward=-0.0182 eps=0.852 loss=0.00013697490794584155\n",
            "[SELL] ep=780/4000 reward=0.0025 eps=0.850 loss=0.0002635763958096504\n",
            "[SELL] ep=790/4000 reward=-0.0889 eps=0.848 loss=0.0001807347871363163\n",
            "[SELL] ep=800/4000 reward=-0.1112 eps=0.846 loss=9.869026689557359e-05\n",
            "target sync @ 9500 loss 6.0199221479706466e-05\n",
            "[SELL] ep=810/4000 reward=0.0607 eps=0.844 loss=8.175331458915025e-05\n",
            "[SELL] ep=820/4000 reward=0.0157 eps=0.842 loss=4.1234037780668586e-05\n",
            "[SELL] ep=830/4000 reward=-0.0039 eps=0.841 loss=8.631790115032345e-05\n",
            "[SELL] ep=840/4000 reward=-0.0186 eps=0.839 loss=6.656799087068066e-05\n",
            "target sync @ 10000 loss 0.00012691931624431163\n",
            "[SELL] ep=850/4000 reward=-0.0396 eps=0.837 loss=6.421713624149561e-05\n",
            "[SELL] ep=860/4000 reward=0.0041 eps=0.835 loss=0.00012084240734111518\n",
            "[SELL] ep=870/4000 reward=-0.0614 eps=0.833 loss=3.7942456401651725e-05\n",
            "[SELL] ep=880/4000 reward=0.0493 eps=0.831 loss=9.779760148376226e-05\n",
            "target sync @ 10500 loss 0.00014319916954264045\n",
            "[SELL] ep=890/4000 reward=-0.1021 eps=0.829 loss=7.59830727474764e-05\n",
            "[SELL] ep=900/4000 reward=-0.0009 eps=0.827 loss=0.00011740949412342161\n",
            "[SELL] ep=910/4000 reward=-0.0041 eps=0.825 loss=0.000138655595947057\n",
            "[SELL] ep=920/4000 reward=0.0345 eps=0.823 loss=6.539872993016616e-05\n",
            "target sync @ 11000 loss 8.587482443545014e-05\n",
            "[SELL] ep=930/4000 reward=0.1482 eps=0.821 loss=5.542238068301231e-05\n",
            "[SELL] ep=940/4000 reward=0.0439 eps=0.819 loss=0.00011883361730724573\n",
            "[SELL] ep=950/4000 reward=-0.0220 eps=0.817 loss=0.00010942924564005807\n",
            "[SELL] ep=960/4000 reward=0.1303 eps=0.815 loss=6.309618765953928e-05\n",
            "target sync @ 11500 loss 8.574116509407759e-05\n",
            "[SELL] ep=970/4000 reward=0.0101 eps=0.813 loss=0.00012316714855842292\n",
            "[SELL] ep=980/4000 reward=0.0679 eps=0.811 loss=0.00012245708785485476\n",
            "[SELL] ep=990/4000 reward=-0.0768 eps=0.809 loss=0.00010032412683358416\n",
            "[SELL] ep=1000/4000 reward=0.0027 eps=0.807 loss=6.826755270594731e-05\n",
            "target sync @ 12000 loss 8.675060234963894e-05\n",
            "[SELL] ep=1010/4000 reward=0.0041 eps=0.805 loss=9.885711915558204e-05\n",
            "[SELL] ep=1020/4000 reward=0.0326 eps=0.804 loss=0.00013570731971412897\n",
            "[SELL] ep=1030/4000 reward=-0.0472 eps=0.802 loss=9.577738092048094e-05\n",
            "[SELL] ep=1040/4000 reward=-0.0418 eps=0.800 loss=0.00011874159827129915\n",
            "target sync @ 12500 loss 6.732642214046791e-05\n",
            "[SELL] ep=1050/4000 reward=-0.0552 eps=0.798 loss=0.00018131780961994082\n",
            "[SELL] ep=1060/4000 reward=-0.0006 eps=0.796 loss=0.0001374775165459141\n",
            "[SELL] ep=1070/4000 reward=0.0634 eps=0.794 loss=0.00010225018922938034\n",
            "[SELL] ep=1080/4000 reward=0.0067 eps=0.792 loss=0.000111378263682127\n",
            "target sync @ 13000 loss 8.895984501577914e-05\n",
            "[SELL] ep=1090/4000 reward=-0.0487 eps=0.790 loss=5.205260094953701e-05\n",
            "[SELL] ep=1100/4000 reward=-0.0093 eps=0.788 loss=5.642438554787077e-05\n",
            "[SELL] ep=1110/4000 reward=-0.0342 eps=0.786 loss=0.00012749960296787322\n",
            "[SELL] ep=1120/4000 reward=0.0084 eps=0.784 loss=6.76482159178704e-05\n",
            "target sync @ 13500 loss 7.613237539771944e-05\n",
            "[SELL] ep=1130/4000 reward=-0.0096 eps=0.782 loss=4.6758981625316665e-05\n",
            "[SELL] ep=1140/4000 reward=0.0590 eps=0.780 loss=8.773486479185522e-05\n",
            "[SELL] ep=1150/4000 reward=-0.0333 eps=0.778 loss=6.506506178993732e-05\n",
            "[SELL] ep=1160/4000 reward=-0.0488 eps=0.776 loss=0.00015617575263604522\n",
            "target sync @ 14000 loss 6.078025762690231e-05\n",
            "[SELL] ep=1170/4000 reward=-0.0040 eps=0.774 loss=0.00014383241068571806\n",
            "[SELL] ep=1180/4000 reward=-0.0190 eps=0.772 loss=0.0001051098806783557\n",
            "[SELL] ep=1190/4000 reward=0.0070 eps=0.770 loss=5.5334508942905813e-05\n",
            "[SELL] ep=1200/4000 reward=-0.0283 eps=0.768 loss=7.689521589782089e-05\n",
            "target sync @ 14500 loss 5.248401066637598e-05\n",
            "[SELL] ep=1210/4000 reward=0.0155 eps=0.766 loss=0.000145395562867634\n",
            "[SELL] ep=1220/4000 reward=-0.0661 eps=0.764 loss=0.00010206934530287981\n",
            "[SELL] ep=1230/4000 reward=0.0120 eps=0.762 loss=0.0001211138951475732\n",
            "[SELL] ep=1240/4000 reward=-0.0937 eps=0.760 loss=8.316570892930031e-05\n",
            "target sync @ 15000 loss 5.41364133823663e-05\n",
            "[SELL] ep=1250/4000 reward=-0.0065 eps=0.758 loss=0.00014012113388162106\n",
            "[SELL] ep=1260/4000 reward=0.0041 eps=0.756 loss=0.00013586624118033797\n",
            "[SELL] ep=1270/4000 reward=-0.0139 eps=0.754 loss=8.046663424465805e-05\n",
            "[SELL] ep=1280/4000 reward=0.0000 eps=0.752 loss=9.441818110644817e-05\n",
            "target sync @ 15500 loss 9.831872012000531e-05\n",
            "[SELL] ep=1290/4000 reward=-0.0291 eps=0.750 loss=0.00011440612433943897\n",
            "[SELL] ep=1300/4000 reward=-0.0523 eps=0.748 loss=6.589246913790703e-05\n",
            "[SELL] ep=1310/4000 reward=-0.0201 eps=0.746 loss=0.00014179669960867614\n",
            "[SELL] ep=1320/4000 reward=0.0442 eps=0.744 loss=0.00014942113193683326\n",
            "target sync @ 16000 loss 9.261577361030504e-05\n",
            "[SELL] ep=1330/4000 reward=0.0023 eps=0.742 loss=0.0001228629844263196\n",
            "[SELL] ep=1340/4000 reward=-0.0331 eps=0.739 loss=0.00018212533905170858\n",
            "[SELL] ep=1350/4000 reward=0.1645 eps=0.737 loss=0.000566663802601397\n",
            "[SELL] ep=1360/4000 reward=-0.0076 eps=0.735 loss=8.414647891186178e-05\n",
            "target sync @ 16500 loss 6.720510282320902e-05\n",
            "[SELL] ep=1370/4000 reward=0.0572 eps=0.733 loss=0.00015397652168758214\n",
            "[SELL] ep=1380/4000 reward=0.0000 eps=0.731 loss=8.057527156779543e-05\n",
            "[SELL] ep=1390/4000 reward=0.0205 eps=0.729 loss=0.0001132244651671499\n",
            "target sync @ 17000 loss 0.00021695077884942293\n",
            "[SELL] ep=1400/4000 reward=0.0178 eps=0.727 loss=0.00025932677090168\n",
            "[SELL] ep=1410/4000 reward=-0.0150 eps=0.725 loss=6.489688530564308e-05\n",
            "[SELL] ep=1420/4000 reward=0.0030 eps=0.723 loss=0.00018945452757179737\n",
            "[SELL] ep=1430/4000 reward=-0.0191 eps=0.721 loss=0.0001743403699947521\n",
            "target sync @ 17500 loss 8.991692448034883e-05\n",
            "[SELL] ep=1440/4000 reward=-0.0587 eps=0.719 loss=0.0004062811494804919\n",
            "[SELL] ep=1450/4000 reward=-0.0339 eps=0.717 loss=0.0001116579951485619\n",
            "[SELL] ep=1460/4000 reward=0.0027 eps=0.714 loss=9.14042757358402e-05\n",
            "[SELL] ep=1470/4000 reward=-0.0025 eps=0.713 loss=0.00016579196380916983\n",
            "target sync @ 18000 loss 0.00019488261023070663\n",
            "[SELL] ep=1480/4000 reward=0.0597 eps=0.711 loss=0.00013406755169853568\n",
            "[SELL] ep=1490/4000 reward=0.0178 eps=0.709 loss=0.00022310529311653227\n",
            "[SELL] ep=1500/4000 reward=-0.0061 eps=0.707 loss=0.0002634765696711838\n",
            "[SELL] ep=1510/4000 reward=-0.0563 eps=0.705 loss=0.00014485447900369763\n",
            "target sync @ 18500 loss 0.00013294223754201084\n",
            "[SELL] ep=1520/4000 reward=-0.2265 eps=0.703 loss=8.430240995949134e-05\n",
            "[SELL] ep=1530/4000 reward=0.0064 eps=0.701 loss=0.00020266795763745904\n",
            "[SELL] ep=1540/4000 reward=-0.0212 eps=0.699 loss=0.00013462308561429381\n",
            "[SELL] ep=1550/4000 reward=0.0443 eps=0.697 loss=0.0001767684007063508\n",
            "target sync @ 19000 loss 0.00012818681716453284\n",
            "[SELL] ep=1560/4000 reward=0.1195 eps=0.695 loss=0.00013897111057303846\n",
            "[SELL] ep=1570/4000 reward=-0.0923 eps=0.693 loss=5.8358222304377705e-05\n",
            "[SELL] ep=1580/4000 reward=0.0548 eps=0.691 loss=0.0002683897619135678\n",
            "[SELL] ep=1590/4000 reward=-0.0039 eps=0.689 loss=0.00011675544374156743\n",
            "target sync @ 19500 loss 0.0001361686154268682\n",
            "[SELL] ep=1600/4000 reward=-0.0488 eps=0.687 loss=0.00018369138706475496\n",
            "[SELL] ep=1610/4000 reward=-0.0052 eps=0.685 loss=0.00021319653023965657\n",
            "[SELL] ep=1620/4000 reward=0.0151 eps=0.683 loss=0.00018358556553721428\n",
            "[SELL] ep=1630/4000 reward=0.0430 eps=0.681 loss=6.450769433286041e-05\n",
            "target sync @ 20000 loss 0.00010566579294390976\n",
            "[SELL] ep=1640/4000 reward=-0.0077 eps=0.679 loss=0.0001391680125379935\n",
            "[SELL] ep=1650/4000 reward=-0.0031 eps=0.677 loss=0.00033112423261627555\n",
            "[SELL] ep=1660/4000 reward=-0.0311 eps=0.675 loss=0.00015857159451115876\n",
            "[SELL] ep=1670/4000 reward=0.0633 eps=0.673 loss=0.00016688162577338517\n",
            "target sync @ 20500 loss 0.00019579342915676534\n",
            "[SELL] ep=1680/4000 reward=0.0123 eps=0.671 loss=0.00010824629134731367\n",
            "[SELL] ep=1690/4000 reward=0.0272 eps=0.669 loss=0.00016704556765034795\n",
            "[SELL] ep=1700/4000 reward=0.0072 eps=0.667 loss=0.0001631846243981272\n",
            "[SELL] ep=1710/4000 reward=-0.0145 eps=0.665 loss=0.00010257672693114728\n",
            "target sync @ 21000 loss 7.290475332411006e-05\n",
            "[SELL] ep=1720/4000 reward=-0.0305 eps=0.662 loss=0.00012904599134344608\n",
            "[SELL] ep=1730/4000 reward=-0.0873 eps=0.660 loss=0.00022526306565850973\n",
            "[SELL] ep=1740/4000 reward=0.0303 eps=0.658 loss=0.00033132435055449605\n",
            "[SELL] ep=1750/4000 reward=0.0494 eps=0.656 loss=0.0001787199726095423\n",
            "target sync @ 21500 loss 0.00020456561469472945\n",
            "[SELL] ep=1760/4000 reward=0.1711 eps=0.654 loss=0.0004357521829660982\n",
            "[SELL] ep=1770/4000 reward=-0.0176 eps=0.652 loss=0.00011402629752410576\n",
            "[SELL] ep=1780/4000 reward=-0.0588 eps=0.650 loss=0.0001868589606601745\n",
            "target sync @ 22000 loss 0.00017851880693342537\n",
            "[SELL] ep=1790/4000 reward=0.0487 eps=0.647 loss=0.00015281280502676964\n",
            "[SELL] ep=1800/4000 reward=0.0487 eps=0.645 loss=0.00016949739074334502\n",
            "[SELL] ep=1810/4000 reward=-0.0465 eps=0.643 loss=0.00010179720266023651\n",
            "[SELL] ep=1820/4000 reward=-0.0577 eps=0.641 loss=8.914899080991745e-05\n",
            "target sync @ 22500 loss 0.0001943526731338352\n",
            "[SELL] ep=1830/4000 reward=0.0030 eps=0.639 loss=0.00015702105883974582\n",
            "[SELL] ep=1840/4000 reward=0.0914 eps=0.637 loss=0.00014496545190922916\n",
            "[SELL] ep=1850/4000 reward=-0.0118 eps=0.635 loss=0.00016108487034216523\n",
            "[SELL] ep=1860/4000 reward=0.0278 eps=0.633 loss=0.00012447376502677798\n",
            "target sync @ 23000 loss 7.947390258777887e-05\n",
            "[SELL] ep=1870/4000 reward=0.0343 eps=0.631 loss=0.00015753245679661632\n",
            "[SELL] ep=1880/4000 reward=-0.0651 eps=0.629 loss=0.0007015372975729406\n",
            "[SELL] ep=1890/4000 reward=0.0125 eps=0.627 loss=0.00012589062680490315\n",
            "[SELL] ep=1900/4000 reward=-0.0257 eps=0.625 loss=8.300029003294185e-05\n",
            "target sync @ 23500 loss 0.00010646339796949178\n",
            "[SELL] ep=1910/4000 reward=-0.0027 eps=0.623 loss=0.0001381278270855546\n",
            "[SELL] ep=1920/4000 reward=0.2029 eps=0.621 loss=0.0006478978903032839\n",
            "[SELL] ep=1930/4000 reward=-0.0277 eps=0.619 loss=0.0001145963033195585\n",
            "[SELL] ep=1940/4000 reward=0.0201 eps=0.617 loss=0.00012642792717088014\n",
            "target sync @ 24000 loss 0.0004555649356916547\n",
            "[SELL] ep=1950/4000 reward=0.0323 eps=0.615 loss=0.0002840164816007018\n",
            "[SELL] ep=1960/4000 reward=0.0858 eps=0.613 loss=0.000165433477377519\n",
            "[SELL] ep=1970/4000 reward=-0.0111 eps=0.611 loss=0.00022005350911058486\n",
            "[SELL] ep=1980/4000 reward=-0.0300 eps=0.609 loss=0.0007813630509190261\n",
            "target sync @ 24500 loss 0.00032283883774653077\n",
            "[SELL] ep=1990/4000 reward=-0.0968 eps=0.606 loss=0.00023776176385581493\n",
            "[SELL] ep=2000/4000 reward=-0.0563 eps=0.604 loss=0.00017600724822841585\n",
            "[SELL] ep=2010/4000 reward=-0.1032 eps=0.602 loss=0.0001221909187734127\n",
            "target sync @ 25000 loss 0.0001709199568722397\n",
            "[SELL] ep=2020/4000 reward=-0.2265 eps=0.600 loss=0.00012835263623856008\n",
            "[SELL] ep=2030/4000 reward=-0.0302 eps=0.598 loss=0.000130691725644283\n",
            "[SELL] ep=2040/4000 reward=0.0125 eps=0.596 loss=0.00021228392142802477\n",
            "[SELL] ep=2050/4000 reward=0.0092 eps=0.594 loss=0.0001204909203806892\n",
            "target sync @ 25500 loss 0.00015049043577164412\n",
            "[SELL] ep=2060/4000 reward=0.0000 eps=0.592 loss=0.0006501954048871994\n",
            "[SELL] ep=2070/4000 reward=-0.0337 eps=0.589 loss=0.00036148191429674625\n",
            "[SELL] ep=2080/4000 reward=0.0534 eps=0.587 loss=0.0001189712857012637\n",
            "[SELL] ep=2090/4000 reward=0.0053 eps=0.585 loss=0.0007319280412048101\n",
            "target sync @ 26000 loss 0.00017302717606071383\n",
            "[SELL] ep=2100/4000 reward=0.0017 eps=0.583 loss=0.0001331079110968858\n",
            "[SELL] ep=2110/4000 reward=-0.0451 eps=0.581 loss=0.00010903744259849191\n",
            "[SELL] ep=2120/4000 reward=-0.0924 eps=0.579 loss=0.00012254048488102853\n",
            "target sync @ 26500 loss 0.00011643902689684182\n",
            "[SELL] ep=2130/4000 reward=-0.0458 eps=0.577 loss=0.00016370747471228242\n",
            "[SELL] ep=2140/4000 reward=0.1024 eps=0.575 loss=0.0005551393842324615\n",
            "[SELL] ep=2150/4000 reward=0.1232 eps=0.573 loss=0.00013647726154886186\n",
            "[SELL] ep=2160/4000 reward=-0.0472 eps=0.571 loss=7.950207509566098e-05\n",
            "target sync @ 27000 loss 0.00015314998745452613\n",
            "[SELL] ep=2170/4000 reward=-0.0100 eps=0.569 loss=0.000593661330640316\n",
            "[SELL] ep=2180/4000 reward=0.2758 eps=0.567 loss=0.00032013541203923523\n",
            "[SELL] ep=2190/4000 reward=0.0888 eps=0.564 loss=0.00021336186910048127\n",
            "[SELL] ep=2200/4000 reward=-0.0680 eps=0.562 loss=0.00017173957894556224\n",
            "target sync @ 27500 loss 0.00015872521908022463\n",
            "[SELL] ep=2210/4000 reward=-0.0057 eps=0.560 loss=0.00018966203788295388\n",
            "[SELL] ep=2220/4000 reward=0.0622 eps=0.558 loss=0.0001800166501197964\n",
            "[SELL] ep=2230/4000 reward=0.0322 eps=0.556 loss=0.00027359637897461653\n",
            "[SELL] ep=2240/4000 reward=0.0215 eps=0.554 loss=0.00011234255362069234\n",
            "target sync @ 28000 loss 0.0004250090569257736\n",
            "[SELL] ep=2250/4000 reward=-0.0541 eps=0.552 loss=0.0012064386392012239\n",
            "[SELL] ep=2260/4000 reward=0.0289 eps=0.549 loss=0.00024993388797156513\n",
            "[SELL] ep=2270/4000 reward=-0.0065 eps=0.547 loss=0.00012402553693391383\n",
            "[SELL] ep=2280/4000 reward=0.0093 eps=0.545 loss=0.000195195316337049\n",
            "target sync @ 28500 loss 0.0002607625210657716\n",
            "[SELL] ep=2290/4000 reward=0.0000 eps=0.543 loss=0.0001512440067017451\n",
            "[SELL] ep=2300/4000 reward=0.0957 eps=0.541 loss=0.0002499958500266075\n",
            "[SELL] ep=2310/4000 reward=-0.0463 eps=0.539 loss=0.00025180086959153414\n",
            "target sync @ 29000 loss 8.552530198358e-05\n",
            "[SELL] ep=2320/4000 reward=-0.1629 eps=0.537 loss=0.0008575116517022252\n",
            "[SELL] ep=2330/4000 reward=-0.0229 eps=0.534 loss=0.0001851997512858361\n",
            "[SELL] ep=2340/4000 reward=-0.0041 eps=0.532 loss=0.0027260081842541695\n",
            "[SELL] ep=2350/4000 reward=0.0153 eps=0.530 loss=0.00015556445578113198\n",
            "target sync @ 29500 loss 0.00027036023675464094\n",
            "[SELL] ep=2360/4000 reward=0.0239 eps=0.528 loss=0.0005876347422599792\n",
            "[SELL] ep=2370/4000 reward=-0.0392 eps=0.526 loss=0.000106602325104177\n",
            "[SELL] ep=2380/4000 reward=-0.0171 eps=0.524 loss=0.000516445143148303\n",
            "[SELL] ep=2390/4000 reward=-0.0228 eps=0.521 loss=0.0013291502837091684\n",
            "target sync @ 30000 loss 0.0030003178399056196\n",
            "[SELL] ep=2400/4000 reward=0.0214 eps=0.519 loss=0.0002575136022642255\n",
            "[SELL] ep=2410/4000 reward=-0.0687 eps=0.517 loss=0.0003992897109128535\n",
            "[SELL] ep=2420/4000 reward=0.0556 eps=0.515 loss=0.0004502969386521727\n",
            "target sync @ 30500 loss 0.00015563637134619057\n",
            "[SELL] ep=2430/4000 reward=-0.0561 eps=0.513 loss=0.00011380341311451048\n",
            "[SELL] ep=2440/4000 reward=0.0695 eps=0.511 loss=0.0003013368113897741\n",
            "[SELL] ep=2450/4000 reward=-0.0258 eps=0.508 loss=0.00013807610957883298\n",
            "[SELL] ep=2460/4000 reward=-0.0173 eps=0.506 loss=0.00018221503705717623\n",
            "target sync @ 31000 loss 0.00025987494154833257\n",
            "[SELL] ep=2470/4000 reward=-0.0135 eps=0.504 loss=0.0003225622931495309\n",
            "[SELL] ep=2480/4000 reward=-0.0305 eps=0.502 loss=0.0013905162923038006\n",
            "[SELL] ep=2490/4000 reward=0.0702 eps=0.500 loss=0.0006290957098826766\n",
            "[SELL] ep=2500/4000 reward=-0.0407 eps=0.498 loss=0.00011587268090806901\n",
            "target sync @ 31500 loss 0.00030961335869506\n",
            "[SELL] ep=2510/4000 reward=0.0566 eps=0.496 loss=0.0032348011154681444\n",
            "[SELL] ep=2520/4000 reward=0.0607 eps=0.494 loss=0.00013600873353425413\n",
            "[SELL] ep=2530/4000 reward=-0.1140 eps=0.492 loss=0.0003479640290606767\n",
            "[SELL] ep=2540/4000 reward=0.0084 eps=0.490 loss=0.0005830200389027596\n",
            "target sync @ 32000 loss 0.00016704274457879364\n",
            "[SELL] ep=2550/4000 reward=0.0430 eps=0.488 loss=0.00016228877939283848\n",
            "[SELL] ep=2560/4000 reward=0.0774 eps=0.486 loss=0.00027839222457259893\n",
            "[SELL] ep=2570/4000 reward=-0.0887 eps=0.484 loss=0.0003083550836890936\n",
            "target sync @ 32500 loss 0.00013208453310653567\n",
            "[SELL] ep=2580/4000 reward=0.0545 eps=0.482 loss=0.00018604914657771587\n",
            "[SELL] ep=2590/4000 reward=-0.0282 eps=0.479 loss=0.00025394107797183096\n",
            "[SELL] ep=2600/4000 reward=-0.0298 eps=0.477 loss=0.0005441847024485469\n",
            "[SELL] ep=2610/4000 reward=0.0026 eps=0.475 loss=0.00029479159275069833\n",
            "target sync @ 33000 loss 0.0003403729060664773\n",
            "[SELL] ep=2620/4000 reward=0.0199 eps=0.473 loss=0.001197411329485476\n",
            "[SELL] ep=2630/4000 reward=-0.0615 eps=0.471 loss=0.002178087830543518\n",
            "[SELL] ep=2640/4000 reward=-0.0091 eps=0.469 loss=0.00022571353474631906\n",
            "[SELL] ep=2650/4000 reward=-0.0059 eps=0.467 loss=0.0002856222272384912\n",
            "target sync @ 33500 loss 0.0003393581719137728\n",
            "[SELL] ep=2660/4000 reward=0.0126 eps=0.464 loss=0.0001531626476207748\n",
            "[SELL] ep=2670/4000 reward=0.0254 eps=0.462 loss=0.00024207080423366278\n",
            "[SELL] ep=2680/4000 reward=-0.0604 eps=0.460 loss=0.0004137141222599894\n",
            "[SELL] ep=2690/4000 reward=0.0259 eps=0.459 loss=0.0001671540958341211\n",
            "target sync @ 34000 loss 0.0003010269720107317\n",
            "[SELL] ep=2700/4000 reward=0.0315 eps=0.456 loss=0.0017796937609091401\n",
            "[SELL] ep=2710/4000 reward=-0.0492 eps=0.454 loss=0.0013862585183233023\n",
            "[SELL] ep=2720/4000 reward=-0.0152 eps=0.452 loss=0.0002121809811796993\n",
            "target sync @ 34500 loss 0.00042358931386843324\n",
            "[SELL] ep=2730/4000 reward=-0.0454 eps=0.450 loss=0.003007004503160715\n",
            "[SELL] ep=2740/4000 reward=-0.0304 eps=0.448 loss=0.0002716000599320978\n",
            "[SELL] ep=2750/4000 reward=-0.0220 eps=0.446 loss=0.0006502235773950815\n",
            "[SELL] ep=2760/4000 reward=-0.0788 eps=0.444 loss=0.000329843518557027\n",
            "target sync @ 35000 loss 0.0003387939650565386\n",
            "[SELL] ep=2770/4000 reward=-0.0677 eps=0.441 loss=0.0017950196051970124\n",
            "[SELL] ep=2780/4000 reward=-0.0064 eps=0.439 loss=0.0003681632224470377\n",
            "[SELL] ep=2790/4000 reward=0.0067 eps=0.437 loss=0.00030590876122005284\n",
            "[SELL] ep=2800/4000 reward=0.0508 eps=0.435 loss=0.00031694245990365744\n",
            "target sync @ 35500 loss 0.0002695340081118047\n",
            "[SELL] ep=2810/4000 reward=0.0858 eps=0.433 loss=0.00027491728542372584\n",
            "[SELL] ep=2820/4000 reward=-0.0089 eps=0.430 loss=0.0005182235036045313\n",
            "[SELL] ep=2830/4000 reward=0.0430 eps=0.428 loss=0.00020744180073961616\n",
            "[SELL] ep=2840/4000 reward=0.0008 eps=0.426 loss=0.0013118535280227661\n",
            "target sync @ 36000 loss 0.0005641200696118176\n",
            "[SELL] ep=2850/4000 reward=-0.0457 eps=0.424 loss=0.00031112792203202844\n",
            "[SELL] ep=2860/4000 reward=-0.0499 eps=0.422 loss=0.008070183917880058\n",
            "[SELL] ep=2870/4000 reward=0.0303 eps=0.420 loss=0.0004719345015473664\n",
            "target sync @ 36500 loss 0.00040376250399276614\n",
            "[SELL] ep=2880/4000 reward=-0.0341 eps=0.418 loss=0.0015078848227858543\n",
            "[SELL] ep=2890/4000 reward=-0.0201 eps=0.416 loss=0.00022829865338280797\n",
            "[SELL] ep=2900/4000 reward=-0.0317 eps=0.414 loss=0.0002630343078635633\n",
            "[SELL] ep=2910/4000 reward=-0.0282 eps=0.411 loss=0.002522003836929798\n",
            "target sync @ 37000 loss 0.0009948242222890258\n",
            "[SELL] ep=2920/4000 reward=0.0223 eps=0.409 loss=0.0002776625333353877\n",
            "[SELL] ep=2930/4000 reward=-0.0171 eps=0.407 loss=0.0004240422567818314\n",
            "[SELL] ep=2940/4000 reward=-0.0710 eps=0.405 loss=0.0002079163969028741\n",
            "[SELL] ep=2950/4000 reward=0.0161 eps=0.403 loss=0.000246923096710816\n",
            "target sync @ 37500 loss 0.0010743500897660851\n",
            "[SELL] ep=2960/4000 reward=-0.0514 eps=0.401 loss=0.0004006404196843505\n",
            "[SELL] ep=2970/4000 reward=0.0419 eps=0.399 loss=0.009928724728524685\n",
            "[SELL] ep=2980/4000 reward=-0.0722 eps=0.396 loss=0.0002615871198941022\n",
            "target sync @ 38000 loss 0.0003694246697705239\n",
            "[SELL] ep=2990/4000 reward=-0.0372 eps=0.394 loss=0.000276926759397611\n",
            "[SELL] ep=3000/4000 reward=0.0041 eps=0.392 loss=0.00024048425257205963\n",
            "[SELL] ep=3010/4000 reward=0.0470 eps=0.390 loss=0.0017628062050789595\n",
            "[SELL] ep=3020/4000 reward=-0.0579 eps=0.388 loss=0.000956184696406126\n",
            "target sync @ 38500 loss 0.0005323442746885121\n",
            "[SELL] ep=3030/4000 reward=-0.0234 eps=0.386 loss=0.0009275848278775811\n",
            "[SELL] ep=3040/4000 reward=-0.0201 eps=0.384 loss=0.0020283900666981936\n",
            "[SELL] ep=3050/4000 reward=-0.0312 eps=0.382 loss=0.0021024595480412245\n",
            "[SELL] ep=3060/4000 reward=0.0169 eps=0.379 loss=0.00587724894285202\n",
            "target sync @ 39000 loss 0.00042089889757335186\n",
            "[SELL] ep=3070/4000 reward=0.1399 eps=0.378 loss=0.0003404787858016789\n",
            "[SELL] ep=3080/4000 reward=0.0034 eps=0.376 loss=0.0002409451553830877\n",
            "[SELL] ep=3090/4000 reward=-0.1718 eps=0.374 loss=0.0028690325561910868\n",
            "[SELL] ep=3100/4000 reward=0.0071 eps=0.371 loss=0.0006832788931205869\n",
            "target sync @ 39500 loss 0.0003415971004869789\n",
            "[SELL] ep=3110/4000 reward=-0.0188 eps=0.369 loss=0.0013486620737239718\n",
            "[SELL] ep=3120/4000 reward=0.0000 eps=0.367 loss=0.0014123248402029276\n",
            "[SELL] ep=3130/4000 reward=0.0108 eps=0.365 loss=0.0005533808725886047\n",
            "[SELL] ep=3140/4000 reward=0.0051 eps=0.363 loss=0.0005267425440251827\n",
            "target sync @ 40000 loss 0.00041857772157527506\n",
            "[SELL] ep=3150/4000 reward=0.0227 eps=0.361 loss=0.0037745151203125715\n",
            "[SELL] ep=3160/4000 reward=-0.0140 eps=0.359 loss=0.0002378662466071546\n",
            "[SELL] ep=3170/4000 reward=-0.0342 eps=0.357 loss=0.002567388117313385\n",
            "target sync @ 40500 loss 0.0001774732954800129\n",
            "[SELL] ep=3180/4000 reward=-0.0852 eps=0.354 loss=0.003923369571566582\n",
            "[SELL] ep=3190/4000 reward=0.0233 eps=0.352 loss=0.0002312521100975573\n",
            "[SELL] ep=3200/4000 reward=0.2018 eps=0.350 loss=0.0031765298917889595\n",
            "[SELL] ep=3210/4000 reward=-0.0142 eps=0.348 loss=0.0002378156059421599\n",
            "target sync @ 41000 loss 0.003232058137655258\n",
            "[SELL] ep=3220/4000 reward=-0.0093 eps=0.346 loss=0.0005116650136187673\n",
            "[SELL] ep=3230/4000 reward=0.0040 eps=0.344 loss=0.0010824189521372318\n",
            "[SELL] ep=3240/4000 reward=-0.0237 eps=0.342 loss=0.0006956817815080285\n",
            "[SELL] ep=3250/4000 reward=0.0442 eps=0.340 loss=0.0003390689380466938\n",
            "target sync @ 41500 loss 0.009380321949720383\n",
            "[SELL] ep=3260/4000 reward=-0.0331 eps=0.337 loss=0.0013823184417560697\n",
            "[SELL] ep=3270/4000 reward=0.0284 eps=0.335 loss=0.0038780884351581335\n",
            "[SELL] ep=3280/4000 reward=0.0000 eps=0.332 loss=0.0003002559533342719\n",
            "target sync @ 42000 loss 0.000950641289819032\n",
            "[SELL] ep=3290/4000 reward=-0.0264 eps=0.330 loss=0.0002719011390581727\n",
            "[SELL] ep=3300/4000 reward=0.0041 eps=0.328 loss=0.0010604121489450336\n",
            "[SELL] ep=3310/4000 reward=0.0158 eps=0.325 loss=0.0014793111477047205\n",
            "target sync @ 42500 loss 0.00041189268813468516\n",
            "[SELL] ep=3320/4000 reward=-0.0111 eps=0.323 loss=0.00046404788736253977\n",
            "[SELL] ep=3330/4000 reward=-0.0133 eps=0.321 loss=0.0010736174881458282\n",
            "[SELL] ep=3340/4000 reward=-0.0158 eps=0.318 loss=0.0008098895195871592\n",
            "[SELL] ep=3350/4000 reward=0.0737 eps=0.316 loss=0.0003707046271301806\n",
            "target sync @ 43000 loss 0.00035843922523781657\n",
            "[SELL] ep=3360/4000 reward=0.0000 eps=0.314 loss=0.0002707175153773278\n",
            "[SELL] ep=3370/4000 reward=0.0326 eps=0.312 loss=0.0020423776004463434\n",
            "[SELL] ep=3380/4000 reward=0.0442 eps=0.310 loss=0.0002144213649444282\n",
            "target sync @ 43500 loss 0.00025781572912819684\n",
            "[SELL] ep=3390/4000 reward=0.0406 eps=0.307 loss=0.000985905178822577\n",
            "[SELL] ep=3400/4000 reward=-0.0229 eps=0.305 loss=0.000829749449621886\n",
            "[SELL] ep=3410/4000 reward=-0.0698 eps=0.303 loss=0.0004102499224245548\n",
            "[SELL] ep=3420/4000 reward=-0.0222 eps=0.301 loss=0.0002788984857033938\n",
            "target sync @ 44000 loss 0.00040689134038984776\n",
            "[SELL] ep=3430/4000 reward=-0.0217 eps=0.299 loss=0.00015303479449357837\n",
            "[SELL] ep=3440/4000 reward=-0.0284 eps=0.296 loss=0.0008888900047168136\n",
            "[SELL] ep=3450/4000 reward=0.0051 eps=0.294 loss=0.0002914253855124116\n",
            "[SELL] ep=3460/4000 reward=0.0508 eps=0.292 loss=0.004376781173050404\n",
            "target sync @ 44500 loss 0.0003653729218058288\n",
            "[SELL] ep=3470/4000 reward=-0.0066 eps=0.290 loss=0.00031670197495259345\n",
            "[SELL] ep=3480/4000 reward=0.0000 eps=0.288 loss=0.00031564003438688815\n",
            "[SELL] ep=3490/4000 reward=-0.0488 eps=0.285 loss=0.0006663570529781282\n",
            "target sync @ 45000 loss 0.00033778254874050617\n",
            "[SELL] ep=3500/4000 reward=0.0000 eps=0.283 loss=0.0005571531946770847\n",
            "[SELL] ep=3510/4000 reward=-0.0228 eps=0.281 loss=0.007661138195544481\n",
            "[SELL] ep=3520/4000 reward=-0.0005 eps=0.279 loss=0.00025161454686895013\n",
            "[SELL] ep=3530/4000 reward=0.0612 eps=0.276 loss=0.0015266184927895665\n",
            "target sync @ 45500 loss 0.0005652058171108365\n",
            "[SELL] ep=3540/4000 reward=-0.0192 eps=0.274 loss=0.0013119103386998177\n",
            "[SELL] ep=3550/4000 reward=0.0015 eps=0.271 loss=0.0002398457727394998\n",
            "[SELL] ep=3560/4000 reward=-0.1718 eps=0.269 loss=0.005227752961218357\n",
            "target sync @ 46000 loss 0.0006932619726285338\n",
            "[SELL] ep=3570/4000 reward=0.0042 eps=0.266 loss=0.0003110105753876269\n",
            "[SELL] ep=3580/4000 reward=-0.0874 eps=0.264 loss=0.00014823215315118432\n",
            "[SELL] ep=3590/4000 reward=0.0205 eps=0.262 loss=0.0005542067810893059\n",
            "target sync @ 46500 loss 0.0006565414951182902\n",
            "[SELL] ep=3600/4000 reward=-0.0250 eps=0.260 loss=0.0022183044347912073\n",
            "[SELL] ep=3610/4000 reward=-0.0372 eps=0.257 loss=0.000769691658206284\n",
            "[SELL] ep=3620/4000 reward=-0.1894 eps=0.255 loss=0.0001491848670411855\n",
            "target sync @ 47000 loss 0.0002046466979663819\n",
            "[SELL] ep=3630/4000 reward=-0.0421 eps=0.252 loss=0.00021768614533357322\n",
            "[SELL] ep=3640/4000 reward=-0.0250 eps=0.250 loss=0.00043364588054828346\n",
            "[SELL] ep=3650/4000 reward=-0.0160 eps=0.248 loss=0.0005935181980021298\n",
            "[SELL] ep=3660/4000 reward=0.1068 eps=0.245 loss=0.00018080518930219114\n",
            "target sync @ 47500 loss 0.0002857209765352309\n",
            "[SELL] ep=3670/4000 reward=0.0029 eps=0.243 loss=0.0004221976560074836\n",
            "[SELL] ep=3680/4000 reward=0.0447 eps=0.241 loss=0.004473275970667601\n",
            "[SELL] ep=3690/4000 reward=-0.0475 eps=0.238 loss=0.0006254187901504338\n",
            "target sync @ 48000 loss 0.000226935648242943\n",
            "[SELL] ep=3700/4000 reward=-0.0362 eps=0.236 loss=0.0001821444311644882\n",
            "[SELL] ep=3710/4000 reward=-0.0201 eps=0.234 loss=0.0009494050755165517\n",
            "[SELL] ep=3720/4000 reward=0.0227 eps=0.232 loss=0.0002937785175163299\n",
            "[SELL] ep=3730/4000 reward=0.0058 eps=0.229 loss=0.0003179216291755438\n",
            "target sync @ 48500 loss 0.0004918145714327693\n",
            "[SELL] ep=3740/4000 reward=-0.0015 eps=0.227 loss=0.0014942591078579426\n",
            "[SELL] ep=3750/4000 reward=0.0483 eps=0.224 loss=0.00012318931112531573\n",
            "[SELL] ep=3760/4000 reward=-0.0082 eps=0.222 loss=0.0002253790298709646\n",
            "target sync @ 49000 loss 0.00011609622015384957\n",
            "[SELL] ep=3770/4000 reward=-0.0196 eps=0.219 loss=0.00048070302000269294\n",
            "[SELL] ep=3780/4000 reward=-0.0499 eps=0.217 loss=0.003367573954164982\n",
            "[SELL] ep=3790/4000 reward=0.0104 eps=0.214 loss=0.0002809675643220544\n",
            "target sync @ 49500 loss 0.0007979993242770433\n",
            "[SELL] ep=3800/4000 reward=-0.0365 eps=0.212 loss=0.004850282333791256\n",
            "[SELL] ep=3810/4000 reward=-0.0372 eps=0.209 loss=0.0001802727929316461\n",
            "[SELL] ep=3820/4000 reward=0.0957 eps=0.207 loss=0.0017283821944147348\n",
            "[SELL] ep=3830/4000 reward=-0.0209 eps=0.205 loss=0.00028296298114582896\n",
            "target sync @ 50000 loss 0.0007643135031685233\n",
            "[SELL] ep=3840/4000 reward=-0.0277 eps=0.202 loss=0.0011157842818647623\n",
            "[SELL] ep=3850/4000 reward=0.0127 eps=0.200 loss=0.0016606010030955076\n",
            "[SELL] ep=3860/4000 reward=-0.0608 eps=0.198 loss=0.0002376572956563905\n",
            "target sync @ 50500 loss 0.00016807530482765287\n",
            "[SELL] ep=3870/4000 reward=-0.0201 eps=0.195 loss=0.00023880010121501982\n",
            "[SELL] ep=3880/4000 reward=0.0000 eps=0.193 loss=0.00017933400522451848\n",
            "[SELL] ep=3890/4000 reward=-0.0101 eps=0.190 loss=0.0001673246151767671\n",
            "target sync @ 51000 loss 0.00010627813026076183\n",
            "[SELL] ep=3900/4000 reward=-0.0172 eps=0.187 loss=0.00024084807955659926\n",
            "[SELL] ep=3910/4000 reward=0.0483 eps=0.185 loss=0.0007858315948396921\n",
            "[SELL] ep=3920/4000 reward=-0.0040 eps=0.183 loss=0.0001931562292156741\n",
            "target sync @ 51500 loss 0.0003587160026654601\n",
            "[SELL] ep=3930/4000 reward=-0.0160 eps=0.180 loss=0.0005709613906219602\n",
            "[SELL] ep=3940/4000 reward=0.0175 eps=0.177 loss=0.0005195080302655697\n",
            "[SELL] ep=3950/4000 reward=-0.0324 eps=0.175 loss=0.00015369341417681426\n",
            "target sync @ 52000 loss 0.00016915057494770736\n",
            "[SELL] ep=3960/4000 reward=0.0224 eps=0.173 loss=0.00016788032371550798\n",
            "[SELL] ep=3970/4000 reward=-0.0250 eps=0.170 loss=0.0013658939860761166\n",
            "[SELL] ep=3980/4000 reward=0.0087 eps=0.168 loss=0.0003774890792556107\n",
            "[SELL] ep=3990/4000 reward=0.0000 eps=0.165 loss=0.00027083978056907654\n",
            "target sync @ 52500 loss 0.0002209866070188582\n",
            "[SELL] ep=4000/4000 reward=0.0041 eps=0.163 loss=0.0005788092385046184\n",
            "Saved: runs/sell_agent.pt\n",
            "\n",
            "=== SELL EVAL (TEST entries) ===\n",
            "n_entries: 94\n",
            "\n",
            "SellAgent:\n",
            "mean: -0.006572012637821749 median: 0.0 win_rate: 0.3617021276595745 min/max: -0.16316315466595177 0.09281238847084017\n",
            "\n",
            "Fixed horizon baseline (hold->forced exit):\n",
            "mean: 0.0 median: 0.0 win_rate: 0.0 min/max: 0.0 0.0\n",
            "\n",
            "Delta (agent - baseline):\n",
            "mean delta: -0.006572012637821749 median delta: 0.0 better %: 0.3617021276595745\n",
            "\n",
            "Per-entry delta: [ 0.000e+00  3.360e-02 -6.200e-03  1.790e-02  0.000e+00  0.000e+00\n",
            " -9.300e-03  1.000e-04  2.030e-02  0.000e+00  0.000e+00 -3.000e-04\n",
            " -3.950e-02 -4.600e-03 -7.050e-02 -8.000e-04  0.000e+00 -8.110e-02\n",
            "  5.540e-02  0.000e+00  7.400e-03 -3.000e-04  0.000e+00 -3.660e-02\n",
            "  5.330e-02 -7.750e-02  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            " -3.040e-02  7.240e-02  3.200e-03 -4.890e-02 -3.080e-02  2.000e-03\n",
            "  0.000e+00  4.400e-03  3.610e-02  1.940e-02  5.270e-02 -8.840e-02\n",
            " -1.740e-02 -3.100e-02  0.000e+00 -1.700e-02  5.600e-03 -3.000e-04\n",
            "  0.000e+00 -4.890e-02  0.000e+00 -6.300e-03  9.000e-03  1.040e-02\n",
            " -4.960e-02 -7.320e-02 -9.100e-03 -1.632e-01  3.660e-02  8.300e-02\n",
            "  2.690e-02  3.560e-02  0.000e+00 -2.150e-02 -4.890e-02 -4.570e-02\n",
            " -1.860e-02  2.880e-02 -5.100e-03  2.830e-02  2.000e-03  6.110e-02\n",
            " -4.700e-02  2.080e-02  0.000e+00 -1.059e-01  1.480e-02 -7.780e-02\n",
            "  5.510e-02  2.350e-02  6.700e-03 -4.120e-02 -1.600e-03 -8.810e-02\n",
            "  2.300e-03 -6.200e-03 -5.240e-02  2.590e-02 -3.480e-02  0.000e+00\n",
            " -6.600e-03  9.280e-02 -3.190e-02  9.300e-03]\n",
            "Better count: 34 / 94\n",
            "\n",
            "SellAgent exit stats:\n",
            "avg hold bars: 15.074468085106384 min/max hold bars: 10.0 20.0\n",
            "exit reasons: {'limit': 19, 'sell': 75}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "from agents.ddqn_agent import DDQNAgent\n",
        "from envs.sell_env import SellEnv  # <- make sure this is the TM-consistent SellEnv v1\n",
        "\n",
        "# Load entry indices saved by your TM cell\n",
        "entries_train = np.load(os.path.join(out_dir, \"entry_indices_train.npy\"))\n",
        "entries_test  = np.load(os.path.join(out_dir, \"entry_indices_test.npy\"))\n",
        "\n",
        "print(\"entries_train:\", entries_train.shape, \"entries_test:\", entries_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# Create SellEnv (TRAIN)\n",
        "# -----------------------\n",
        "sell_env_train = SellEnv(\n",
        "    features=X_train,\n",
        "    prices=p_train,\n",
        "    entry_indices=entries_train,\n",
        "    transaction_cost=cfg.reward.transaction_cost,\n",
        "    sell_horizon=cfg.trade_manager.sell_horizon,\n",
        "    min_hold_bars=cfg.trade_manager.min_hold_bars,\n",
        "    segment_len=SEG_TRAIN,\n",
        "    include_pos_features=True,\n",
        ")\n",
        "\n",
        "print(\"sell include_pos:\", sell_env_train.include_pos)\n",
        "print(\"sell feat_dim:\", sell_env_train.feat_dim, \"state_dim:\", sell_env_train.state_dim)\n",
        "\n",
        "# -----------------------\n",
        "# Create SellAgent config\n",
        "# # -----------------------\n",
        "# sell_cfg = deepcopy(cfg.agent)\n",
        "# sell_cfg.state_dim = int(sell_env_train.state_dim)\n",
        "# sell_cfg.n_actions = 2\n",
        "\n",
        "# # faster decay is fine, but DON'T double-count total_steps\n",
        "# sell_cfg.epsilon_start = 1.0\n",
        "# sell_cfg.epsilon_end = 0.05\n",
        "# sell_cfg.epsilon_decay_steps = 40000\n",
        "\n",
        "# sell_agent = DDQNAgent(sell_cfg)\n",
        "# -----------------------\n",
        "# Create SellAgent config\n",
        "# -----------------------\n",
        "sell_cfg = deepcopy(cfg.agent)\n",
        "sell_cfg.state_dim = int(sell_env_train.state_dim)\n",
        "sell_cfg.n_actions = 2\n",
        "\n",
        "# OPTIMIZED HYPERPARAMS FOR SELL AGENT\n",
        "sell_cfg.lr = 0.0005                 # Slightly lower LR for stability\n",
        "sell_cfg.epsilon_start = 1.0\n",
        "sell_cfg.epsilon_end = 0.05\n",
        "\n",
        "# We want decay to finish at ~80% of training\n",
        "EPISODES = 5000\n",
        "avg_steps_per_ep = 15  # horizon is 20, exits often happen around 10-20\n",
        "total_estimated_steps = EPISODES * avg_steps_per_ep\n",
        "\n",
        "sell_cfg.epsilon_decay_steps = int(total_estimated_steps * 0.8)\n",
        "\n",
        "sell_agent = DDQNAgent(sell_cfg)\n",
        "print(f\"Sell Decay Steps: {sell_cfg.epsilon_decay_steps} / Est Total: {total_estimated_steps}\")\n",
        "\n",
        "# ... training loop follows ...\n",
        "print(\"SELL state_dim:\", sell_cfg.state_dim, \"n_actions:\", sell_cfg.n_actions)\n",
        "\n",
        "# -----------------------\n",
        "# Train loop (episode-based)\n",
        "# -----------------------\n",
        "# EPISODES = 800\n",
        "EPISODES = 4000\n",
        "MAX_STEPS = 200         # safety cap (horizon is small anyway)\n",
        "UPDATES_PER_STEP = 1\n",
        "\n",
        "for ep in range(EPISODES):\n",
        "    s = sell_env_train.reset()\n",
        "    done = False\n",
        "    ep_reward = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    while (not done) and (steps < MAX_STEPS):\n",
        "        a = sell_agent.select_action(s, greedy=False)  # <- this increments total_steps internally\n",
        "        ns, r, done, info = sell_env_train.step(a)\n",
        "\n",
        "        sell_agent.push(s, a, r, ns, done)\n",
        "\n",
        "        # update after warmup (based on agent.total_steps, which is now correct)\n",
        "        if sell_agent.total_steps >= int(cfg.training.warmup_steps):\n",
        "            for _ in range(UPDATES_PER_STEP):\n",
        "                sell_agent.update()\n",
        "\n",
        "        s = ns\n",
        "        ep_reward += float(r)\n",
        "        steps += 1\n",
        "\n",
        "    if (ep + 1) % 10 == 0:\n",
        "        loss = sell_agent.loss_history[-1] if sell_agent.loss_history else None\n",
        "        print(f\"[SELL] ep={ep+1}/{EPISODES} reward={ep_reward:.4f} eps={sell_agent.eps:.3f} loss={loss}\")\n",
        "\n",
        "# Save model\n",
        "sell_path = os.path.join(out_dir, \"sell_agent.pt\")\n",
        "sell_agent.save(sell_path)\n",
        "print(\"Saved:\", sell_path)\n",
        "\n",
        "# -----------------------\n",
        "# Evaluation helpers\n",
        "# -----------------------\n",
        "def eval_sell_agent(env, agent, entry_indices, greedy=True):\n",
        "    rets, holds, exits, reasons = [], [], [], []\n",
        "    for e in entry_indices:\n",
        "        s = env.reset(int(e))\n",
        "        done = False\n",
        "        total_r = 0.0\n",
        "        steps = 0\n",
        "        last_info = None\n",
        "\n",
        "        while (not done) and (steps < 500):\n",
        "            a = agent.select_action(s, greedy=greedy)  # greedy=True => no eps update, no step increment\n",
        "            ns, r, done, info = env.step(a)\n",
        "            s = ns\n",
        "            total_r += float(r)\n",
        "            steps += 1\n",
        "            last_info = info\n",
        "\n",
        "        rets.append(total_r)\n",
        "        if last_info:\n",
        "            exits.append(last_info.get(\"exit_idx\", np.nan))\n",
        "            holds.append(last_info.get(\"bars_held\", np.nan))\n",
        "            reasons.append(last_info.get(\"reason\", \"\"))\n",
        "        else:\n",
        "            exits.append(np.nan); holds.append(np.nan); reasons.append(\"\")\n",
        "\n",
        "    return np.array(rets, float), np.array(holds, float), np.array(exits, float), reasons\n",
        "\n",
        "\n",
        "def eval_fixed_horizon(env, entry_indices):\n",
        "    # Just HOLD until forced exit\n",
        "    rets = []\n",
        "    for e in entry_indices:\n",
        "        s = env.reset(int(e))\n",
        "        done = False\n",
        "        total_r = 0.0\n",
        "        steps = 0\n",
        "        while (not done) and (steps < 500):\n",
        "            ns, r, done, info = env.step(0)  # HOLD\n",
        "            s = ns\n",
        "            total_r += float(r)\n",
        "            steps += 1\n",
        "        rets.append(total_r)\n",
        "    return np.array(rets, dtype=float)\n",
        "\n",
        "# -----------------------\n",
        "# Evaluate on TEST subset\n",
        "# -----------------------\n",
        "sell_env_test = SellEnv(\n",
        "    features=X_test,\n",
        "    prices=p_test,\n",
        "    entry_indices=entries_test,\n",
        "    transaction_cost=cfg.reward.transaction_cost,\n",
        "    sell_horizon=cfg.trade_manager.sell_horizon,\n",
        "    min_hold_bars=cfg.trade_manager.min_hold_bars,\n",
        "    segment_len=SEG_TEST,\n",
        "    include_pos_features=True,\n",
        ")\n",
        "\n",
        "rets_agent, holds_agent, exits_agent, reasons_agent = eval_sell_agent(\n",
        "    sell_env_test, sell_agent, entries_test, greedy=True\n",
        ")\n",
        "rets_base = eval_fixed_horizon(sell_env_test, entries_test)\n",
        "\n",
        "print(\"\\n=== SELL EVAL (TEST entries) ===\")\n",
        "print(\"n_entries:\", len(entries_test))\n",
        "\n",
        "print(\"\\nSellAgent:\")\n",
        "print(\"mean:\", float(rets_agent.mean()),\n",
        "      \"median:\", float(np.median(rets_agent)),\n",
        "      \"win_rate:\", float((rets_agent > 0).mean()),\n",
        "      \"min/max:\", float(rets_agent.min()), float(rets_agent.max()))\n",
        "\n",
        "print(\"\\nFixed horizon baseline (hold->forced exit):\")\n",
        "print(\"mean:\", float(rets_base.mean()),\n",
        "      \"median:\", float(np.median(rets_base)),\n",
        "      \"win_rate:\", float((rets_base > 0).mean()),\n",
        "      \"min/max:\", float(rets_base.min()), float(rets_base.max()))\n",
        "\n",
        "delta = rets_agent - rets_base\n",
        "print(\"\\nDelta (agent - baseline):\")\n",
        "print(\"mean delta:\", float(delta.mean()),\n",
        "      \"median delta:\", float(np.median(delta)),\n",
        "      \"better %:\", float((delta > 0).mean()))\n",
        "\n",
        "print(\"\\nPer-entry delta:\", np.round(delta, 4))\n",
        "print(\"Better count:\", int((delta > 0).sum()), \"/\", len(delta))\n",
        "\n",
        "# Optional: inspect exit behavior\n",
        "if len(exits_agent) > 0:\n",
        "    print(\"\\nSellAgent exit stats:\")\n",
        "    print(\"avg hold bars:\", float(np.nanmean(holds_agent)),\n",
        "          \"min/max hold bars:\", float(np.nanmin(holds_agent)), float(np.nanmax(holds_agent)))\n",
        "    # quick breakdown of reasons\n",
        "    unique, counts = np.unique(np.array(reasons_agent, dtype=str), return_counts=True)\n",
        "    print(\"exit reasons:\", dict(zip(unique.tolist(), counts.tolist())))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d010456a",
      "metadata": {},
      "source": [
        "# TradeManager with Sell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "51ee24b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA SPLIT ===\n",
            "features: (6195, 10) prices: (6195,)\n",
            "SEG_LEN: 1239 N_SEGS: 5 TRAIN_FRAC: 0.7\n",
            "train_len per seg: 867 test_len per seg: 372\n",
            "X_train: (4335, 10) p_train: (4335,)\n",
            "X_test : (1860, 10) p_test : (1860,)\n",
            "Sell seen: 1282 Sell actions: 192\n",
            "Exit reasons: {'sell_agent': 82, 'time': 75}\n",
            "Non-time exits: [{'entry_idx': 29, 'exit_idx': 44, 'entry_price': 43.37913513183594, 'exit_price': 46.63539123535156, 'gross_return': 0.07506503054104885, 'net_return': 0.07291597554499729, 'hold_bars': 15, 'forced_exit': False, 'meta': {'buy_conf': 0.4434249997152017, 'reason': 'sell_agent', 'sell_conf': 0.8961800799052166, 'sell_q0': -0.01670204848051071, 'sell_q1': 0.026407621800899506, 'sell_margin': 0.04310967028141022}}, {'entry_idx': 49, 'exit_idx': 63, 'entry_price': 47.89346694946289, 'exit_price': 50.256874084472656, 'gross_return': 0.04934717166129526, 'net_return': 0.04724952666514426, 'hold_bars': 14, 'forced_exit': False, 'meta': {'buy_conf': 0.4503135303246825, 'reason': 'sell_agent', 'sell_conf': 0.8451702663477944, 'sell_q0': 0.0029861479997634888, 'sell_q1': 0.0369303897023201, 'sell_margin': 0.03394424170255661}}, {'entry_idx': 145, 'exit_idx': 159, 'entry_price': 49.112152099609375, 'exit_price': 53.090965270996094, 'gross_return': 0.08101484054937767, 'net_return': 0.07885389188311942, 'hold_bars': 14, 'forced_exit': False, 'meta': {'buy_conf': 0.5147126970329038, 'reason': 'sell_agent', 'sell_conf': 0.9446548037183303, 'sell_q0': -0.008749909698963165, 'sell_q1': 0.04799468442797661, 'sell_margin': 0.056744594126939774}}]\n",
            "ENTRY DEBUG: {'checked': 937, 'blocked_trend': 757, 'blocked_latest_entry': 13, 'blocked_conf': 10, 'opened': 157, 'conf_min': 0.34276009760483894, 'conf_max': 0.6326474680829068}\n",
            "SELL DEBUG: {'seen': 1282, 'sell_actions': 192}\n",
            "EXIT REASONS: {'sell_agent': 82, 'time': 75}\n",
            "\n",
            "=== TRADE MANAGER (TRAIN) ===\n",
            "n_steps: 4335\n",
            "segment_len: 867\n",
            "n_trades: 157\n",
            "final_equity: 15.117348035932656\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.021986141079320023\n",
            "win rate: 0.6305732484076433\n",
            "min/median/max net: -0.28433169138937053 0.02845088297121401 0.39307612553170723\n",
            "median hold bars: 19.0\n",
            "top 5 net: [0.21719871 0.22808477 0.23178971 0.29425715 0.39307613]\n",
            "bottom 5 net: [-0.28433169 -0.28274551 -0.22404041 -0.21338848 -0.20219203]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 44, 'entry_price': 43.37913513183594, 'exit_price': 46.63539123535156, 'gross_return': 0.07506503054104885, 'net_return': 0.07291597554499729, 'hold_bars': 15, 'forced_exit': False, 'meta': {'buy_conf': 0.4434249997152017, 'reason': 'sell_agent', 'sell_conf': 0.8961800799052166, 'sell_q0': -0.01670204848051071, 'sell_q1': 0.026407621800899506, 'sell_margin': 0.04310967028141022}}\n",
            "{'entry_idx': 49, 'exit_idx': 63, 'entry_price': 47.89346694946289, 'exit_price': 50.256874084472656, 'gross_return': 0.04934717166129526, 'net_return': 0.04724952666514426, 'hold_bars': 14, 'forced_exit': False, 'meta': {'buy_conf': 0.4503135303246825, 'reason': 'sell_agent', 'sell_conf': 0.8451702663477944, 'sell_q0': 0.0029861479997634888, 'sell_q1': 0.0369303897023201, 'sell_margin': 0.03394424170255661}}\n",
            "{'entry_idx': 68, 'exit_idx': 88, 'entry_price': 48.43777084350586, 'exit_price': 44.3873405456543, 'gross_return': -0.08362131921672707, 'net_return': -0.0854531601996128, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.48387201657702644, 'reason': 'time'}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 4206, 'exit_idx': 4224, 'entry_price': 143.90420532226562, 'exit_price': 132.3509979248047, 'gross_return': -0.08028401513067723, 'net_return': -0.08212252738443093, 'hold_bars': 18, 'forced_exit': False, 'meta': {'buy_conf': 0.47810427643921516, 'reason': 'sell_agent', 'sell_conf': 0.9902397270959075, 'sell_q0': 0.11411482095718384, 'sell_q1': 0.2065073549747467, 'sell_margin': 0.09239253401756287}}\n",
            "{'entry_idx': 4232, 'exit_idx': 4248, 'entry_price': 140.53018188476562, 'exit_price': 130.9173126220703, 'gross_return': -0.06840430385678838, 'net_return': -0.07026656365337869, 'hold_bars': 16, 'forced_exit': False, 'meta': {'buy_conf': 0.4886996732506806, 'reason': 'sell_agent', 'sell_conf': 0.7971909330822307, 'sell_q0': -0.006150960922241211, 'sell_q1': 0.021225623786449432, 'sell_margin': 0.027376584708690643}}\n",
            "{'entry_idx': 4261, 'exit_idx': 4281, 'entry_price': 137.2385711669922, 'exit_price': 123.87850952148438, 'gross_return': -0.09734917473930244, 'net_return': -0.09915357373899858, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4819703375121182, 'reason': 'time'}}\n",
            "Sell seen: 469 Sell actions: 92\n",
            "Exit reasons: {'time': 20, 'sell_agent': 44}\n",
            "Non-time exits: [{'entry_idx': 78, 'exit_idx': 95, 'entry_price': 153.17129516601562, 'exit_price': 147.94802856445312, 'gross_return': -0.034100818928907, 'net_return': -0.03603165139186815, 'hold_bars': 17, 'forced_exit': False, 'meta': {'buy_conf': 0.4694508576365352, 'reason': 'sell_agent', 'sell_conf': 0.9857361018300572, 'sell_q0': 0.06598176807165146, 'sell_q1': 0.15069490671157837, 'sell_margin': 0.08471313863992691}}, {'entry_idx': 100, 'exit_idx': 112, 'entry_price': 145.82997131347656, 'exit_price': 132.5109100341797, 'gross_return': -0.09133281148815485, 'net_return': -0.09314923719799006, 'hold_bars': 12, 'forced_exit': False, 'meta': {'buy_conf': 0.4886996732506806, 'reason': 'sell_agent', 'sell_conf': 0.7537934484876248, 'sell_q0': 0.06523077189922333, 'sell_q1': 0.08760972321033478, 'sell_margin': 0.02237895131111145}}, {'entry_idx': 135, 'exit_idx': 151, 'entry_price': 139.0128173828125, 'exit_price': 151.15354919433594, 'gross_return': 0.08733534101456479, 'net_return': 0.0851617576678767, 'hold_bars': 16, 'forced_exit': False, 'meta': {'buy_conf': 0.4886996732506806, 'reason': 'sell_agent', 'sell_conf': 0.936743939839045, 'sell_q0': 0.008411183953285217, 'sell_q1': 0.06231556460261345, 'sell_margin': 0.05390438064932823}}]\n",
            "ENTRY DEBUG: {'checked': 539, 'blocked_trend': 445, 'blocked_latest_entry': 23, 'blocked_conf': 7, 'opened': 64, 'conf_min': 0.34916556097223117, 'conf_max': 0.7190483883581484}\n",
            "SELL DEBUG: {'seen': 469, 'sell_actions': 92}\n",
            "EXIT REASONS: {'time': 20, 'sell_agent': 44}\n",
            "\n",
            "=== TRADE MANAGER (TEST) ===\n",
            "n_steps: 1860\n",
            "segment_len: 372\n",
            "n_trades: 64\n",
            "final_equity: 1.7458026800581703\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.012674308405690126\n",
            "win rate: 0.578125\n",
            "min/median/max net: -0.2515424823319645 0.009336668718870067 0.2732439362825714\n",
            "median hold bars: 17.0\n",
            "top 5 net: [0.14007444 0.16812953 0.21336543 0.2554038  0.27324394]\n",
            "bottom 5 net: [-0.25154248 -0.15082036 -0.1068503  -0.10245507 -0.10114737]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 49, 'entry_price': 168.69102478027344, 'exit_price': 151.9320831298828, 'gross_return': -0.09934696687165007, 'net_return': -0.10114737228487358, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.45140584156165353, 'reason': 'time'}}\n",
            "{'entry_idx': 78, 'exit_idx': 95, 'entry_price': 153.17129516601562, 'exit_price': 147.94802856445312, 'gross_return': -0.034100818928907, 'net_return': -0.03603165139186815, 'hold_bars': 17, 'forced_exit': False, 'meta': {'buy_conf': 0.4694508576365352, 'reason': 'sell_agent', 'sell_conf': 0.9857361018300572, 'sell_q0': 0.06598176807165146, 'sell_q1': 0.15069490671157837, 'sell_margin': 0.08471313863992691}}\n",
            "{'entry_idx': 100, 'exit_idx': 112, 'entry_price': 145.82997131347656, 'exit_price': 132.5109100341797, 'gross_return': -0.09133281148815485, 'net_return': -0.09314923719799006, 'hold_bars': 12, 'forced_exit': False, 'meta': {'buy_conf': 0.4886996732506806, 'reason': 'sell_agent', 'sell_conf': 0.7537934484876248, 'sell_q0': 0.06523077189922333, 'sell_q1': 0.08760972321033478, 'sell_margin': 0.02237895131111145}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 1773, 'exit_idx': 1792, 'entry_price': 130.0275421142578, 'exit_price': 130.12681579589844, 'gross_return': 0.0007634819517959547, 'net_return': -0.001237044248625696, 'hold_bars': 19, 'forced_exit': False, 'meta': {'buy_conf': 0.5026480978488047, 'reason': 'sell_agent', 'sell_conf': 0.9559192317125801, 'sell_q0': -0.027755968272686005, 'sell_q1': 0.03377702832221985, 'sell_margin': 0.06153299659490585}}\n",
            "{'entry_idx': 1804, 'exit_idx': 1822, 'entry_price': 139.49600219726562, 'exit_price': 129.2732391357422, 'gross_return': -0.07328355580446698, 'net_return': -0.07513606197641387, 'hold_bars': 18, 'forced_exit': False, 'meta': {'buy_conf': 0.4965437307131889, 'reason': 'sell_agent', 'sell_conf': 0.865779820920636, 'sell_q0': -0.011838734149932861, 'sell_q1': 0.02544424682855606, 'sell_margin': 0.03728298097848892}}\n",
            "{'entry_idx': 1832, 'exit_idx': 1849, 'entry_price': 135.22824096679688, 'exit_price': 130.95057678222656, 'gross_return': -0.03163292041653187, 'net_return': -0.03356868620861919, 'hold_bars': 17, 'forced_exit': False, 'meta': {'buy_conf': 0.4527240995917555, 'reason': 'sell_agent', 'sell_conf': 0.954635856836421, 'sell_q0': -0.02828299254179001, 'sell_q1': 0.032649166882038116, 'sell_margin': 0.060932159423828125}}\n",
            "SELL Î” TRAIN: {'count': 82, 'mean_delta': -0.0008212925749830902, 'median_delta': 0.004466597456485033, 'win_rate': 0.5365853658536586}\n",
            "SELL Î” TEST : {'count': 44, 'mean_delta': -0.00923448707908392, 'median_delta': -0.0023464690893888474, 'win_rate': 0.5}\n",
            "\n",
            "=== SAVED ===\n",
            " - runs/entry_indices_train_sell.npy (HARVESTED)\n",
            " - runs/entry_indices_test_sell.npy (HARVESTED)\n",
            " - runs/trades_buy_only_train.json\n",
            " - runs/trades_buy_only_test.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from trade.trade_manager import TradeManager\n",
        "\n",
        "# -----------------------\n",
        "# SETTINGS\n",
        "# -----------------------\n",
        "SEG_LEN = 1239          # rows per ticker segment from your build_features validation\n",
        "N_SEGS  = 5             # AAPL, MSFT, NVDA, AMZN, GOOGL\n",
        "TRAIN_FRAC = 0.70       # time-based split within each segment\n",
        "\n",
        "# NEW: entry harvesting (for SellAgent training)\n",
        "TOPK_PER_SEG_TRAIN = 80     # try 50â€“150\n",
        "TOPK_PER_SEG_TEST  = 40     # fewer is fine for eval\n",
        "MIN_GAP_TRAIN = None        # None => defaults inside TradeManager\n",
        "MIN_GAP_TEST  = None\n",
        "USE_CONF_SCORE = False      # False => uses q1-q0 margin (recommended)\n",
        "\n",
        "# -----------------------\n",
        "# BUILD TRAIN/TEST INDEX (per segment, no leakage)\n",
        "# -----------------------\n",
        "train_len = int(SEG_LEN * TRAIN_FRAC)\n",
        "\n",
        "train_idx = []\n",
        "test_idx = []\n",
        "\n",
        "for seg in range(N_SEGS):\n",
        "    start = seg * SEG_LEN\n",
        "    train_idx.extend(range(start, start + train_len))\n",
        "    test_idx.extend(range(start + train_len, start + SEG_LEN))\n",
        "\n",
        "train_idx = np.array(train_idx, dtype=np.int32)\n",
        "test_idx  = np.array(test_idx, dtype=np.int32)\n",
        "\n",
        "X_train = features[train_idx]\n",
        "p_train = prices[train_idx]\n",
        "X_test  = features[test_idx]\n",
        "p_test  = prices[test_idx]\n",
        "\n",
        "# Segment length inside each split subset (since we concatenated segments in order)\n",
        "SEG_TRAIN = train_len\n",
        "SEG_TEST  = SEG_LEN - train_len\n",
        "\n",
        "print(\"=== DATA SPLIT ===\")\n",
        "print(\"features:\", features.shape, \"prices:\", prices.shape)\n",
        "print(\"SEG_LEN:\", SEG_LEN, \"N_SEGS:\", N_SEGS, \"TRAIN_FRAC:\", TRAIN_FRAC)\n",
        "print(\"train_len per seg:\", SEG_TRAIN, \"test_len per seg:\", SEG_TEST)\n",
        "print(\"X_train:\", X_train.shape, \"p_train:\", p_train.shape)\n",
        "print(\"X_test :\", X_test.shape,  \"p_test :\", p_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# HELPER: run TM + debug logs (unchanged backtest)\n",
        "# -----------------------\n",
        "def run_tm(name: str, X: np.ndarray, p: np.ndarray, seg_len: int, sell_agent=None):\n",
        "    tm = TradeManager(\n",
        "        buy_agent=agent,            # trained buy agent\n",
        "        sell_agent=sell_agent,      # optional\n",
        "        state=X,\n",
        "        prices=p,\n",
        "        reward=cfg.reward,\n",
        "        trade=cfg.trade_manager,\n",
        "        segment_len=seg_len,        # IMPORTANT for boundary correctness\n",
        "    )\n",
        "\n",
        "    res = tm.run()\n",
        "    trades = res[\"trades\"]\n",
        "\n",
        "    print(\"Sell seen:\", tm._sell_debug[\"seen\"], \"Sell actions:\", tm._sell_debug[\"sell_actions\"])\n",
        "\n",
        "    from collections import Counter\n",
        "    reasons = Counter([t[\"meta\"].get(\"reason\", \"none\") for t in res[\"trades\"]])\n",
        "    print(\"Exit reasons:\", dict(reasons))\n",
        "    print(\"Non-time exits:\", [t for t in res[\"trades\"] if t[\"meta\"].get(\"reason\") != \"time\"][:3])\n",
        "\n",
        "    print(\"ENTRY DEBUG:\", res[\"entry_debug\"])\n",
        "    print(\"SELL DEBUG:\", res[\"sell_debug\"])\n",
        "    print(\"EXIT REASONS:\", res.get(\"exit_reasons\"))\n",
        "\n",
        "\n",
        "    print(f\"\\n=== TRADE MANAGER ({name}) ===\")\n",
        "    print(\"n_steps:\", len(p))\n",
        "    print(\"segment_len:\", seg_len)\n",
        "    print(\"n_trades:\", res[\"n_trades\"])\n",
        "    print(\"final_equity:\", res[\"final_equity\"])\n",
        "\n",
        "    # Boundary-crossing check (must be 0)\n",
        "    if trades:\n",
        "        cross = sum((t[\"entry_idx\"] // seg_len) != (t[\"exit_idx\"] // seg_len) for t in trades)\n",
        "    else:\n",
        "        cross = 0\n",
        "    print(\"Trades crossing segment boundary:\", cross)\n",
        "\n",
        "    # Return stats\n",
        "    if trades:\n",
        "        net = np.array([t[\"net_return\"] for t in trades], dtype=float)\n",
        "        hold = np.array([t[\"hold_bars\"] for t in trades], dtype=float)\n",
        "\n",
        "        print(\"avg net return:\", float(net.mean()))\n",
        "        print(\"win rate:\", float((net > 0).mean()))\n",
        "        print(\"min/median/max net:\", float(net.min()), float(np.median(net)), float(net.max()))\n",
        "        print(\"median hold bars:\", float(np.median(hold)))\n",
        "        print(\"top 5 net:\", np.sort(net)[-5:])\n",
        "        print(\"bottom 5 net:\", np.sort(net)[:5])\n",
        "\n",
        "        # A few sample trades (head + tail)\n",
        "        print(\"\\nSample trades (first 3):\")\n",
        "        for t in trades[:3]:\n",
        "            print(t)\n",
        "        print(\"\\nSample trades (last 3):\")\n",
        "        for t in trades[-3:]:\n",
        "            print(t)\n",
        "    else:\n",
        "        print(\"No trades produced. Try lowering buy_min_confidence or disabling trend filter.\")\n",
        "\n",
        "    return tm, res\n",
        "\n",
        "# -----------------------\n",
        "# NEW: Harvest entry indices for SellAgent training (no trade execution)\n",
        "# -----------------------\n",
        "# def harvest_entries(name: str, tm: TradeManager, topk_per_seg: int, min_gap=None, use_confidence_score=False):\n",
        "#     entries = tm.collect_entry_indices_topk(\n",
        "#         topk_per_segment=topk_per_seg,\n",
        "#         min_gap=min_gap,\n",
        "#         use_confidence_score=use_confidence_score,\n",
        "#     )\n",
        "#     entries = np.array(entries, dtype=np.int32)\n",
        "\n",
        "#     # Quick sanity: segment boundary + horizon feasibility check (should hold by construction)\n",
        "#     horizon = int(cfg.trade_manager.sell_horizon)\n",
        "#     if len(entries) > 0:\n",
        "#         seg_ok = np.all((entries % tm.segment_len) <= (tm.segment_len - 1 - horizon))\n",
        "#     else:\n",
        "#         seg_ok = True\n",
        "\n",
        "#     print(f\"\\n=== ENTRY HARVEST ({name}) ===\")\n",
        "#     print(\"topk_per_segment:\", topk_per_seg, \"min_gap:\", min_gap, \"use_conf_score:\", use_confidence_score)\n",
        "#     print(\"n_entries:\", len(entries))\n",
        "#     print(\"horizon:\", horizon, \"segment_len:\", tm.segment_len, \"feasible_in_segment:\", bool(seg_ok))\n",
        "#     if len(entries) > 0:\n",
        "#         print(\"first 10:\", entries[:10].tolist())\n",
        "#         print(\"last 10 :\", entries[-10:].tolist())\n",
        "\n",
        "#     return entries\n",
        "\n",
        "# -----------------------\n",
        "# RUN TRAIN + TEST (backtest as before)\n",
        "# -----------------------\n",
        "tm_train_sell, res_train_sell = run_tm(\"TRAIN\", X_train, p_train, seg_len=SEG_TRAIN, sell_agent=sell_agent)\n",
        "tm_test_sell,  res_test_sell  = run_tm(\"TEST\",  X_test,  p_test,  seg_len=SEG_TEST,  sell_agent=sell_agent)\n",
        "\n",
        "stats_train = compute_mean_delta(\n",
        "    trades=res_train_sell[\"trades\"],\n",
        "    prices=p_train,\n",
        "    horizon=cfg.trade_manager.sell_horizon,\n",
        "    tc=cfg.reward.transaction_cost,\n",
        ")\n",
        "\n",
        "stats_test = compute_mean_delta(\n",
        "    trades=res_test_sell[\"trades\"],\n",
        "    prices=p_test,\n",
        "    horizon=cfg.trade_manager.sell_horizon,\n",
        "    tc=cfg.reward.transaction_cost,\n",
        ")\n",
        "\n",
        "print(\"SELL Î” TRAIN:\", stats_train)\n",
        "print(\"SELL Î” TEST :\", stats_test)\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# HARVEST ENTRIES (NEW LOGIC) â€” use these for SellEnv training\n",
        "# -----------------------\n",
        "# train_entries_sell = harvest_entries(\n",
        "#     \"TRAIN\",\n",
        "#     tm_train_sell,\n",
        "#     topk_per_seg=TOPK_PER_SEG_TRAIN,\n",
        "#     min_gap=MIN_GAP_TRAIN,\n",
        "#     use_confidence_score=USE_CONF_SCORE,\n",
        "# )\n",
        "\n",
        "# test_entries_sell = harvest_entries(\n",
        "#     \"TEST\",\n",
        "#     tm_test_sell,\n",
        "#     topk_per_seg=TOPK_PER_SEG_TEST,\n",
        "#     min_gap=MIN_GAP_TEST,\n",
        "#     use_confidence_score=USE_CONF_SCORE,\n",
        "# )\n",
        "\n",
        "# -----------------------\n",
        "# SAVE ARTIFACTS (into out_dir)\n",
        "# -----------------------\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "train_entries_path = os.path.join(out_dir, \"entry_indices_train_sell.npy\")\n",
        "test_entries_path  = os.path.join(out_dir, \"entry_indices_test_sell.npy\")\n",
        "\n",
        "# NEW: save harvested entries (not trade entries)\n",
        "np.save(train_entries_path, train_entries)\n",
        "np.save(test_entries_path,  test_entries)\n",
        "\n",
        "train_trades_sell_json = os.path.join(out_dir, \"trades_buy_sell_train.json\")\n",
        "test_trades_sell_json  = os.path.join(out_dir, \"trades_buy_sell_test.json\")\n",
        "\n",
        "with open(train_trades_json, \"w\") as f:\n",
        "    json.dump(res_train[\"trades\"], f, indent=2)\n",
        "\n",
        "with open(test_trades_json, \"w\") as f:\n",
        "    json.dump(res_test[\"trades\"], f, indent=2)\n",
        "\n",
        "print(\"\\n=== SAVED ===\")\n",
        "print(\" -\", train_entries_path, \"(HARVESTED)\")\n",
        "print(\" -\", test_entries_path,  \"(HARVESTED)\")\n",
        "print(\" -\", train_trades_json)\n",
        "print(\" -\", test_trades_json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50bd130b",
      "metadata": {},
      "source": [
        "## How to \"pass args\" in a notebook\n",
        "\n",
        "Instead of CLI args, edit the **Parameters** cell.\n",
        "\n",
        "If you really want args-style overrides, you can do:\n",
        "\n",
        "```python\n",
        "import os\n",
        "config_path = os.getenv(\"CFG\", config_path)\n",
        "features_npy = os.getenv(\"FEAT\", features_npy)\n",
        "prices_npy = os.getenv(\"PRICES\", prices_npy)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263d95cb",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3.10.14",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
