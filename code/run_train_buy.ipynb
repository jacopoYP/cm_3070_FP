{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Buy DDQN training (notebook version)\n",
        "\n",
        "Converted from your `run_train_buy.py`-style script.\n",
        "\n",
        "1. Edit paths in **Parameters**.\n",
        "2. Run cells top-to-bottom.\n",
        "3. Model + diagnostics saved under `out_dir/<run_id>/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters (edit these)\n",
        "config_path = \"config.yaml\"                 # path to your YAML config\n",
        "features_npy = \"data/features.npy\"      # (n_steps, state_dim)\n",
        "prices_npy = \"data/prices.npy\"          # (n_steps,)\n",
        "out_dir = \"runs\"                            # output root folder\n",
        "run_id = None                               # set to a string to override, or leave None for timestamp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# If you run this notebook from outside your project root, you may need:\n",
        "# import sys\n",
        "# sys.path.append(\"/absolute/path/to/clean_trading_rl\")\n",
        "\n",
        "from core.config import load_config\n",
        "from agents.ddqn_agent import DDQNAgent\n",
        "from envs.buy_env import BuyEnv\n",
        "from diagnostics.q_gap import compute_q_gap, plot_q_gap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features: (6195, 12) prices: (6195,)\n",
            "state_dim: 12 n_actions: 2\n"
          ]
        }
      ],
      "source": [
        "# Load config + data\n",
        "cfg = load_config(config_path)\n",
        "\n",
        "features = np.load(features_npy)\n",
        "prices = np.load(prices_npy)\n",
        "\n",
        "cfg.agent.state_dim = int(features.shape[1])\n",
        "cfg.agent.n_actions = 2\n",
        "\n",
        "env = BuyEnv(features, prices, cfg.reward, cfg.trade_manager)\n",
        "agent = DDQNAgent(cfg.agent)\n",
        "\n",
        "print(\"features:\", features.shape, \"prices:\", prices.shape)\n",
        "print(\"state_dim:\", cfg.agent.state_dim, \"n_actions:\", cfg.agent.n_actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_path: runs/20260120-153602\n"
          ]
        }
      ],
      "source": [
        "# Output folder\n",
        "if run_id is None:\n",
        "    run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "run_path = os.path.join(out_dir, run_id)\n",
        "os.makedirs(run_path, exist_ok=True)\n",
        "\n",
        "print(\"run_path:\", run_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ced7936b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.is_inference_mode_enabled(): False\n",
            "torch.is_grad_enabled(): True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"torch.is_inference_mode_enabled():\", torch.is_inference_mode_enabled())\n",
        "print(\"torch.is_grad_enabled():\", torch.is_grad_enabled())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BUY] ep=1/200 reward=1.0488 eps=0.988 total_steps=500 learn_steps=263 loss=0.007416038308292627\n",
            "target sync @ 500 loss 0.014199010096490383\n",
            "[BUY] ep=2/200 reward=1.0622 eps=0.976 total_steps=1000 learn_steps=763 loss=0.00021972361719235778\n",
            "target sync @ 1000 loss 0.00024648397811688483\n",
            "[BUY] ep=3/200 reward=0.9759 eps=0.964 total_steps=1500 learn_steps=1263 loss=0.0001806607615435496\n",
            "target sync @ 1500 loss 0.0004453724541235715\n",
            "[BUY] ep=4/200 reward=1.3858 eps=0.953 total_steps=2000 learn_steps=1763 loss=0.00022078111942391843\n",
            "target sync @ 2000 loss 0.00012917084677610546\n",
            "[BUY] ep=5/200 reward=1.1455 eps=0.941 total_steps=2500 learn_steps=2263 loss=0.0005821251543238759\n",
            "target sync @ 2500 loss 0.0001174423232441768\n",
            "[BUY] ep=6/200 reward=0.8265 eps=0.929 total_steps=3000 learn_steps=2763 loss=6.814674998167902e-05\n",
            "target sync @ 3000 loss 0.0004012243007309735\n",
            "[BUY] ep=7/200 reward=1.1505 eps=0.917 total_steps=3500 learn_steps=3263 loss=0.00021706799452658743\n",
            "target sync @ 3500 loss 0.00012835462985094637\n",
            "[BUY] ep=8/200 reward=0.7981 eps=0.905 total_steps=4000 learn_steps=3763 loss=0.00017927124281413853\n",
            "target sync @ 4000 loss 0.00015379555406980217\n",
            "[BUY] ep=9/200 reward=0.7756 eps=0.893 total_steps=4500 learn_steps=4263 loss=0.00011149008059874177\n",
            "target sync @ 4500 loss 0.00030200398759916425\n",
            "[BUY] ep=10/200 reward=1.2062 eps=0.881 total_steps=5000 learn_steps=4763 loss=0.00035240116994827986\n",
            "target sync @ 5000 loss 0.00012935078120790422\n",
            "[BUY] ep=11/200 reward=0.8021 eps=0.869 total_steps=5500 learn_steps=5263 loss=0.000115987888420932\n",
            "target sync @ 5500 loss 0.000330430077156052\n",
            "[BUY] ep=12/200 reward=0.7657 eps=0.858 total_steps=6000 learn_steps=5763 loss=0.000677073432598263\n",
            "target sync @ 6000 loss 0.0002466936130076647\n",
            "[BUY] ep=13/200 reward=0.7553 eps=0.846 total_steps=6500 learn_steps=6263 loss=0.0002768646809272468\n",
            "target sync @ 6500 loss 0.0008953114738687873\n",
            "[BUY] ep=14/200 reward=0.8831 eps=0.834 total_steps=7000 learn_steps=6763 loss=0.0002062593266600743\n",
            "target sync @ 7000 loss 6.098107041907497e-05\n",
            "[BUY] ep=15/200 reward=0.9879 eps=0.822 total_steps=7500 learn_steps=7263 loss=0.00032469534198753536\n",
            "target sync @ 7500 loss 0.00028590220608748496\n",
            "[BUY] ep=16/200 reward=1.0191 eps=0.810 total_steps=8000 learn_steps=7763 loss=0.00031541052157990634\n",
            "target sync @ 8000 loss 0.0001336671266471967\n",
            "[BUY] ep=17/200 reward=1.0520 eps=0.798 total_steps=8500 learn_steps=8263 loss=0.00020122693968005478\n",
            "target sync @ 8500 loss 4.116463242098689e-05\n",
            "[BUY] ep=18/200 reward=1.0623 eps=0.786 total_steps=9000 learn_steps=8763 loss=0.00010308994387742132\n",
            "target sync @ 9000 loss 0.0003181768115609884\n",
            "[BUY] ep=19/200 reward=0.7391 eps=0.774 total_steps=9500 learn_steps=9263 loss=0.0002157279523089528\n",
            "target sync @ 9500 loss 8.593988604843616e-05\n",
            "[BUY] ep=20/200 reward=1.0082 eps=0.762 total_steps=10000 learn_steps=9763 loss=8.13507940620184e-05\n",
            "target sync @ 10000 loss 0.0001581865071784705\n",
            "[BUY] ep=21/200 reward=0.9451 eps=0.751 total_steps=10500 learn_steps=10263 loss=0.0004411787958815694\n",
            "target sync @ 10500 loss 0.0002598936844151467\n",
            "[BUY] ep=22/200 reward=1.0686 eps=0.739 total_steps=11000 learn_steps=10763 loss=0.00015686870028730482\n",
            "target sync @ 11000 loss 0.00015240273205563426\n",
            "[BUY] ep=23/200 reward=0.5388 eps=0.727 total_steps=11500 learn_steps=11263 loss=0.0001647325698286295\n",
            "target sync @ 11500 loss 4.084481770405546e-05\n",
            "[BUY] ep=24/200 reward=1.1783 eps=0.715 total_steps=12000 learn_steps=11763 loss=0.0001968360593309626\n",
            "target sync @ 12000 loss 0.00013768955250270665\n",
            "[BUY] ep=25/200 reward=1.2616 eps=0.703 total_steps=12500 learn_steps=12263 loss=0.0007740533910691738\n",
            "target sync @ 12500 loss 0.0010269004851579666\n",
            "[BUY] ep=26/200 reward=0.5856 eps=0.691 total_steps=13000 learn_steps=12763 loss=0.00018420518608763814\n",
            "target sync @ 13000 loss 0.00015829206677153707\n",
            "[BUY] ep=27/200 reward=0.8116 eps=0.679 total_steps=13500 learn_steps=13263 loss=0.00019624480046331882\n",
            "target sync @ 13500 loss 0.0004570449818857014\n",
            "[BUY] ep=28/200 reward=0.8690 eps=0.667 total_steps=14000 learn_steps=13763 loss=9.87702514976263e-05\n",
            "target sync @ 14000 loss 0.00036045233719050884\n",
            "[BUY] ep=29/200 reward=1.0125 eps=0.656 total_steps=14500 learn_steps=14263 loss=0.00018617772730067372\n",
            "target sync @ 14500 loss 0.00014434746117331088\n",
            "[BUY] ep=30/200 reward=1.0763 eps=0.644 total_steps=15000 learn_steps=14763 loss=0.0003634542808867991\n",
            "target sync @ 15000 loss 3.409598866710439e-05\n",
            "[BUY] ep=31/200 reward=0.8142 eps=0.632 total_steps=15500 learn_steps=15263 loss=6.0414717154344544e-05\n",
            "target sync @ 15500 loss 0.00016987828712444752\n",
            "[BUY] ep=32/200 reward=0.7217 eps=0.620 total_steps=16000 learn_steps=15763 loss=6.499868322862312e-05\n",
            "target sync @ 16000 loss 0.00022081786300987005\n",
            "[BUY] ep=33/200 reward=0.9136 eps=0.608 total_steps=16500 learn_steps=16263 loss=9.24713458516635e-05\n",
            "target sync @ 16500 loss 0.0001393877755617723\n",
            "[BUY] ep=34/200 reward=1.0118 eps=0.596 total_steps=17000 learn_steps=16763 loss=6.206788384588435e-05\n",
            "target sync @ 17000 loss 0.00013770937221124768\n",
            "[BUY] ep=35/200 reward=0.4524 eps=0.584 total_steps=17500 learn_steps=17263 loss=0.00040536289452575147\n",
            "target sync @ 17500 loss 0.00017011103045661002\n",
            "[BUY] ep=36/200 reward=1.0748 eps=0.573 total_steps=18000 learn_steps=17763 loss=0.0002558306441642344\n",
            "target sync @ 18000 loss 0.00017433453467674553\n",
            "[BUY] ep=37/200 reward=0.6725 eps=0.561 total_steps=18500 learn_steps=18263 loss=0.0002108524349750951\n",
            "target sync @ 18500 loss 5.35078979737591e-05\n",
            "[BUY] ep=38/200 reward=0.8679 eps=0.549 total_steps=19000 learn_steps=18763 loss=9.476042032474652e-05\n",
            "target sync @ 19000 loss 4.5161832531448454e-05\n",
            "[BUY] ep=39/200 reward=0.9773 eps=0.537 total_steps=19500 learn_steps=19263 loss=0.00016975580365397036\n",
            "target sync @ 19500 loss 0.0001774282136466354\n",
            "[BUY] ep=40/200 reward=1.1565 eps=0.525 total_steps=20000 learn_steps=19763 loss=0.00022617248760070652\n",
            "target sync @ 20000 loss 0.0001739280705805868\n",
            "[BUY] ep=41/200 reward=0.8521 eps=0.513 total_steps=20500 learn_steps=20263 loss=8.87158967088908e-05\n",
            "target sync @ 20500 loss 0.0004996441421099007\n",
            "[BUY] ep=42/200 reward=0.9465 eps=0.501 total_steps=21000 learn_steps=20763 loss=0.000343972205882892\n",
            "target sync @ 21000 loss 0.0001056960754795\n",
            "[BUY] ep=43/200 reward=0.7357 eps=0.489 total_steps=21500 learn_steps=21263 loss=4.957745841238648e-05\n",
            "target sync @ 21500 loss 0.00033237237948924303\n",
            "[BUY] ep=44/200 reward=1.2923 eps=0.478 total_steps=22000 learn_steps=21763 loss=0.0004345513880252838\n",
            "target sync @ 22000 loss 4.760214505949989e-05\n",
            "[BUY] ep=45/200 reward=0.9158 eps=0.466 total_steps=22500 learn_steps=22263 loss=0.0005205846391618252\n",
            "target sync @ 22500 loss 4.001077468274161e-05\n",
            "[BUY] ep=46/200 reward=1.0971 eps=0.454 total_steps=23000 learn_steps=22763 loss=0.0001620657421881333\n",
            "target sync @ 23000 loss 0.0002304376830579713\n",
            "[BUY] ep=47/200 reward=0.9425 eps=0.442 total_steps=23500 learn_steps=23263 loss=0.0002491447958163917\n",
            "target sync @ 23500 loss 5.7293091231258586e-05\n",
            "[BUY] ep=48/200 reward=1.1637 eps=0.430 total_steps=24000 learn_steps=23763 loss=0.00015552001423202455\n",
            "target sync @ 24000 loss 0.0003842850564979017\n",
            "[BUY] ep=49/200 reward=1.0625 eps=0.418 total_steps=24500 learn_steps=24263 loss=7.67586097936146e-05\n",
            "target sync @ 24500 loss 0.00020351020793896168\n",
            "[BUY] ep=50/200 reward=0.9185 eps=0.406 total_steps=25000 learn_steps=24763 loss=6.909210060257465e-05\n",
            "target sync @ 25000 loss 4.993864058633335e-05\n",
            "[BUY] ep=51/200 reward=1.0894 eps=0.394 total_steps=25500 learn_steps=25263 loss=0.00040473401895724237\n",
            "target sync @ 25500 loss 8.764045196585357e-05\n",
            "[BUY] ep=52/200 reward=1.1772 eps=0.383 total_steps=26000 learn_steps=25763 loss=3.7849291402380913e-05\n",
            "target sync @ 26000 loss 0.0005021854885853827\n",
            "[BUY] ep=53/200 reward=0.6449 eps=0.371 total_steps=26500 learn_steps=26263 loss=4.192435517325066e-05\n",
            "target sync @ 26500 loss 0.00022436899598687887\n",
            "[BUY] ep=54/200 reward=0.9320 eps=0.359 total_steps=27000 learn_steps=26763 loss=0.00010120902152266353\n",
            "target sync @ 27000 loss 0.00037059083115309477\n",
            "[BUY] ep=55/200 reward=0.5442 eps=0.347 total_steps=27500 learn_steps=27263 loss=0.00026130833430215716\n",
            "target sync @ 27500 loss 0.00016718967526685447\n",
            "[BUY] ep=56/200 reward=0.6692 eps=0.335 total_steps=28000 learn_steps=27763 loss=0.00010882150672841817\n",
            "target sync @ 28000 loss 0.00036908622132614255\n",
            "[BUY] ep=57/200 reward=0.6910 eps=0.323 total_steps=28500 learn_steps=28263 loss=0.00026967067969962955\n",
            "target sync @ 28500 loss 0.0001287455961573869\n",
            "[BUY] ep=58/200 reward=0.8408 eps=0.311 total_steps=29000 learn_steps=28763 loss=0.00010842413757927716\n",
            "target sync @ 29000 loss 0.00033596577122807503\n",
            "[BUY] ep=59/200 reward=0.8747 eps=0.299 total_steps=29500 learn_steps=29263 loss=3.447446215432137e-05\n",
            "target sync @ 29500 loss 7.724353781668469e-05\n",
            "[BUY] ep=60/200 reward=0.9003 eps=0.288 total_steps=30000 learn_steps=29763 loss=0.00016672047786414623\n",
            "target sync @ 30000 loss 7.92226564954035e-05\n",
            "[BUY] ep=61/200 reward=0.6225 eps=0.276 total_steps=30500 learn_steps=30263 loss=0.0003243345709051937\n",
            "target sync @ 30500 loss 0.00010256907262373716\n",
            "[BUY] ep=62/200 reward=1.5032 eps=0.264 total_steps=31000 learn_steps=30763 loss=5.029099702369422e-05\n",
            "target sync @ 31000 loss 0.000254227954428643\n",
            "[BUY] ep=63/200 reward=0.8585 eps=0.252 total_steps=31500 learn_steps=31263 loss=2.9328199161682278e-05\n",
            "target sync @ 31500 loss 0.0005022014956921339\n",
            "[BUY] ep=64/200 reward=0.7631 eps=0.240 total_steps=32000 learn_steps=31763 loss=0.00032573204953223467\n",
            "target sync @ 32000 loss 0.0007464673253707588\n",
            "[BUY] ep=65/200 reward=0.2766 eps=0.228 total_steps=32500 learn_steps=32263 loss=0.00032796943560242653\n",
            "target sync @ 32500 loss 0.0002294345322297886\n",
            "[BUY] ep=66/200 reward=0.4465 eps=0.216 total_steps=33000 learn_steps=32763 loss=0.00016920894267968833\n",
            "target sync @ 33000 loss 0.0001018949769786559\n",
            "[BUY] ep=67/200 reward=0.9530 eps=0.204 total_steps=33500 learn_steps=33263 loss=0.00027633365243673325\n",
            "target sync @ 33500 loss 0.00019787992641795427\n",
            "[BUY] ep=68/200 reward=1.0593 eps=0.193 total_steps=34000 learn_steps=33763 loss=8.250126847997308e-05\n",
            "target sync @ 34000 loss 0.0003095993015449494\n",
            "[BUY] ep=69/200 reward=0.6807 eps=0.181 total_steps=34500 learn_steps=34263 loss=9.094255801755935e-05\n",
            "target sync @ 34500 loss 0.00024181783373933285\n",
            "[BUY] ep=70/200 reward=0.6838 eps=0.169 total_steps=35000 learn_steps=34763 loss=0.0002793415333144367\n",
            "target sync @ 35000 loss 0.000491008220706135\n",
            "[BUY] ep=71/200 reward=0.7500 eps=0.157 total_steps=35500 learn_steps=35263 loss=8.30136050353758e-05\n",
            "target sync @ 35500 loss 0.0008696684380993247\n",
            "[BUY] ep=72/200 reward=0.4661 eps=0.145 total_steps=36000 learn_steps=35763 loss=0.0001700208376860246\n",
            "target sync @ 36000 loss 0.0003653743478935212\n",
            "[BUY] ep=73/200 reward=0.8466 eps=0.133 total_steps=36500 learn_steps=36263 loss=2.5557525077601895e-05\n",
            "target sync @ 36500 loss 0.0003422819427214563\n",
            "[BUY] ep=74/200 reward=0.7435 eps=0.121 total_steps=37000 learn_steps=36763 loss=0.0003109176177531481\n",
            "target sync @ 37000 loss 5.516237433766946e-05\n",
            "[BUY] ep=75/200 reward=0.7683 eps=0.109 total_steps=37500 learn_steps=37263 loss=0.0008634681580588222\n",
            "target sync @ 37500 loss 0.00011853429896291345\n",
            "[BUY] ep=76/200 reward=0.5854 eps=0.098 total_steps=38000 learn_steps=37763 loss=0.00026209483621641994\n",
            "target sync @ 38000 loss 0.0005618510767817497\n",
            "[BUY] ep=77/200 reward=0.9786 eps=0.086 total_steps=38500 learn_steps=38263 loss=3.175564779667184e-05\n",
            "target sync @ 38500 loss 0.00012353049532976002\n",
            "[BUY] ep=78/200 reward=0.8337 eps=0.074 total_steps=39000 learn_steps=38763 loss=0.00021715032926294953\n",
            "target sync @ 39000 loss 0.00011044996790587902\n",
            "[BUY] ep=79/200 reward=0.4492 eps=0.062 total_steps=39500 learn_steps=39263 loss=0.000420567172113806\n",
            "target sync @ 39500 loss 8.728527609491721e-05\n",
            "[BUY] ep=80/200 reward=0.6468 eps=0.050 total_steps=40000 learn_steps=39763 loss=8.698149758856744e-05\n",
            "target sync @ 40000 loss 3.123666829196736e-05\n",
            "[BUY] ep=81/200 reward=0.6885 eps=0.050 total_steps=40500 learn_steps=40263 loss=0.00016314179811161011\n",
            "target sync @ 40500 loss 0.00017061697144526988\n",
            "[BUY] ep=82/200 reward=0.6257 eps=0.050 total_steps=41000 learn_steps=40763 loss=2.0374614905449562e-05\n",
            "target sync @ 41000 loss 0.0001194955621031113\n",
            "[BUY] ep=83/200 reward=0.8129 eps=0.050 total_steps=41500 learn_steps=41263 loss=0.0005218040314503014\n",
            "target sync @ 41500 loss 0.00011836274643428624\n",
            "[BUY] ep=84/200 reward=0.8125 eps=0.050 total_steps=42000 learn_steps=41763 loss=2.3164786398410797e-05\n",
            "target sync @ 42000 loss 8.155014074873179e-05\n",
            "[BUY] ep=85/200 reward=0.5150 eps=0.050 total_steps=42500 learn_steps=42263 loss=0.00012642920773942024\n",
            "target sync @ 42500 loss 0.00015665162936784327\n",
            "[BUY] ep=86/200 reward=0.4477 eps=0.050 total_steps=43000 learn_steps=42763 loss=4.4347856601234525e-05\n",
            "target sync @ 43000 loss 0.00015927308413665742\n",
            "[BUY] ep=87/200 reward=0.6888 eps=0.050 total_steps=43500 learn_steps=43263 loss=4.1829192923614755e-05\n",
            "target sync @ 43500 loss 0.0001555989438202232\n",
            "[BUY] ep=88/200 reward=0.8473 eps=0.050 total_steps=44000 learn_steps=43763 loss=0.0005154121899977326\n",
            "target sync @ 44000 loss 9.110472456086427e-05\n",
            "[BUY] ep=89/200 reward=0.7346 eps=0.050 total_steps=44500 learn_steps=44263 loss=8.808254642644897e-05\n",
            "target sync @ 44500 loss 0.0002000613894779235\n",
            "[BUY] ep=90/200 reward=0.6249 eps=0.050 total_steps=45000 learn_steps=44763 loss=0.00031715602381154895\n",
            "target sync @ 45000 loss 0.00021436184761114419\n",
            "[BUY] ep=91/200 reward=0.4980 eps=0.050 total_steps=45500 learn_steps=45263 loss=0.00014283982454799116\n",
            "target sync @ 45500 loss 6.949148519197479e-05\n",
            "[BUY] ep=92/200 reward=0.7726 eps=0.050 total_steps=46000 learn_steps=45763 loss=6.578956526936963e-05\n",
            "target sync @ 46000 loss 0.0002959182020276785\n",
            "[BUY] ep=93/200 reward=0.9530 eps=0.050 total_steps=46500 learn_steps=46263 loss=0.00016335997497662902\n",
            "target sync @ 46500 loss 4.9620230129221454e-05\n",
            "[BUY] ep=94/200 reward=0.3909 eps=0.050 total_steps=47000 learn_steps=46763 loss=0.0002114257513312623\n",
            "target sync @ 47000 loss 3.4612283343449235e-05\n",
            "[BUY] ep=95/200 reward=0.4625 eps=0.050 total_steps=47500 learn_steps=47263 loss=8.119357516989112e-05\n",
            "target sync @ 47500 loss 0.00015923019964247942\n",
            "[BUY] ep=96/200 reward=1.0352 eps=0.050 total_steps=48000 learn_steps=47763 loss=0.00017146005120594054\n",
            "target sync @ 48000 loss 0.0002321664651390165\n",
            "[BUY] ep=97/200 reward=0.3058 eps=0.050 total_steps=48500 learn_steps=48263 loss=0.00029435352189466357\n",
            "target sync @ 48500 loss 8.402140520047396e-05\n",
            "[BUY] ep=98/200 reward=0.9080 eps=0.050 total_steps=49000 learn_steps=48763 loss=0.00012004784366581589\n",
            "target sync @ 49000 loss 8.062536653596908e-05\n",
            "[BUY] ep=99/200 reward=0.3323 eps=0.050 total_steps=49500 learn_steps=49263 loss=0.00016113388119265437\n",
            "target sync @ 49500 loss 0.0012853743974119425\n",
            "[BUY] ep=100/200 reward=0.5269 eps=0.050 total_steps=50000 learn_steps=49763 loss=3.120394831057638e-05\n",
            "target sync @ 50000 loss 0.00017878823564387858\n",
            "[BUY] ep=101/200 reward=0.5630 eps=0.050 total_steps=50500 learn_steps=50263 loss=3.227027991670184e-05\n",
            "target sync @ 50500 loss 0.00013771334488410503\n",
            "[BUY] ep=102/200 reward=0.9134 eps=0.050 total_steps=51000 learn_steps=50763 loss=3.223391104256734e-05\n",
            "target sync @ 51000 loss 5.023023913963698e-05\n",
            "[BUY] ep=103/200 reward=0.6376 eps=0.050 total_steps=51500 learn_steps=51263 loss=0.00025838802685029805\n",
            "target sync @ 51500 loss 0.00013970481813885272\n",
            "[BUY] ep=104/200 reward=0.6207 eps=0.050 total_steps=52000 learn_steps=51763 loss=0.00016031006816774607\n",
            "target sync @ 52000 loss 0.0009219521889463067\n",
            "[BUY] ep=105/200 reward=1.2991 eps=0.050 total_steps=52500 learn_steps=52263 loss=2.689872053451836e-05\n",
            "target sync @ 52500 loss 7.942053343867883e-05\n",
            "[BUY] ep=106/200 reward=0.7576 eps=0.050 total_steps=53000 learn_steps=52763 loss=0.00021634669974446297\n",
            "target sync @ 53000 loss 2.059216785710305e-05\n",
            "[BUY] ep=107/200 reward=0.6113 eps=0.050 total_steps=53500 learn_steps=53263 loss=1.4185274267219938e-05\n",
            "target sync @ 53500 loss 7.486178947146982e-05\n",
            "[BUY] ep=108/200 reward=0.9702 eps=0.050 total_steps=54000 learn_steps=53763 loss=0.00016373643302358687\n",
            "target sync @ 54000 loss 0.00024424385628663003\n",
            "[BUY] ep=109/200 reward=0.9317 eps=0.050 total_steps=54500 learn_steps=54263 loss=1.2553958185890224e-05\n",
            "target sync @ 54500 loss 5.234927448327653e-05\n",
            "[BUY] ep=110/200 reward=0.5539 eps=0.050 total_steps=55000 learn_steps=54763 loss=0.0005305755767039955\n",
            "target sync @ 55000 loss 2.6220892323181033e-05\n",
            "[BUY] ep=111/200 reward=1.1731 eps=0.050 total_steps=55500 learn_steps=55263 loss=0.00018131930846720934\n",
            "target sync @ 55500 loss 0.00011502980487421155\n",
            "[BUY] ep=112/200 reward=0.5219 eps=0.050 total_steps=56000 learn_steps=55763 loss=3.5631830542115495e-05\n",
            "target sync @ 56000 loss 4.764925688505173e-05\n",
            "[BUY] ep=113/200 reward=0.9352 eps=0.050 total_steps=56500 learn_steps=56263 loss=1.8774760974338278e-05\n",
            "target sync @ 56500 loss 3.080751412198879e-05\n",
            "[BUY] ep=114/200 reward=0.6660 eps=0.050 total_steps=57000 learn_steps=56763 loss=0.0002586705086287111\n",
            "target sync @ 57000 loss 0.00012284085096325725\n",
            "[BUY] ep=115/200 reward=1.1020 eps=0.050 total_steps=57500 learn_steps=57263 loss=0.0002650744572747499\n",
            "target sync @ 57500 loss 1.1611215086304583e-05\n",
            "[BUY] ep=116/200 reward=0.4723 eps=0.050 total_steps=58000 learn_steps=57763 loss=0.00013947099796496332\n",
            "target sync @ 58000 loss 1.8475071556167677e-05\n",
            "[BUY] ep=117/200 reward=0.7663 eps=0.050 total_steps=58500 learn_steps=58263 loss=0.00027660877094604075\n",
            "target sync @ 58500 loss 0.00016462660278193653\n",
            "[BUY] ep=118/200 reward=1.1342 eps=0.050 total_steps=59000 learn_steps=58763 loss=6.640564242843539e-05\n",
            "target sync @ 59000 loss 0.00010754801041912287\n",
            "[BUY] ep=119/200 reward=0.9751 eps=0.050 total_steps=59500 learn_steps=59263 loss=9.283265535486862e-05\n",
            "target sync @ 59500 loss 5.100698035676032e-05\n",
            "[BUY] ep=120/200 reward=0.7801 eps=0.050 total_steps=60000 learn_steps=59763 loss=9.874709940049797e-05\n",
            "target sync @ 60000 loss 0.000161081668920815\n",
            "[BUY] ep=121/200 reward=0.7642 eps=0.050 total_steps=60500 learn_steps=60263 loss=0.0002720148768275976\n",
            "target sync @ 60500 loss 0.00016963538655545563\n",
            "[BUY] ep=122/200 reward=0.5740 eps=0.050 total_steps=61000 learn_steps=60763 loss=3.918808215530589e-05\n",
            "target sync @ 61000 loss 2.6962088668369688e-05\n",
            "[BUY] ep=123/200 reward=0.6352 eps=0.050 total_steps=61500 learn_steps=61263 loss=1.8224618543172255e-05\n",
            "target sync @ 61500 loss 0.000370688212569803\n",
            "[BUY] ep=124/200 reward=0.6630 eps=0.050 total_steps=62000 learn_steps=61763 loss=0.00039306789403781295\n",
            "target sync @ 62000 loss 0.00039842387195676565\n",
            "[BUY] ep=125/200 reward=0.5665 eps=0.050 total_steps=62500 learn_steps=62263 loss=3.835386814898811e-05\n",
            "target sync @ 62500 loss 0.0003185924724675715\n",
            "[BUY] ep=126/200 reward=1.0144 eps=0.050 total_steps=63000 learn_steps=62763 loss=0.00024496420519426465\n",
            "target sync @ 63000 loss 0.00015719037037342787\n",
            "[BUY] ep=127/200 reward=0.8858 eps=0.050 total_steps=63500 learn_steps=63263 loss=0.00048248624079860747\n",
            "target sync @ 63500 loss 0.000305580731946975\n",
            "[BUY] ep=128/200 reward=0.2213 eps=0.050 total_steps=64000 learn_steps=63763 loss=0.00017196616681758314\n",
            "target sync @ 64000 loss 0.0005315619055181742\n",
            "[BUY] ep=129/200 reward=0.3795 eps=0.050 total_steps=64500 learn_steps=64263 loss=3.2524752896279097e-05\n",
            "target sync @ 64500 loss 0.00041911518201231956\n",
            "[BUY] ep=130/200 reward=0.7231 eps=0.050 total_steps=65000 learn_steps=64763 loss=0.00029648421332240105\n",
            "target sync @ 65000 loss 0.0001236958196386695\n",
            "[BUY] ep=131/200 reward=0.6610 eps=0.050 total_steps=65500 learn_steps=65263 loss=3.8241483707679436e-05\n",
            "target sync @ 65500 loss 0.00037204273394308984\n",
            "[BUY] ep=132/200 reward=0.6231 eps=0.050 total_steps=66000 learn_steps=65763 loss=0.00020868098363280296\n",
            "target sync @ 66000 loss 0.00018985038332175463\n",
            "[BUY] ep=133/200 reward=0.1763 eps=0.050 total_steps=66500 learn_steps=66263 loss=0.0002519999397918582\n",
            "target sync @ 66500 loss 0.0006265254924073815\n",
            "[BUY] ep=134/200 reward=0.9563 eps=0.050 total_steps=67000 learn_steps=66763 loss=5.27779120602645e-05\n",
            "target sync @ 67000 loss 0.0003827588225249201\n",
            "[BUY] ep=135/200 reward=0.9949 eps=0.050 total_steps=67500 learn_steps=67263 loss=0.00018114494741894305\n",
            "target sync @ 67500 loss 0.00022893892310094088\n",
            "[BUY] ep=136/200 reward=1.1870 eps=0.050 total_steps=68000 learn_steps=67763 loss=0.0008492201450280845\n",
            "target sync @ 68000 loss 0.0007573091425001621\n",
            "[BUY] ep=137/200 reward=0.3892 eps=0.050 total_steps=68500 learn_steps=68263 loss=0.00012824864825233817\n",
            "target sync @ 68500 loss 0.0001207604946102947\n",
            "[BUY] ep=138/200 reward=0.6360 eps=0.050 total_steps=69000 learn_steps=68763 loss=4.107408312847838e-05\n",
            "target sync @ 69000 loss 3.092260885750875e-05\n",
            "[BUY] ep=139/200 reward=0.9392 eps=0.050 total_steps=69500 learn_steps=69263 loss=0.00013369689986575395\n",
            "target sync @ 69500 loss 0.00029525571153499186\n",
            "[BUY] ep=140/200 reward=0.6621 eps=0.050 total_steps=70000 learn_steps=69763 loss=4.723510937765241e-05\n",
            "target sync @ 70000 loss 0.0001048032208927907\n",
            "[BUY] ep=141/200 reward=0.8751 eps=0.050 total_steps=70500 learn_steps=70263 loss=3.324361750856042e-05\n",
            "target sync @ 70500 loss 5.061452247900888e-05\n",
            "[BUY] ep=142/200 reward=0.8182 eps=0.050 total_steps=71000 learn_steps=70763 loss=0.00011632811219897121\n",
            "target sync @ 71000 loss 8.833407628117129e-05\n",
            "[BUY] ep=143/200 reward=0.9242 eps=0.050 total_steps=71500 learn_steps=71263 loss=0.000217122258618474\n",
            "target sync @ 71500 loss 0.000377024058252573\n",
            "[BUY] ep=144/200 reward=0.6810 eps=0.050 total_steps=72000 learn_steps=71763 loss=6.749529711669311e-05\n",
            "target sync @ 72000 loss 9.344593854621053e-05\n",
            "[BUY] ep=145/200 reward=0.8080 eps=0.050 total_steps=72500 learn_steps=72263 loss=6.798477261327207e-05\n",
            "target sync @ 72500 loss 1.3810516975354403e-05\n",
            "[BUY] ep=146/200 reward=0.9653 eps=0.050 total_steps=73000 learn_steps=72763 loss=2.830778612405993e-05\n",
            "target sync @ 73000 loss 4.844348950427957e-05\n",
            "[BUY] ep=147/200 reward=0.6493 eps=0.050 total_steps=73500 learn_steps=73263 loss=0.00047024039668031037\n",
            "target sync @ 73500 loss 6.978159944992512e-05\n",
            "[BUY] ep=148/200 reward=0.7743 eps=0.050 total_steps=74000 learn_steps=73763 loss=2.4186532755265944e-05\n",
            "target sync @ 74000 loss 1.551160130475182e-05\n",
            "[BUY] ep=149/200 reward=0.7966 eps=0.050 total_steps=74500 learn_steps=74263 loss=0.0001985064591281116\n",
            "target sync @ 74500 loss 1.692760270088911e-05\n",
            "[BUY] ep=150/200 reward=0.7242 eps=0.050 total_steps=75000 learn_steps=74763 loss=0.00015008918126113713\n",
            "target sync @ 75000 loss 0.0009700333466753364\n",
            "[BUY] ep=151/200 reward=1.0561 eps=0.050 total_steps=75500 learn_steps=75263 loss=2.26853480853606e-05\n",
            "target sync @ 75500 loss 0.0003207480476703495\n",
            "[BUY] ep=152/200 reward=0.5750 eps=0.050 total_steps=76000 learn_steps=75763 loss=0.00014312294661067426\n",
            "target sync @ 76000 loss 2.4019771444727667e-05\n",
            "[BUY] ep=153/200 reward=0.8890 eps=0.050 total_steps=76500 learn_steps=76263 loss=0.00024476737598888576\n",
            "target sync @ 76500 loss 3.78239928977564e-05\n",
            "[BUY] ep=154/200 reward=0.5016 eps=0.050 total_steps=77000 learn_steps=76763 loss=0.00020032274187542498\n",
            "target sync @ 77000 loss 0.00023508253798354417\n",
            "[BUY] ep=155/200 reward=1.0025 eps=0.050 total_steps=77500 learn_steps=77263 loss=5.716246596421115e-05\n",
            "target sync @ 77500 loss 0.00014632513921242207\n",
            "[BUY] ep=156/200 reward=0.8991 eps=0.050 total_steps=78000 learn_steps=77763 loss=0.0010062585351988673\n",
            "target sync @ 78000 loss 0.00020976411178708076\n",
            "[BUY] ep=157/200 reward=0.7785 eps=0.050 total_steps=78500 learn_steps=78263 loss=9.782493725651875e-05\n",
            "target sync @ 78500 loss 0.00019701072596944869\n",
            "[BUY] ep=158/200 reward=0.7533 eps=0.050 total_steps=79000 learn_steps=78763 loss=0.00012060430162819102\n",
            "target sync @ 79000 loss 0.00030138264992274344\n",
            "[BUY] ep=159/200 reward=0.8302 eps=0.050 total_steps=79500 learn_steps=79263 loss=0.00023342500207945704\n",
            "target sync @ 79500 loss 0.00021248044504318386\n",
            "[BUY] ep=160/200 reward=1.0587 eps=0.050 total_steps=80000 learn_steps=79763 loss=0.000293222808977589\n",
            "target sync @ 80000 loss 0.00013830407988280058\n",
            "[BUY] ep=161/200 reward=0.6646 eps=0.050 total_steps=80500 learn_steps=80263 loss=1.987730138353072e-05\n",
            "target sync @ 80500 loss 1.854576294135768e-05\n",
            "[BUY] ep=162/200 reward=1.0681 eps=0.050 total_steps=81000 learn_steps=80763 loss=0.00015015972894616425\n",
            "target sync @ 81000 loss 2.143702113244217e-05\n",
            "[BUY] ep=163/200 reward=0.3786 eps=0.050 total_steps=81500 learn_steps=81263 loss=0.0002642969193402678\n",
            "target sync @ 81500 loss 0.00021275857579894364\n",
            "[BUY] ep=164/200 reward=0.2734 eps=0.050 total_steps=82000 learn_steps=81763 loss=0.0002927312452811748\n",
            "target sync @ 82000 loss 3.3865318982861936e-05\n",
            "[BUY] ep=165/200 reward=0.9062 eps=0.050 total_steps=82500 learn_steps=82263 loss=0.0002596392296254635\n",
            "target sync @ 82500 loss 3.466246198513545e-05\n",
            "[BUY] ep=166/200 reward=0.8183 eps=0.050 total_steps=83000 learn_steps=82763 loss=5.623402466881089e-05\n",
            "target sync @ 83000 loss 0.0003921135503333062\n",
            "[BUY] ep=167/200 reward=0.6907 eps=0.050 total_steps=83500 learn_steps=83263 loss=0.0001206097294925712\n",
            "target sync @ 83500 loss 4.015735612483695e-05\n",
            "[BUY] ep=168/200 reward=0.6335 eps=0.050 total_steps=84000 learn_steps=83763 loss=0.0002652947441674769\n",
            "target sync @ 84000 loss 0.0002709631226025522\n",
            "[BUY] ep=169/200 reward=0.4696 eps=0.050 total_steps=84500 learn_steps=84263 loss=9.885471081361175e-05\n",
            "target sync @ 84500 loss 0.00020422485249582678\n",
            "[BUY] ep=170/200 reward=0.7783 eps=0.050 total_steps=85000 learn_steps=84763 loss=4.2269846744602546e-05\n",
            "target sync @ 85000 loss 0.00020684035553131253\n",
            "[BUY] ep=171/200 reward=0.8209 eps=0.050 total_steps=85500 learn_steps=85263 loss=8.897195584722795e-06\n",
            "target sync @ 85500 loss 4.922128209727816e-05\n",
            "[BUY] ep=172/200 reward=0.9330 eps=0.050 total_steps=86000 learn_steps=85763 loss=0.00025312593788839877\n",
            "target sync @ 86000 loss 0.00028624769765883684\n",
            "[BUY] ep=173/200 reward=1.1964 eps=0.050 total_steps=86500 learn_steps=86263 loss=0.00011284741049166769\n",
            "target sync @ 86500 loss 0.00017346830281894654\n",
            "[BUY] ep=174/200 reward=0.5369 eps=0.050 total_steps=87000 learn_steps=86763 loss=2.3112472263164818e-05\n",
            "target sync @ 87000 loss 0.0002156974223908037\n",
            "[BUY] ep=175/200 reward=0.9971 eps=0.050 total_steps=87500 learn_steps=87263 loss=9.769752068677917e-05\n",
            "target sync @ 87500 loss 0.0003342510899528861\n",
            "[BUY] ep=176/200 reward=0.6326 eps=0.050 total_steps=88000 learn_steps=87763 loss=0.00014317753084469587\n",
            "target sync @ 88000 loss 9.594381117494777e-05\n",
            "[BUY] ep=177/200 reward=0.7266 eps=0.050 total_steps=88500 learn_steps=88263 loss=0.00032017467310652137\n",
            "target sync @ 88500 loss 2.1713609385187738e-05\n",
            "[BUY] ep=178/200 reward=0.5813 eps=0.050 total_steps=89000 learn_steps=88763 loss=8.50389915285632e-05\n",
            "target sync @ 89000 loss 0.00012668591807596385\n",
            "[BUY] ep=179/200 reward=0.6708 eps=0.050 total_steps=89500 learn_steps=89263 loss=0.0002781445218715817\n",
            "target sync @ 89500 loss 7.335229747695848e-05\n",
            "[BUY] ep=180/200 reward=0.8012 eps=0.050 total_steps=90000 learn_steps=89763 loss=0.00022449468087870628\n",
            "target sync @ 90000 loss 0.00016474179574288428\n",
            "[BUY] ep=181/200 reward=0.5966 eps=0.050 total_steps=90500 learn_steps=90263 loss=4.997476207790896e-05\n",
            "target sync @ 90500 loss 0.00021584064234048128\n",
            "[BUY] ep=182/200 reward=0.6524 eps=0.050 total_steps=91000 learn_steps=90763 loss=2.00389858946437e-05\n",
            "target sync @ 91000 loss 3.620426286943257e-05\n",
            "[BUY] ep=183/200 reward=0.8311 eps=0.050 total_steps=91500 learn_steps=91263 loss=5.32490957994014e-05\n",
            "target sync @ 91500 loss 5.758787301601842e-05\n",
            "[BUY] ep=184/200 reward=0.6377 eps=0.050 total_steps=92000 learn_steps=91763 loss=0.0002631264796946198\n",
            "target sync @ 92000 loss 0.00017058203229680657\n",
            "[BUY] ep=185/200 reward=0.9792 eps=0.050 total_steps=92500 learn_steps=92263 loss=0.0002041376574197784\n",
            "target sync @ 92500 loss 1.2914246326545253e-05\n",
            "[BUY] ep=186/200 reward=0.1057 eps=0.050 total_steps=93000 learn_steps=92763 loss=0.0002828642900567502\n",
            "target sync @ 93000 loss 0.00028507362003438175\n",
            "[BUY] ep=187/200 reward=0.6502 eps=0.050 total_steps=93500 learn_steps=93263 loss=0.0002118399424944073\n",
            "target sync @ 93500 loss 0.00015465583419427276\n",
            "[BUY] ep=188/200 reward=0.5760 eps=0.050 total_steps=94000 learn_steps=93763 loss=0.00023081141989678144\n",
            "target sync @ 94000 loss 0.0003986566443927586\n",
            "[BUY] ep=189/200 reward=0.8072 eps=0.050 total_steps=94500 learn_steps=94263 loss=3.429713979130611e-05\n",
            "target sync @ 94500 loss 4.2760690121212974e-05\n",
            "[BUY] ep=190/200 reward=0.8375 eps=0.050 total_steps=95000 learn_steps=94763 loss=1.5048703062348068e-05\n",
            "target sync @ 95000 loss 1.557538598717656e-05\n",
            "[BUY] ep=191/200 reward=0.9559 eps=0.050 total_steps=95500 learn_steps=95263 loss=0.0003925743803847581\n",
            "target sync @ 95500 loss 1.3737897461396642e-05\n",
            "[BUY] ep=192/200 reward=0.9021 eps=0.050 total_steps=96000 learn_steps=95763 loss=0.00012327688455116004\n",
            "target sync @ 96000 loss 2.2091327991802245e-05\n",
            "[BUY] ep=193/200 reward=0.6895 eps=0.050 total_steps=96500 learn_steps=96263 loss=0.000593529490288347\n",
            "target sync @ 96500 loss 3.1858773581916466e-05\n",
            "[BUY] ep=194/200 reward=0.5903 eps=0.050 total_steps=97000 learn_steps=96763 loss=0.00010331426892662421\n",
            "target sync @ 97000 loss 1.7134610970970243e-05\n",
            "[BUY] ep=195/200 reward=1.0303 eps=0.050 total_steps=97500 learn_steps=97263 loss=0.0001565135025884956\n",
            "target sync @ 97500 loss 1.6844813217176124e-05\n",
            "[BUY] ep=196/200 reward=1.2469 eps=0.050 total_steps=98000 learn_steps=97763 loss=0.000160835130373016\n",
            "target sync @ 98000 loss 0.00024030025815591216\n",
            "[BUY] ep=197/200 reward=1.1049 eps=0.050 total_steps=98500 learn_steps=98263 loss=0.00018587322847452015\n",
            "target sync @ 98500 loss 4.4424861698644236e-05\n",
            "[BUY] ep=198/200 reward=0.6688 eps=0.050 total_steps=99000 learn_steps=98763 loss=0.0005558842676691711\n",
            "target sync @ 99000 loss 0.0007989247096702456\n",
            "[BUY] ep=199/200 reward=0.7130 eps=0.050 total_steps=99500 learn_steps=99263 loss=0.0004344761255197227\n",
            "target sync @ 99500 loss 0.00012370570038910955\n",
            "[BUY] ep=200/200 reward=0.6588 eps=0.050 total_steps=100000 learn_steps=99763 loss=2.5064218789339066e-05\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for ep in range(int(cfg.training.episodes)):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    ep_reward = 0.0\n",
        "    steps = 0\n",
        "    loss = None\n",
        "\n",
        "    while not done:\n",
        "        a = agent.select_action(s, greedy=False)\n",
        "        ns, r, done, info = env.step(a)\n",
        "\n",
        "        agent.push(s, a, r, ns, done)\n",
        "\n",
        "        # âœ… warmup based on env steps (or buffer size), then train\n",
        "        if agent.total_steps >= int(cfg.training.warmup_steps):\n",
        "            loss = agent.update()\n",
        "\n",
        "        s = ns\n",
        "        ep_reward += float(r)\n",
        "        steps += 1\n",
        "\n",
        "        if cfg.training.steps_per_episode is not None and steps >= int(cfg.training.steps_per_episode):\n",
        "            break\n",
        "\n",
        "    if (ep + 1) % int(cfg.training.log_every) == 0:\n",
        "        last_loss = agent.loss_history[-1] if agent.loss_history else None\n",
        "        print(\n",
        "            f\"[BUY] ep={ep+1}/{cfg.training.episodes} \"\n",
        "            f\"reward={ep_reward:.4f} eps={agent.eps:.3f} \"\n",
        "            f\"total_steps={agent.total_steps} learn_steps={agent.learn_steps} \"\n",
        "            f\"loss={last_loss}\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model: runs/20260120-153602/buy_agent.pt\n",
            "Diagnostics: {'line': 'runs/20260120-153602/q_gap_buy.png', 'hist': 'runs/20260120-153602/q_gap_buy_hist.png'}\n"
          ]
        }
      ],
      "source": [
        "# Save model + diagnostics\n",
        "model_path = os.path.join(run_path, \"buy_agent.pt\")\n",
        "agent.save(model_path)\n",
        "\n",
        "gaps = compute_q_gap(agent, features, max_points=2000)\n",
        "paths = plot_q_gap(gaps, run_path, tag=\"buy\")\n",
        "\n",
        "print(\"Saved model:\", model_path)\n",
        "print(\"Diagnostics:\", paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "87c7a72d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mean_delta(trades, prices, horizon, tc):\n",
        "    deltas = []\n",
        "\n",
        "    for tr in trades:\n",
        "        if tr[\"meta\"].get(\"reason\") != \"sell_agent\":\n",
        "            continue\n",
        "\n",
        "        entry = tr[\"entry_idx\"]\n",
        "        # exit_sell = tr[\"exit_idx\"]\n",
        "\n",
        "        horizon_exit = min(entry + horizon, len(prices) - 1)\n",
        "\n",
        "        entry_price = prices[entry]\n",
        "        horizon_price = prices[horizon_exit]\n",
        "\n",
        "        gross_horizon = (horizon_price - entry_price) / (entry_price + 1e-12)\n",
        "        net_horizon = ((1 - tc) ** 2) * (1 + gross_horizon) - 1\n",
        "\n",
        "        delta = tr[\"net_return\"] - net_horizon\n",
        "        deltas.append(delta)\n",
        "\n",
        "    return {\n",
        "        \"count\": len(deltas),\n",
        "        \"mean_delta\": float(np.mean(deltas)) if deltas else 0.0,\n",
        "        \"median_delta\": float(np.median(deltas)) if deltas else 0.0,\n",
        "        \"win_rate\": float(np.mean(np.array(deltas) > 0)) if deltas else 0.0,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f948f3",
      "metadata": {},
      "source": [
        "# TradeManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "63d0a92c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA SPLIT ===\n",
            "features: (6195, 12) prices: (6195,)\n",
            "SEG_LEN: 1239 N_SEGS: 5 TRAIN_FRAC: 0.7\n",
            "train_len per seg: 867 test_len per seg: 372\n",
            "X_train: (4335, 12) p_train: (4335,)\n",
            "X_test : (1860, 12) p_test : (1860,)\n",
            "Exit reasons: {'time': 141}\n",
            "Non-time exits: []\n",
            "ENTRY DEBUG: {'checked': 951, 'blocked_trend': 784, 'blocked_latest_entry': 17, 'blocked_conf': 0, 'opened': 141, 'conf_min': 0.44328194593797604, 'conf_max': 0.5630047978079805, 'blocked_sentiment': 9, 'blocked_sentiment_samples': [{'t': 1536, 'sent': -0.08276594430208206, 'mass': 0.9664360284805298, 'conf': 0.48394337502515344}, {'t': 1661, 'sent': -0.05763888359069824, 'mass': 1.602344036102295, 'conf': 0.48802138636402786}, {'t': 2008, 'sent': 0.02265726961195469, 'mass': 1.600074052810669, 'conf': 0.5337468709507893}, {'t': 2227, 'sent': -0.12670600414276123, 'mass': 0.5854560136795044, 'conf': 0.48546075294912333}, {'t': 3369, 'sent': 0.04114900156855583, 'mass': 0.7033069729804993, 'conf': 0.4853470423161222}]}\n",
            "SELL DEBUG: {'seen': 0, 'sell_actions': 0}\n",
            "EXIT REASONS: {'time': 141}\n",
            "\n",
            "=== TRADE MANAGER (TRAIN) ===\n",
            "n_steps: 4335\n",
            "segment_len: 867\n",
            "n_trades: 141\n",
            "final_equity: 17.9118843975115\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.02484390925389309\n",
            "win rate: 0.6099290780141844\n",
            "min/median/max net: -0.28274541852563084 0.029180738760563596 0.3930759906905483\n",
            "median hold bars: 20.0\n",
            "top 5 net: [0.21719871 0.21813771 0.22808475 0.29425715 0.39307599]\n",
            "bottom 5 net: [-0.28274542 -0.17966912 -0.16360321 -0.1473617  -0.14640543]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 49, 'entry_price': 43.37914276123047, 'exit_price': 47.893470764160156, 'gross_return': 0.10406678683757217, 'net_return': 0.10185975733068386, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4853470423161222, 'reason': 'time'}}\n",
            "{'entry_idx': 54, 'exit_idx': 74, 'entry_price': 48.492679595947266, 'exit_price': 45.55203628540039, 'gross_return': -0.060640973752099606, 'net_return': -0.06251875244556915, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5274239285153728, 'reason': 'time'}}\n",
            "{'entry_idx': 95, 'exit_idx': 115, 'entry_price': 46.465087890625, 'exit_price': 49.00773620605469, 'gross_return': 0.05472169387508354, 'net_return': 0.052613305209027406, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4878637120666459, 'reason': 'time'}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 4206, 'exit_idx': 4226, 'entry_price': 143.90419006347656, 'exit_price': 136.61180114746094, 'gross_return': -0.050675306346527366, 'net_return': -0.052573006409140643, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4854948106007548, 'reason': 'time'}}\n",
            "{'entry_idx': 4232, 'exit_idx': 4252, 'entry_price': 140.53018188476562, 'exit_price': 131.43643188476562, 'gross_return': -0.06471029837175324, 'net_return': -0.06657994248530807, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.49170733296234753, 'reason': 'time'}}\n",
            "{'entry_idx': 4261, 'exit_idx': 4281, 'entry_price': 137.23855590820312, 'exit_price': 123.87850952148438, 'gross_return': -0.09734907437859514, 'net_return': -0.09915347357891235, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.49034636493254063, 'reason': 'time'}}\n",
            "Exit reasons: {'time': 58}\n",
            "Non-time exits: []\n",
            "ENTRY DEBUG: {'checked': 468, 'blocked_trend': 377, 'blocked_latest_entry': 25, 'blocked_conf': 2, 'opened': 58, 'conf_min': 0.3882185785515504, 'conf_max': 0.5665328630788287, 'blocked_sentiment': 6, 'blocked_sentiment_samples': [{'t': 344, 'sent': -0.017996761947870255, 'mass': 1.6865910291671753, 'conf': 0.4868276282150827}, {'t': 510, 'sent': -0.10528286546468735, 'mass': 1.8700300455093384, 'conf': 0.4853470423161222}, {'t': 847, 'sent': -0.11386299878358841, 'mass': 0.6134979724884033, 'conf': 0.4853470423161222}, {'t': 1687, 'sent': -0.3070429861545563, 'mass': 0.6459760069847107, 'conf': 0.4862083768120994}, {'t': 1713, 'sent': 0.03299099951982498, 'mass': 0.5878499746322632, 'conf': 0.48894514027410885}]}\n",
            "SELL DEBUG: {'seen': 0, 'sell_actions': 0}\n",
            "EXIT REASONS: {'time': 58}\n",
            "\n",
            "=== TRADE MANAGER (TEST) ===\n",
            "n_steps: 1860\n",
            "segment_len: 372\n",
            "n_trades: 58\n",
            "final_equity: 1.6551479113351635\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.014071433090416273\n",
            "win rate: 0.5862068965517241\n",
            "min/median/max net: -0.22780488291434964 0.012225962590410977 0.32334161900060887\n",
            "median hold bars: 20.0\n",
            "top 5 net: [0.16342037 0.18209916 0.24502303 0.30975334 0.32334162]\n",
            "bottom 5 net: [-0.22780488 -0.14124062 -0.13894733 -0.12693241 -0.12592149]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 49, 'entry_price': 168.69100952148438, 'exit_price': 151.9320831298828, 'gross_return': -0.09934688540391394, 'net_return': -0.10114729097999142, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.49320325400894033, 'reason': 'time'}}\n",
            "{'entry_idx': 78, 'exit_idx': 98, 'entry_price': 153.17132568359375, 'exit_price': 142.0766143798828, 'gross_return': -0.072433343866392, 'net_return': -0.07428754961200301, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4853470423161222, 'reason': 'time'}}\n",
            "{'entry_idx': 104, 'exit_idx': 124, 'entry_price': 140.78607177734375, 'exit_price': 123.16195678710938, 'gross_return': -0.12518365465944084, 'net_return': -0.12693241253377652, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4770106903977023, 'reason': 'time'}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 1773, 'exit_idx': 1793, 'entry_price': 130.02755737304688, 'exit_price': 127.6058578491211, 'gross_return': -0.01862450985661407, 'net_return': -0.02058627946141056, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5112639305770627, 'reason': 'time'}}\n",
            "{'entry_idx': 1804, 'exit_idx': 1824, 'entry_price': 139.4960174560547, 'exit_price': 130.851318359375, 'gross_return': -0.0619709383416839, 'net_return': -0.06384605843593882, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4951475892049439, 'reason': 'time'}}\n",
            "{'entry_idx': 1832, 'exit_idx': 1852, 'entry_price': 135.22824096679688, 'exit_price': 135.62522888183594, 'gross_return': 0.002935687931757787, 'net_return': 0.0009308194915820245, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.48688253826930367, 'reason': 'time'}}\n",
            "\n",
            "=== ENTRY HARVEST (TRAIN) ===\n",
            "topk_per_segment: 80 min_gap: None use_conf_score: False\n",
            "n_entries: 255\n",
            "horizon: 20 segment_len: 867 feasible_in_segment: True\n",
            "first 10: [29, 39, 49, 61, 71, 97, 107, 121, 132, 142]\n",
            "last 10 : [4101, 4112, 4123, 4133, 4157, 4176, 4207, 4232, 4261, 4271]\n",
            "\n",
            "=== ENTRY HARVEST (TEST) ===\n",
            "topk_per_segment: 40 min_gap: None use_conf_score: False\n",
            "n_entries: 104\n",
            "horizon: 20 segment_len: 372 feasible_in_segment: True\n",
            "first 10: [29, 78, 96, 135, 154, 171, 185, 195, 207, 217]\n",
            "last 10 : [1706, 1716, 1728, 1757, 1770, 1781, 1792, 1804, 1815, 1838]\n",
            "\n",
            "=== SAVED ===\n",
            " - runs/entry_indices_train.npy (HARVESTED)\n",
            " - runs/entry_indices_test.npy (HARVESTED)\n",
            " - runs/trades_buy_only_train.json\n",
            " - runs/trades_buy_only_test.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from trade.trade_manager import TradeManager\n",
        "\n",
        "# -----------------------\n",
        "# SETTINGS\n",
        "# -----------------------\n",
        "SEG_LEN = 1239          # rows per ticker segment from your build_features validation\n",
        "N_SEGS  = 5             # AAPL, MSFT, NVDA, AMZN, GOOGL\n",
        "TRAIN_FRAC = 0.70       # time-based split within each segment\n",
        "\n",
        "# NEW: entry harvesting (for SellAgent training)\n",
        "TOPK_PER_SEG_TRAIN = 80     # try 50â€“150\n",
        "TOPK_PER_SEG_TEST  = 40     # fewer is fine for eval\n",
        "MIN_GAP_TRAIN = None        # None => defaults inside TradeManager\n",
        "MIN_GAP_TEST  = None\n",
        "USE_CONF_SCORE = False      # False => uses q1-q0 margin (recommended)\n",
        "\n",
        "# -----------------------\n",
        "# BUILD TRAIN/TEST INDEX (per segment, no leakage)\n",
        "# -----------------------\n",
        "train_len = int(SEG_LEN * TRAIN_FRAC)\n",
        "\n",
        "train_idx = []\n",
        "test_idx = []\n",
        "\n",
        "for seg in range(N_SEGS):\n",
        "    start = seg * SEG_LEN\n",
        "    train_idx.extend(range(start, start + train_len))\n",
        "    test_idx.extend(range(start + train_len, start + SEG_LEN))\n",
        "\n",
        "train_idx = np.array(train_idx, dtype=np.int32)\n",
        "test_idx  = np.array(test_idx, dtype=np.int32)\n",
        "\n",
        "X_train = features[train_idx]\n",
        "p_train = prices[train_idx]\n",
        "X_test  = features[test_idx]\n",
        "p_test  = prices[test_idx]\n",
        "\n",
        "# Segment length inside each split subset (since we concatenated segments in order)\n",
        "SEG_TRAIN = train_len\n",
        "SEG_TEST  = SEG_LEN - train_len\n",
        "\n",
        "print(\"=== DATA SPLIT ===\")\n",
        "print(\"features:\", features.shape, \"prices:\", prices.shape)\n",
        "print(\"SEG_LEN:\", SEG_LEN, \"N_SEGS:\", N_SEGS, \"TRAIN_FRAC:\", TRAIN_FRAC)\n",
        "print(\"train_len per seg:\", SEG_TRAIN, \"test_len per seg:\", SEG_TEST)\n",
        "print(\"X_train:\", X_train.shape, \"p_train:\", p_train.shape)\n",
        "print(\"X_test :\", X_test.shape,  \"p_test :\", p_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# HELPER: run TM + debug logs (unchanged backtest)\n",
        "# -----------------------\n",
        "def run_tm(name: str, X: np.ndarray, p: np.ndarray, seg_len: int, sell_agent=None):\n",
        "    tm = TradeManager(\n",
        "        buy_agent=agent,            # trained buy agent\n",
        "        sell_agent=None,      # optional\n",
        "        state=X,\n",
        "        prices=p,\n",
        "        reward=cfg.reward,\n",
        "        trade=cfg.trade_manager,\n",
        "        segment_len=seg_len,        # IMPORTANT for boundary correctness\n",
        "    )\n",
        "\n",
        "    res = tm.run()\n",
        "    trades = res[\"trades\"]\n",
        "\n",
        "    from collections import Counter\n",
        "    reasons = Counter([t[\"meta\"].get(\"reason\", \"none\") for t in res[\"trades\"]])\n",
        "    print(\"Exit reasons:\", dict(reasons))\n",
        "    print(\"Non-time exits:\", [t for t in res[\"trades\"] if t[\"meta\"].get(\"reason\") != \"time\"][:3])\n",
        "\n",
        "    print(\"ENTRY DEBUG:\", res[\"entry_debug\"])\n",
        "    print(\"SELL DEBUG:\", res[\"sell_debug\"])\n",
        "    print(\"EXIT REASONS:\", res.get(\"exit_reasons\"))\n",
        "\n",
        "\n",
        "    print(f\"\\n=== TRADE MANAGER ({name}) ===\")\n",
        "    print(\"n_steps:\", len(p))\n",
        "    print(\"segment_len:\", seg_len)\n",
        "    print(\"n_trades:\", res[\"n_trades\"])\n",
        "    print(\"final_equity:\", res[\"final_equity\"])\n",
        "\n",
        "    # Boundary-crossing check (must be 0)\n",
        "    if trades:\n",
        "        cross = sum((t[\"entry_idx\"] // seg_len) != (t[\"exit_idx\"] // seg_len) for t in trades)\n",
        "    else:\n",
        "        cross = 0\n",
        "    print(\"Trades crossing segment boundary:\", cross)\n",
        "\n",
        "    # Return stats\n",
        "    if trades:\n",
        "        net = np.array([t[\"net_return\"] for t in trades], dtype=float)\n",
        "        hold = np.array([t[\"hold_bars\"] for t in trades], dtype=float)\n",
        "\n",
        "        print(\"avg net return:\", float(net.mean()))\n",
        "        print(\"win rate:\", float((net > 0).mean()))\n",
        "        print(\"min/median/max net:\", float(net.min()), float(np.median(net)), float(net.max()))\n",
        "        print(\"median hold bars:\", float(np.median(hold)))\n",
        "        print(\"top 5 net:\", np.sort(net)[-5:])\n",
        "        print(\"bottom 5 net:\", np.sort(net)[:5])\n",
        "\n",
        "        # A few sample trades (head + tail)\n",
        "        print(\"\\nSample trades (first 3):\")\n",
        "        for t in trades[:3]:\n",
        "            print(t)\n",
        "        print(\"\\nSample trades (last 3):\")\n",
        "        for t in trades[-3:]:\n",
        "            print(t)\n",
        "    else:\n",
        "        print(\"No trades produced. Try lowering buy_min_confidence or disabling trend filter.\")\n",
        "\n",
        "    return tm, res\n",
        "\n",
        "# -----------------------\n",
        "# NEW: Harvest entry indices for SellAgent training (no trade execution)\n",
        "# -----------------------\n",
        "def harvest_entries(name: str, tm: TradeManager, topk_per_seg: int, min_gap=None, use_confidence_score=False):\n",
        "    entries = tm.collect_entry_indices_topk(\n",
        "        topk_per_segment=topk_per_seg,\n",
        "        min_gap=min_gap,\n",
        "        use_confidence_score=use_confidence_score,\n",
        "    )\n",
        "    entries = np.array(entries, dtype=np.int32)\n",
        "\n",
        "    # Quick sanity: segment boundary + horizon feasibility check (should hold by construction)\n",
        "    horizon = int(cfg.trade_manager.sell_horizon)\n",
        "    if len(entries) > 0:\n",
        "        seg_ok = np.all((entries % tm.segment_len) <= (tm.segment_len - 1 - horizon))\n",
        "    else:\n",
        "        seg_ok = True\n",
        "\n",
        "    print(f\"\\n=== ENTRY HARVEST ({name}) ===\")\n",
        "    print(\"topk_per_segment:\", topk_per_seg, \"min_gap:\", min_gap, \"use_conf_score:\", use_confidence_score)\n",
        "    print(\"n_entries:\", len(entries))\n",
        "    print(\"horizon:\", horizon, \"segment_len:\", tm.segment_len, \"feasible_in_segment:\", bool(seg_ok))\n",
        "    if len(entries) > 0:\n",
        "        print(\"first 10:\", entries[:10].tolist())\n",
        "        print(\"last 10 :\", entries[-10:].tolist())\n",
        "\n",
        "    return entries\n",
        "\n",
        "# -----------------------\n",
        "# RUN TRAIN + TEST (backtest as before)\n",
        "# -----------------------\n",
        "tm_train, res_train = run_tm(\"TRAIN\", X_train, p_train, seg_len=SEG_TRAIN, sell_agent=None)\n",
        "tm_test,  res_test  = run_tm(\"TEST\",  X_test,  p_test,  seg_len=SEG_TEST,  sell_agent=None)\n",
        "\n",
        "# -----------------------\n",
        "# HARVEST ENTRIES (NEW LOGIC) â€” use these for SellEnv training\n",
        "# -----------------------\n",
        "train_entries = harvest_entries(\n",
        "    \"TRAIN\",\n",
        "    tm_train,\n",
        "    topk_per_seg=TOPK_PER_SEG_TRAIN,\n",
        "    min_gap=MIN_GAP_TRAIN,\n",
        "    use_confidence_score=USE_CONF_SCORE,\n",
        ")\n",
        "\n",
        "test_entries = harvest_entries(\n",
        "    \"TEST\",\n",
        "    tm_test,\n",
        "    topk_per_seg=TOPK_PER_SEG_TEST,\n",
        "    min_gap=MIN_GAP_TEST,\n",
        "    use_confidence_score=USE_CONF_SCORE,\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# SAVE ARTIFACTS (into out_dir)\n",
        "# -----------------------\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "train_entries_path = os.path.join(out_dir, \"entry_indices_train.npy\")\n",
        "test_entries_path  = os.path.join(out_dir, \"entry_indices_test.npy\")\n",
        "\n",
        "# NEW: save harvested entries (not trade entries)\n",
        "np.save(train_entries_path, train_entries)\n",
        "np.save(test_entries_path,  test_entries)\n",
        "\n",
        "train_trades_json = os.path.join(out_dir, \"trades_buy_only_train.json\")\n",
        "test_trades_json  = os.path.join(out_dir, \"trades_buy_only_test.json\")\n",
        "\n",
        "with open(train_trades_json, \"w\") as f:\n",
        "    json.dump(res_train[\"trades\"], f, indent=2)\n",
        "\n",
        "with open(test_trades_json, \"w\") as f:\n",
        "    json.dump(res_test[\"trades\"], f, indent=2)\n",
        "\n",
        "print(\"\\n=== SAVED ===\")\n",
        "print(\" -\", train_entries_path, \"(HARVESTED)\")\n",
        "print(\" -\", test_entries_path,  \"(HARVESTED)\")\n",
        "print(\" -\", train_trades_json)\n",
        "print(\" -\", test_trades_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d5aa5cee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST final_equity: 1.6551479113351635\n",
            "TEST n_trades: 58\n",
            "TEST avg net: 0.014071433090416273\n",
            "TEST win rate: 0.5862068965517241\n",
            "TEST min/median/max: (np.float64(-0.22780488291434964), np.float64(0.012225962590410977), np.float64(0.32334161900060887))\n"
          ]
        }
      ],
      "source": [
        "print(\"TEST final_equity:\", res_test[\"final_equity\"])\n",
        "print(\"TEST n_trades:\", res_test[\"n_trades\"])\n",
        "\n",
        "trades = res_test[\"trades\"]\n",
        "net = np.array([t[\"net_return\"] for t in trades], dtype=float) if trades else np.array([])\n",
        "print(\"TEST avg net:\", net.mean() if len(net) else None)\n",
        "print(\"TEST win rate:\", (net > 0).mean() if len(net) else None)\n",
        "print(\"TEST min/median/max:\", (net.min(), np.median(net), net.max()) if len(net) else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2a420c7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def filter_entries(entries, seg_len, horizon, n):\n",
        "#     kept = []\n",
        "#     drop_oob = 0\n",
        "#     drop_seg = 0\n",
        "#     drop_horizon = 0\n",
        "\n",
        "#     for e in entries:\n",
        "#         e = int(e)\n",
        "#         if e < 0 or e >= n:\n",
        "#             drop_oob += 1\n",
        "#             continue\n",
        "\n",
        "#         # segment end (inclusive)\n",
        "#         seg_end = min(((e // seg_len) + 1) * seg_len - 1, n - 1)\n",
        "\n",
        "#         # need room for at least horizon bars INSIDE segment\n",
        "#         last_allowed = min(e + horizon, seg_end, n - 1)\n",
        "\n",
        "#         # if episode would immediately be at/over last_allowed, it's useless\n",
        "#         # (or you can make this stricter: require at least 1 step)\n",
        "#         if last_allowed <= e:\n",
        "#             drop_horizon += 1\n",
        "#             continue\n",
        "\n",
        "#         kept.append(e)\n",
        "\n",
        "#     kept = np.array(sorted(set(kept)), dtype=np.int32)\n",
        "\n",
        "#     print(\"entries raw:\", len(entries))\n",
        "#     print(\"entries kept:\", len(kept))\n",
        "#     print(\"dropped oob:\", drop_oob)\n",
        "#     print(\"dropped horizon/seg:\", drop_horizon)\n",
        "#     return kept\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3058fa38",
      "metadata": {},
      "source": [
        "# Sell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ec8c30cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "entries_train: (255,) entries_test: (104,)\n",
            "sell include_pos: True\n",
            "sell feat_dim: 12 state_dim: 15\n",
            "Sell Decay Steps: 60000 / Est Total: 75000\n",
            "SELL state_dim: 15 n_actions: 2\n",
            "[SELL] ep=10/4000 reward=0.0292 eps=0.998 loss=None\n",
            "[SELL] ep=20/4000 reward=0.0491 eps=0.996 loss=0.007139499299228191\n",
            "[SELL] ep=30/4000 reward=0.0473 eps=0.994 loss=0.0029287170618772507\n",
            "[SELL] ep=40/4000 reward=-0.0235 eps=0.992 loss=0.0038308484945446253\n",
            "[SELL] ep=50/4000 reward=0.0158 eps=0.990 loss=0.003946135751903057\n",
            "[SELL] ep=60/4000 reward=0.0917 eps=0.988 loss=0.00416955491527915\n",
            "target sync @ 500 loss 0.003707529976963997\n",
            "[SELL] ep=70/4000 reward=-0.0076 eps=0.987 loss=0.0011420457158237696\n",
            "[SELL] ep=80/4000 reward=-0.0094 eps=0.985 loss=0.0009302844991907477\n",
            "[SELL] ep=90/4000 reward=-0.0127 eps=0.983 loss=0.0007311805966310203\n",
            "[SELL] ep=100/4000 reward=-0.0002 eps=0.981 loss=0.000597604492213577\n",
            "target sync @ 1000 loss 0.0005814125179313123\n",
            "[SELL] ep=110/4000 reward=-0.0616 eps=0.979 loss=0.0006146843079477549\n",
            "[SELL] ep=120/4000 reward=-0.0418 eps=0.977 loss=0.0005744999507442117\n",
            "[SELL] ep=130/4000 reward=-0.0235 eps=0.975 loss=0.00044021717621944845\n",
            "[SELL] ep=140/4000 reward=-0.0456 eps=0.973 loss=0.001069515710696578\n",
            "target sync @ 1500 loss 0.0005318584735505283\n",
            "[SELL] ep=150/4000 reward=-0.0783 eps=0.971 loss=0.0007612424669787288\n",
            "[SELL] ep=160/4000 reward=-0.0276 eps=0.969 loss=0.0005044412100687623\n",
            "[SELL] ep=170/4000 reward=-0.0072 eps=0.967 loss=0.00043513462878763676\n",
            "[SELL] ep=180/4000 reward=-0.0362 eps=0.966 loss=0.0008662540931254625\n",
            "target sync @ 2000 loss 0.0002736265305429697\n",
            "[SELL] ep=190/4000 reward=-0.0249 eps=0.964 loss=0.0005431773606687784\n",
            "[SELL] ep=200/4000 reward=-0.0704 eps=0.962 loss=0.0005090865306556225\n",
            "[SELL] ep=210/4000 reward=0.0460 eps=0.960 loss=0.0006216542096808553\n",
            "[SELL] ep=220/4000 reward=0.0824 eps=0.958 loss=0.0003361568960826844\n",
            "target sync @ 2500 loss 0.0003273579350207001\n",
            "[SELL] ep=230/4000 reward=0.0254 eps=0.956 loss=0.0002310348500031978\n",
            "[SELL] ep=240/4000 reward=-0.0123 eps=0.954 loss=0.0002917142992373556\n",
            "[SELL] ep=250/4000 reward=-0.0309 eps=0.952 loss=0.00029229989740997553\n",
            "[SELL] ep=260/4000 reward=0.0603 eps=0.950 loss=0.0003591646673157811\n",
            "target sync @ 3000 loss 0.0005734662991017103\n",
            "[SELL] ep=270/4000 reward=-0.0837 eps=0.948 loss=0.00028897158335894346\n",
            "[SELL] ep=280/4000 reward=-0.0369 eps=0.947 loss=0.000526215648278594\n",
            "[SELL] ep=290/4000 reward=-0.0461 eps=0.945 loss=0.0002326701651327312\n",
            "[SELL] ep=300/4000 reward=-0.0284 eps=0.943 loss=0.0002999434946104884\n",
            "[SELL] ep=310/4000 reward=0.1151 eps=0.941 loss=0.00035643050796352327\n",
            "target sync @ 3500 loss 0.0008752773283049464\n",
            "[SELL] ep=320/4000 reward=0.0701 eps=0.939 loss=0.00033325402182526886\n",
            "[SELL] ep=330/4000 reward=0.0350 eps=0.937 loss=0.00027763633988797665\n",
            "[SELL] ep=340/4000 reward=-0.0266 eps=0.935 loss=0.00024291352019645274\n",
            "[SELL] ep=350/4000 reward=-0.0707 eps=0.933 loss=0.0002702188794501126\n",
            "target sync @ 4000 loss 0.00040445371996611357\n",
            "[SELL] ep=360/4000 reward=0.0290 eps=0.931 loss=0.0004699314013123512\n",
            "[SELL] ep=370/4000 reward=0.0751 eps=0.929 loss=0.00037436842103488743\n",
            "[SELL] ep=380/4000 reward=-0.0366 eps=0.927 loss=0.0005157650448381901\n",
            "[SELL] ep=390/4000 reward=0.0356 eps=0.925 loss=0.0006734189810231328\n",
            "target sync @ 4500 loss 0.0004782888572663069\n",
            "[SELL] ep=400/4000 reward=-0.0575 eps=0.923 loss=0.0006987389642745256\n",
            "[SELL] ep=410/4000 reward=0.0580 eps=0.921 loss=0.00034884968772530556\n",
            "[SELL] ep=420/4000 reward=-0.0238 eps=0.919 loss=0.0004964369582012296\n",
            "[SELL] ep=430/4000 reward=-0.0427 eps=0.918 loss=0.0002339488419238478\n",
            "target sync @ 5000 loss 0.0004637411329895258\n",
            "[SELL] ep=440/4000 reward=0.0145 eps=0.916 loss=0.00026090111350640655\n",
            "[SELL] ep=450/4000 reward=-0.0880 eps=0.914 loss=0.0007931990548968315\n",
            "[SELL] ep=460/4000 reward=0.1373 eps=0.912 loss=0.0003970471443608403\n",
            "[SELL] ep=470/4000 reward=0.0335 eps=0.910 loss=0.00043998786713927984\n",
            "target sync @ 5500 loss 0.0003377606044523418\n",
            "[SELL] ep=480/4000 reward=0.1017 eps=0.908 loss=0.00025761465076357126\n",
            "[SELL] ep=490/4000 reward=-0.1168 eps=0.906 loss=0.0004205973236821592\n",
            "[SELL] ep=500/4000 reward=0.0293 eps=0.904 loss=0.0003314297064207494\n",
            "[SELL] ep=510/4000 reward=-0.0334 eps=0.902 loss=0.0005576980765908957\n",
            "target sync @ 6000 loss 0.0006391839124262333\n",
            "[SELL] ep=520/4000 reward=-0.0958 eps=0.900 loss=0.0004396769218146801\n",
            "[SELL] ep=530/4000 reward=0.0293 eps=0.898 loss=0.0004901279462501407\n",
            "[SELL] ep=540/4000 reward=-0.0063 eps=0.896 loss=0.0005287022795528173\n",
            "[SELL] ep=550/4000 reward=-0.0750 eps=0.894 loss=0.0005900892429053783\n",
            "target sync @ 6500 loss 0.0003506746143102646\n",
            "[SELL] ep=560/4000 reward=-0.0226 eps=0.892 loss=0.0005227260990068316\n",
            "[SELL] ep=570/4000 reward=0.0104 eps=0.891 loss=0.0007288378546945751\n",
            "[SELL] ep=580/4000 reward=-0.1870 eps=0.889 loss=0.0001991370809264481\n",
            "[SELL] ep=590/4000 reward=-0.0037 eps=0.887 loss=0.00041553840856067836\n",
            "target sync @ 7000 loss 0.0005496281664818525\n",
            "[SELL] ep=600/4000 reward=0.0437 eps=0.885 loss=0.000353469600668177\n",
            "[SELL] ep=610/4000 reward=-0.0350 eps=0.883 loss=0.0004270139615982771\n",
            "[SELL] ep=620/4000 reward=0.0439 eps=0.881 loss=0.0004143211408518255\n",
            "[SELL] ep=630/4000 reward=-0.0000 eps=0.879 loss=0.0005891803884878755\n",
            "target sync @ 7500 loss 0.0007838761666789651\n",
            "[SELL] ep=640/4000 reward=-0.0249 eps=0.877 loss=0.00045915739610791206\n",
            "[SELL] ep=650/4000 reward=-0.0194 eps=0.875 loss=0.00026247670757584274\n",
            "[SELL] ep=660/4000 reward=-0.0761 eps=0.873 loss=0.0002532295184209943\n",
            "[SELL] ep=670/4000 reward=-0.0408 eps=0.871 loss=0.0006870219367556274\n",
            "target sync @ 8000 loss 0.0004575407656375319\n",
            "[SELL] ep=680/4000 reward=-0.1010 eps=0.869 loss=0.0014122972497716546\n",
            "[SELL] ep=690/4000 reward=-0.0387 eps=0.867 loss=0.00031121401116251945\n",
            "[SELL] ep=700/4000 reward=0.0322 eps=0.865 loss=0.00037584968958981335\n",
            "[SELL] ep=710/4000 reward=0.0137 eps=0.863 loss=0.0002906329755205661\n",
            "target sync @ 8500 loss 0.00039869360625743866\n",
            "[SELL] ep=720/4000 reward=0.0988 eps=0.861 loss=0.0005659000598825514\n",
            "[SELL] ep=730/4000 reward=0.0025 eps=0.859 loss=0.0005035811918787658\n",
            "[SELL] ep=740/4000 reward=-0.0904 eps=0.857 loss=0.0004391625407151878\n",
            "[SELL] ep=750/4000 reward=0.0028 eps=0.855 loss=0.0008680358878336847\n",
            "target sync @ 9000 loss 0.00043480092426761985\n",
            "[SELL] ep=760/4000 reward=0.0728 eps=0.853 loss=0.0003289062879048288\n",
            "[SELL] ep=770/4000 reward=-0.0198 eps=0.851 loss=0.00046227729762904346\n",
            "[SELL] ep=780/4000 reward=-0.0194 eps=0.849 loss=0.0006288038566708565\n",
            "[SELL] ep=790/4000 reward=0.0511 eps=0.847 loss=0.0009471987723372877\n",
            "target sync @ 9500 loss 0.000379791425075382\n",
            "[SELL] ep=800/4000 reward=0.1159 eps=0.845 loss=0.000706758233718574\n",
            "[SELL] ep=810/4000 reward=0.0607 eps=0.843 loss=0.00030531801166944206\n",
            "[SELL] ep=820/4000 reward=0.0607 eps=0.841 loss=0.0003695909399539232\n",
            "[SELL] ep=830/4000 reward=-0.0689 eps=0.839 loss=0.0005710178520530462\n",
            "target sync @ 10000 loss 0.00029425317188724875\n",
            "[SELL] ep=840/4000 reward=-0.0033 eps=0.837 loss=0.0004709250060841441\n",
            "[SELL] ep=850/4000 reward=-0.1919 eps=0.835 loss=0.0015891615767031908\n",
            "[SELL] ep=860/4000 reward=-0.0316 eps=0.833 loss=0.0013506175018846989\n",
            "[SELL] ep=870/4000 reward=-0.0020 eps=0.832 loss=0.0006073822733014822\n",
            "target sync @ 10500 loss 0.000830079719889909\n",
            "[SELL] ep=880/4000 reward=-0.0307 eps=0.830 loss=0.0008203766774386168\n",
            "[SELL] ep=890/4000 reward=-0.0293 eps=0.828 loss=0.00040770144551061094\n",
            "[SELL] ep=900/4000 reward=0.0669 eps=0.826 loss=0.00035946164280176163\n",
            "[SELL] ep=910/4000 reward=-0.0618 eps=0.824 loss=0.0004243702278472483\n",
            "target sync @ 11000 loss 0.0002840502420440316\n",
            "[SELL] ep=920/4000 reward=-0.0553 eps=0.822 loss=0.0006080862367525697\n",
            "[SELL] ep=930/4000 reward=0.0427 eps=0.820 loss=0.00044907850679010153\n",
            "[SELL] ep=940/4000 reward=-0.0620 eps=0.818 loss=0.0007425230578519404\n",
            "[SELL] ep=950/4000 reward=-0.0920 eps=0.816 loss=0.0004003332578577101\n",
            "target sync @ 11500 loss 0.0010146129643544555\n",
            "[SELL] ep=960/4000 reward=0.0585 eps=0.814 loss=0.002254804829135537\n",
            "[SELL] ep=970/4000 reward=-0.0167 eps=0.812 loss=0.0012012006482109427\n",
            "[SELL] ep=980/4000 reward=-0.1208 eps=0.810 loss=0.0005071587511338294\n",
            "[SELL] ep=990/4000 reward=0.0101 eps=0.808 loss=0.000820038840174675\n",
            "target sync @ 12000 loss 0.0004080261569470167\n",
            "[SELL] ep=1000/4000 reward=0.0175 eps=0.806 loss=0.0006037767743691802\n",
            "[SELL] ep=1010/4000 reward=-0.1856 eps=0.804 loss=0.0009901319863274693\n",
            "[SELL] ep=1020/4000 reward=-0.0179 eps=0.802 loss=0.0006227291887626052\n",
            "[SELL] ep=1030/4000 reward=-0.0202 eps=0.800 loss=0.00036632304545491934\n",
            "target sync @ 12500 loss 0.000653623603284359\n",
            "[SELL] ep=1040/4000 reward=0.0584 eps=0.798 loss=0.0005076192901469767\n",
            "[SELL] ep=1050/4000 reward=0.0035 eps=0.796 loss=0.000942329119425267\n",
            "[SELL] ep=1060/4000 reward=-0.0218 eps=0.794 loss=0.0004873849102295935\n",
            "[SELL] ep=1070/4000 reward=0.0488 eps=0.792 loss=0.0005449749296531081\n",
            "target sync @ 13000 loss 0.0005093510262668133\n",
            "[SELL] ep=1080/4000 reward=-0.0339 eps=0.790 loss=0.001287615392357111\n",
            "[SELL] ep=1090/4000 reward=0.0710 eps=0.788 loss=0.000760715629439801\n",
            "[SELL] ep=1100/4000 reward=0.1221 eps=0.786 loss=0.0006956744473427534\n",
            "[SELL] ep=1110/4000 reward=-0.0713 eps=0.784 loss=0.0007862488855607808\n",
            "target sync @ 13500 loss 0.0008031934266909957\n",
            "[SELL] ep=1120/4000 reward=-0.0282 eps=0.782 loss=0.0004777543363161385\n",
            "[SELL] ep=1130/4000 reward=0.0248 eps=0.780 loss=0.00043488250230439007\n",
            "[SELL] ep=1140/4000 reward=0.0222 eps=0.778 loss=0.0011337987380102277\n",
            "[SELL] ep=1150/4000 reward=-0.0104 eps=0.776 loss=0.0005275660078041255\n",
            "target sync @ 14000 loss 0.001059763366356492\n",
            "[SELL] ep=1160/4000 reward=0.0482 eps=0.774 loss=0.0009849155321717262\n",
            "[SELL] ep=1170/4000 reward=0.0110 eps=0.772 loss=0.00063839100766927\n",
            "[SELL] ep=1180/4000 reward=-0.0135 eps=0.770 loss=0.0006095231510698795\n",
            "[SELL] ep=1190/4000 reward=-0.0114 eps=0.768 loss=0.0005236840806901455\n",
            "target sync @ 14500 loss 0.0006741179386153817\n",
            "[SELL] ep=1200/4000 reward=-0.0256 eps=0.767 loss=0.0009470515651628375\n",
            "[SELL] ep=1210/4000 reward=-0.0721 eps=0.765 loss=0.0006102667539380491\n",
            "[SELL] ep=1220/4000 reward=-0.0129 eps=0.763 loss=0.0005908656166866422\n",
            "[SELL] ep=1230/4000 reward=0.0562 eps=0.761 loss=0.0003957569715566933\n",
            "target sync @ 15000 loss 0.000952676753513515\n",
            "[SELL] ep=1240/4000 reward=-0.0208 eps=0.759 loss=0.0018359423847869039\n",
            "[SELL] ep=1250/4000 reward=-0.0229 eps=0.756 loss=0.0006462358869612217\n",
            "[SELL] ep=1260/4000 reward=-0.0331 eps=0.754 loss=0.00035525354905985296\n",
            "[SELL] ep=1270/4000 reward=-0.0606 eps=0.752 loss=0.00048745330423116684\n",
            "target sync @ 15500 loss 0.0007676200475543737\n",
            "[SELL] ep=1280/4000 reward=-0.0272 eps=0.751 loss=0.0004131758469156921\n",
            "[SELL] ep=1290/4000 reward=-0.0370 eps=0.749 loss=0.0010986283887177706\n",
            "[SELL] ep=1300/4000 reward=-0.0479 eps=0.747 loss=0.0005300154443830252\n",
            "[SELL] ep=1310/4000 reward=-0.0226 eps=0.745 loss=0.0004827199736610055\n",
            "target sync @ 16000 loss 0.00046815272071398795\n",
            "[SELL] ep=1320/4000 reward=-0.0109 eps=0.743 loss=0.0006814991356804967\n",
            "[SELL] ep=1330/4000 reward=-0.0687 eps=0.741 loss=0.000696372298989445\n",
            "[SELL] ep=1340/4000 reward=0.0097 eps=0.739 loss=0.0007532074814662337\n",
            "[SELL] ep=1350/4000 reward=-0.1022 eps=0.737 loss=0.0005764499073848128\n",
            "[SELL] ep=1360/4000 reward=-0.0173 eps=0.735 loss=0.0004724914615508169\n",
            "target sync @ 16500 loss 0.0004327641800045967\n",
            "[SELL] ep=1370/4000 reward=-0.0235 eps=0.733 loss=0.0008758303592912853\n",
            "[SELL] ep=1380/4000 reward=0.0222 eps=0.731 loss=0.0006488043582066894\n",
            "[SELL] ep=1390/4000 reward=-0.0341 eps=0.729 loss=0.0004582345427479595\n",
            "target sync @ 17000 loss 0.0004607283044606447\n",
            "[SELL] ep=1400/4000 reward=-0.0249 eps=0.727 loss=0.0003660641086753458\n",
            "[SELL] ep=1410/4000 reward=0.0284 eps=0.725 loss=0.0005984589224681258\n",
            "[SELL] ep=1420/4000 reward=-0.1565 eps=0.723 loss=0.0004909588606096804\n",
            "[SELL] ep=1430/4000 reward=0.0101 eps=0.721 loss=0.0008457074873149395\n",
            "target sync @ 17500 loss 0.00044569879537448287\n",
            "[SELL] ep=1440/4000 reward=-0.0827 eps=0.719 loss=0.0004182051634415984\n",
            "[SELL] ep=1450/4000 reward=-0.0868 eps=0.717 loss=0.0004665889427997172\n",
            "[SELL] ep=1460/4000 reward=-0.0610 eps=0.715 loss=0.00038921396480873227\n",
            "[SELL] ep=1470/4000 reward=-0.0376 eps=0.713 loss=0.0006878661806695163\n",
            "target sync @ 18000 loss 0.0007242283318191767\n",
            "[SELL] ep=1480/4000 reward=-0.0349 eps=0.711 loss=0.0003504506603349\n",
            "[SELL] ep=1490/4000 reward=-0.0952 eps=0.709 loss=0.00035449251299723983\n",
            "[SELL] ep=1500/4000 reward=-0.0209 eps=0.707 loss=0.0004092525923624635\n",
            "[SELL] ep=1510/4000 reward=0.0922 eps=0.705 loss=0.00034765928285196424\n",
            "target sync @ 18500 loss 0.0004992950707674026\n",
            "[SELL] ep=1520/4000 reward=0.0014 eps=0.703 loss=0.0003735257196240127\n",
            "[SELL] ep=1530/4000 reward=0.0515 eps=0.701 loss=0.0006236953195184469\n",
            "[SELL] ep=1540/4000 reward=-0.0256 eps=0.699 loss=0.0003531544643919915\n",
            "[SELL] ep=1550/4000 reward=-0.0335 eps=0.697 loss=0.0007757250568829477\n",
            "target sync @ 19000 loss 0.0004570993478409946\n",
            "[SELL] ep=1560/4000 reward=-0.0350 eps=0.695 loss=0.0005122523289173841\n",
            "[SELL] ep=1570/4000 reward=-0.0890 eps=0.693 loss=0.00038929167203605175\n",
            "[SELL] ep=1580/4000 reward=-0.0318 eps=0.691 loss=0.0003416478866711259\n",
            "[SELL] ep=1590/4000 reward=-0.0689 eps=0.689 loss=0.0003265838313382119\n",
            "target sync @ 19500 loss 0.00038904446410015225\n",
            "[SELL] ep=1600/4000 reward=-0.1380 eps=0.687 loss=0.000394258473534137\n",
            "[SELL] ep=1610/4000 reward=-0.0398 eps=0.685 loss=0.0004582296824082732\n",
            "[SELL] ep=1620/4000 reward=0.0018 eps=0.683 loss=0.00038768298691138625\n",
            "[SELL] ep=1630/4000 reward=-0.0398 eps=0.681 loss=0.00038771843537688255\n",
            "target sync @ 20000 loss 0.0002947764005511999\n",
            "[SELL] ep=1640/4000 reward=-0.0266 eps=0.679 loss=0.0005439237575046718\n",
            "[SELL] ep=1650/4000 reward=-0.0291 eps=0.677 loss=0.0008172295638360083\n",
            "[SELL] ep=1660/4000 reward=-0.0298 eps=0.675 loss=0.000597342848777771\n",
            "[SELL] ep=1670/4000 reward=-0.0209 eps=0.672 loss=0.0006672657327726483\n",
            "target sync @ 20500 loss 0.00041745160706341267\n",
            "[SELL] ep=1680/4000 reward=-0.0610 eps=0.671 loss=0.0006054983823560178\n",
            "[SELL] ep=1690/4000 reward=-0.0431 eps=0.669 loss=0.00033492519287392497\n",
            "[SELL] ep=1700/4000 reward=-0.0575 eps=0.667 loss=0.0002983617887366563\n",
            "[SELL] ep=1710/4000 reward=0.0940 eps=0.665 loss=0.0005298409378156066\n",
            "target sync @ 21000 loss 0.00016021929332055151\n",
            "[SELL] ep=1720/4000 reward=-0.0370 eps=0.663 loss=0.00042689056135714054\n",
            "[SELL] ep=1730/4000 reward=-0.1211 eps=0.661 loss=0.0005195439443923533\n",
            "[SELL] ep=1740/4000 reward=0.0924 eps=0.659 loss=0.0004369043745100498\n",
            "[SELL] ep=1750/4000 reward=-0.0781 eps=0.656 loss=0.0004207371093798429\n",
            "target sync @ 21500 loss 0.00044466429972089827\n",
            "[SELL] ep=1760/4000 reward=0.0220 eps=0.654 loss=0.0004391129477880895\n",
            "[SELL] ep=1770/4000 reward=0.0882 eps=0.652 loss=0.000427249469794333\n",
            "[SELL] ep=1780/4000 reward=0.0627 eps=0.650 loss=0.00042916223173961043\n",
            "target sync @ 22000 loss 0.0006788872997276485\n",
            "[SELL] ep=1790/4000 reward=0.0008 eps=0.648 loss=0.0003896397538483143\n",
            "[SELL] ep=1800/4000 reward=0.0356 eps=0.646 loss=0.00033967100898735225\n",
            "[SELL] ep=1810/4000 reward=-0.0082 eps=0.644 loss=0.0002994767564814538\n",
            "[SELL] ep=1820/4000 reward=0.0135 eps=0.642 loss=0.0003261178790125996\n",
            "target sync @ 22500 loss 0.00029576290398836136\n",
            "[SELL] ep=1830/4000 reward=0.0314 eps=0.640 loss=0.00046352611389011145\n",
            "[SELL] ep=1840/4000 reward=0.2134 eps=0.638 loss=0.0006120844045653939\n",
            "[SELL] ep=1850/4000 reward=0.0256 eps=0.636 loss=0.0004104410472791642\n",
            "[SELL] ep=1860/4000 reward=-0.0252 eps=0.634 loss=0.0013206293806433678\n",
            "target sync @ 23000 loss 0.00040574706508778036\n",
            "[SELL] ep=1870/4000 reward=0.0701 eps=0.632 loss=0.00030655559385195374\n",
            "[SELL] ep=1880/4000 reward=-0.1894 eps=0.630 loss=0.000517941196449101\n",
            "[SELL] ep=1890/4000 reward=0.0556 eps=0.628 loss=0.00040119269397109747\n",
            "[SELL] ep=1900/4000 reward=0.0489 eps=0.626 loss=0.0002936720848083496\n",
            "target sync @ 23500 loss 0.00041537958895787597\n",
            "[SELL] ep=1910/4000 reward=-0.0129 eps=0.624 loss=0.0004143714322708547\n",
            "[SELL] ep=1920/4000 reward=-0.0626 eps=0.622 loss=0.00031210097949951887\n",
            "[SELL] ep=1930/4000 reward=-0.0247 eps=0.620 loss=0.00037460625753737986\n",
            "[SELL] ep=1940/4000 reward=0.0514 eps=0.617 loss=0.00031219550874084234\n",
            "target sync @ 24000 loss 0.00030992066604085267\n",
            "[SELL] ep=1950/4000 reward=-0.0291 eps=0.615 loss=0.00021559695596806705\n",
            "[SELL] ep=1960/4000 reward=-0.0903 eps=0.613 loss=0.00038164498982951045\n",
            "[SELL] ep=1970/4000 reward=0.0517 eps=0.611 loss=0.0003703865804709494\n",
            "[SELL] ep=1980/4000 reward=0.0380 eps=0.609 loss=0.00043126498349010944\n",
            "target sync @ 24500 loss 0.00036323690437711775\n",
            "[SELL] ep=1990/4000 reward=-0.0391 eps=0.607 loss=0.0005789755377918482\n",
            "[SELL] ep=2000/4000 reward=0.0979 eps=0.605 loss=0.0003444126050453633\n",
            "[SELL] ep=2010/4000 reward=-0.0079 eps=0.602 loss=0.00038386275991797447\n",
            "target sync @ 25000 loss 0.0006674302276223898\n",
            "[SELL] ep=2020/4000 reward=-0.0152 eps=0.600 loss=0.0003274924820289016\n",
            "[SELL] ep=2030/4000 reward=-0.0615 eps=0.598 loss=0.00036863284185528755\n",
            "[SELL] ep=2040/4000 reward=0.0556 eps=0.596 loss=0.00025013519916683435\n",
            "[SELL] ep=2050/4000 reward=0.0042 eps=0.594 loss=0.0001945163239724934\n",
            "target sync @ 25500 loss 0.00032168126199394464\n",
            "[SELL] ep=2060/4000 reward=0.0003 eps=0.592 loss=0.00041809340473264456\n",
            "[SELL] ep=2070/4000 reward=-0.0026 eps=0.590 loss=0.00028329004999250174\n",
            "[SELL] ep=2080/4000 reward=-0.0428 eps=0.588 loss=0.0005309596890583634\n",
            "[SELL] ep=2090/4000 reward=-0.0063 eps=0.586 loss=0.00025175936752930284\n",
            "target sync @ 26000 loss 0.0002827309363055974\n",
            "[SELL] ep=2100/4000 reward=0.0023 eps=0.584 loss=0.000328443682519719\n",
            "[SELL] ep=2110/4000 reward=0.0379 eps=0.582 loss=0.0004072088631801307\n",
            "[SELL] ep=2120/4000 reward=-0.0194 eps=0.580 loss=0.00024394490174017847\n",
            "[SELL] ep=2130/4000 reward=0.0405 eps=0.578 loss=0.0003057637659367174\n",
            "target sync @ 26500 loss 0.00020807707915082574\n",
            "[SELL] ep=2140/4000 reward=0.0003 eps=0.576 loss=0.0004664988664444536\n",
            "[SELL] ep=2150/4000 reward=-0.0061 eps=0.574 loss=0.0002083371509797871\n",
            "[SELL] ep=2160/4000 reward=-0.0202 eps=0.571 loss=0.0002840380766429007\n",
            "[SELL] ep=2170/4000 reward=-0.0005 eps=0.569 loss=0.00035381471388973296\n",
            "target sync @ 27000 loss 0.00030972465174272656\n",
            "[SELL] ep=2180/4000 reward=-0.0718 eps=0.567 loss=0.00031215412309393287\n",
            "[SELL] ep=2190/4000 reward=0.0661 eps=0.565 loss=0.00023418516502715647\n",
            "[SELL] ep=2200/4000 reward=0.0256 eps=0.563 loss=0.00026428609271533787\n",
            "[SELL] ep=2210/4000 reward=0.0439 eps=0.561 loss=0.00020415271865203977\n",
            "target sync @ 27500 loss 0.00020264284103177488\n",
            "[SELL] ep=2220/4000 reward=-0.0005 eps=0.559 loss=0.00022580320364795625\n",
            "[SELL] ep=2230/4000 reward=0.0430 eps=0.557 loss=0.0002589490031823516\n",
            "[SELL] ep=2240/4000 reward=-0.0003 eps=0.555 loss=0.0002765512326732278\n",
            "target sync @ 28000 loss 0.0001981452660402283\n",
            "[SELL] ep=2250/4000 reward=-0.0879 eps=0.553 loss=0.00030919554410502315\n",
            "[SELL] ep=2260/4000 reward=0.0115 eps=0.550 loss=0.0005268530803732574\n",
            "[SELL] ep=2270/4000 reward=0.0008 eps=0.548 loss=0.000302327360259369\n",
            "[SELL] ep=2280/4000 reward=-0.0336 eps=0.546 loss=0.0005060380790382624\n",
            "target sync @ 28500 loss 0.0002285538794239983\n",
            "[SELL] ep=2290/4000 reward=-0.0026 eps=0.544 loss=0.0003850811335723847\n",
            "[SELL] ep=2300/4000 reward=0.0430 eps=0.542 loss=0.00024374363420065492\n",
            "[SELL] ep=2310/4000 reward=0.0195 eps=0.540 loss=0.00027903474983759224\n",
            "[SELL] ep=2320/4000 reward=-0.0167 eps=0.538 loss=0.0009034785907715559\n",
            "target sync @ 29000 loss 0.0002564951137173921\n",
            "[SELL] ep=2330/4000 reward=0.0000 eps=0.535 loss=0.0004520472139120102\n",
            "[SELL] ep=2340/4000 reward=-0.0481 eps=0.533 loss=0.0003508437075652182\n",
            "[SELL] ep=2350/4000 reward=-0.0072 eps=0.531 loss=0.00044010201236233115\n",
            "target sync @ 29500 loss 0.0002931900671683252\n",
            "[SELL] ep=2360/4000 reward=0.0531 eps=0.529 loss=0.00023633692762814462\n",
            "[SELL] ep=2370/4000 reward=-0.0294 eps=0.527 loss=0.00037494261050596833\n",
            "[SELL] ep=2380/4000 reward=0.0000 eps=0.524 loss=0.00022308490588329732\n",
            "[SELL] ep=2390/4000 reward=0.0071 eps=0.522 loss=0.00028452498372644186\n",
            "target sync @ 30000 loss 0.00023475533816963434\n",
            "[SELL] ep=2400/4000 reward=-0.0060 eps=0.520 loss=0.0003904896730091423\n",
            "[SELL] ep=2410/4000 reward=-0.0135 eps=0.518 loss=0.00034624565159901977\n",
            "[SELL] ep=2420/4000 reward=-0.0026 eps=0.516 loss=0.0002509238547645509\n",
            "target sync @ 30500 loss 0.00034844939364120364\n",
            "[SELL] ep=2430/4000 reward=0.0000 eps=0.513 loss=0.00029114450444467366\n",
            "[SELL] ep=2440/4000 reward=-0.0247 eps=0.511 loss=0.00018055064720101655\n",
            "[SELL] ep=2450/4000 reward=-0.0236 eps=0.509 loss=0.00026509189046919346\n",
            "[SELL] ep=2460/4000 reward=-0.0743 eps=0.507 loss=0.00028617604402825236\n",
            "target sync @ 31000 loss 0.00021855684462934732\n",
            "[SELL] ep=2470/4000 reward=0.0940 eps=0.504 loss=0.000286580208921805\n",
            "[SELL] ep=2480/4000 reward=-0.0903 eps=0.502 loss=0.00039965007454156876\n",
            "[SELL] ep=2490/4000 reward=0.0602 eps=0.500 loss=0.00048099219566211104\n",
            "[SELL] ep=2500/4000 reward=0.0064 eps=0.498 loss=0.0002847191644832492\n",
            "target sync @ 31500 loss 0.00030547211645171046\n",
            "[SELL] ep=2510/4000 reward=0.0000 eps=0.496 loss=0.00039814587216824293\n",
            "[SELL] ep=2520/4000 reward=0.0157 eps=0.494 loss=0.00020141774439252913\n",
            "[SELL] ep=2530/4000 reward=0.0427 eps=0.492 loss=0.0004186982987448573\n",
            "target sync @ 32000 loss 0.000645991531200707\n",
            "[SELL] ep=2540/4000 reward=-0.0362 eps=0.490 loss=0.00039741292130202055\n",
            "[SELL] ep=2550/4000 reward=-0.0398 eps=0.487 loss=0.0003525168285705149\n",
            "[SELL] ep=2560/4000 reward=-0.0733 eps=0.485 loss=0.0002584169269539416\n",
            "[SELL] ep=2570/4000 reward=0.0266 eps=0.483 loss=0.0002693741407711059\n",
            "target sync @ 32500 loss 0.000533405807800591\n",
            "[SELL] ep=2580/4000 reward=-0.1208 eps=0.481 loss=0.00027329043950885534\n",
            "[SELL] ep=2590/4000 reward=0.0377 eps=0.479 loss=0.0002429365849820897\n",
            "[SELL] ep=2600/4000 reward=-0.0422 eps=0.477 loss=0.00023647233319934458\n",
            "[SELL] ep=2610/4000 reward=0.0584 eps=0.475 loss=0.00015498997527174652\n",
            "target sync @ 33000 loss 0.0002903742133639753\n",
            "[SELL] ep=2620/4000 reward=-0.0189 eps=0.472 loss=0.0005900191608816385\n",
            "[SELL] ep=2630/4000 reward=0.0275 eps=0.470 loss=0.00029270947561599314\n",
            "[SELL] ep=2640/4000 reward=-0.0131 eps=0.468 loss=0.00028936672606505454\n",
            "target sync @ 33500 loss 0.00020332122221589088\n",
            "[SELL] ep=2650/4000 reward=-0.0072 eps=0.466 loss=0.00020332122221589088\n",
            "[SELL] ep=2660/4000 reward=0.0028 eps=0.464 loss=0.00036373839247971773\n",
            "[SELL] ep=2670/4000 reward=-0.0721 eps=0.462 loss=0.00027015447267331183\n",
            "[SELL] ep=2680/4000 reward=0.0174 eps=0.460 loss=0.00042871819459833205\n",
            "target sync @ 34000 loss 0.00032536324579268694\n",
            "[SELL] ep=2690/4000 reward=0.0273 eps=0.457 loss=0.0008590651559643447\n",
            "[SELL] ep=2700/4000 reward=-0.0070 eps=0.455 loss=0.00027484516613185406\n",
            "[SELL] ep=2710/4000 reward=0.0784 eps=0.453 loss=0.00023509893799200654\n",
            "[SELL] ep=2720/4000 reward=-0.0938 eps=0.451 loss=0.00041973686893470585\n",
            "target sync @ 34500 loss 0.00034976500319316983\n",
            "[SELL] ep=2730/4000 reward=-0.0152 eps=0.449 loss=0.0005344933597370982\n",
            "[SELL] ep=2740/4000 reward=-0.0328 eps=0.447 loss=0.0002347517729504034\n",
            "[SELL] ep=2750/4000 reward=0.0000 eps=0.444 loss=0.00018702971283346415\n",
            "target sync @ 35000 loss 0.0002458936651237309\n",
            "[SELL] ep=2760/4000 reward=0.0675 eps=0.442 loss=0.0002458936651237309\n",
            "[SELL] ep=2770/4000 reward=-0.0032 eps=0.440 loss=0.0003335911897011101\n",
            "[SELL] ep=2780/4000 reward=-0.0713 eps=0.438 loss=0.0005564207676798105\n",
            "[SELL] ep=2790/4000 reward=0.0002 eps=0.436 loss=0.0002452339103911072\n",
            "target sync @ 35500 loss 0.00043109507532790303\n",
            "[SELL] ep=2800/4000 reward=-0.0103 eps=0.434 loss=0.0002802203525789082\n",
            "[SELL] ep=2810/4000 reward=-0.0858 eps=0.432 loss=0.00030652424902655184\n",
            "[SELL] ep=2820/4000 reward=-0.1464 eps=0.429 loss=0.00015653553418815136\n",
            "[SELL] ep=2830/4000 reward=-0.0398 eps=0.427 loss=0.00026137474924325943\n",
            "target sync @ 36000 loss 0.00018222468497697264\n",
            "[SELL] ep=2840/4000 reward=0.0373 eps=0.425 loss=0.00039126817137002945\n",
            "[SELL] ep=2850/4000 reward=0.0232 eps=0.423 loss=0.000248581578489393\n",
            "[SELL] ep=2860/4000 reward=0.0427 eps=0.421 loss=0.00020717694133054465\n",
            "[SELL] ep=2870/4000 reward=0.0858 eps=0.419 loss=0.00019493269792292267\n",
            "target sync @ 36500 loss 0.0001514499308541417\n",
            "[SELL] ep=2880/4000 reward=-0.0001 eps=0.417 loss=0.00016542174853384495\n",
            "[SELL] ep=2890/4000 reward=-0.0497 eps=0.414 loss=0.000210317230084911\n",
            "[SELL] ep=2900/4000 reward=0.0334 eps=0.412 loss=0.00026161156711168587\n",
            "target sync @ 37000 loss 0.00023804875672794878\n",
            "[SELL] ep=2910/4000 reward=0.0464 eps=0.410 loss=0.00018536674906499684\n",
            "[SELL] ep=2920/4000 reward=-0.0269 eps=0.407 loss=0.00023703582701273263\n",
            "[SELL] ep=2930/4000 reward=-0.0294 eps=0.405 loss=0.00021342217223718762\n",
            "[SELL] ep=2940/4000 reward=-0.0236 eps=0.403 loss=0.00018976908177137375\n",
            "target sync @ 37500 loss 0.00012981021427549422\n",
            "[SELL] ep=2950/4000 reward=0.0000 eps=0.400 loss=0.0001519002253189683\n",
            "[SELL] ep=2960/4000 reward=-0.1565 eps=0.398 loss=0.0002078894031001255\n",
            "[SELL] ep=2970/4000 reward=-0.0212 eps=0.395 loss=0.0004324436013121158\n",
            "target sync @ 38000 loss 0.0001698842243058607\n",
            "[SELL] ep=2980/4000 reward=-0.2139 eps=0.393 loss=0.00021639413898810744\n",
            "[SELL] ep=2990/4000 reward=0.0410 eps=0.391 loss=0.0001282939629163593\n",
            "[SELL] ep=3000/4000 reward=0.0020 eps=0.389 loss=0.0002146714978152886\n",
            "[SELL] ep=3010/4000 reward=-0.0175 eps=0.387 loss=0.0001060573267750442\n",
            "target sync @ 38500 loss 0.00012026303738821298\n",
            "[SELL] ep=3020/4000 reward=0.0290 eps=0.385 loss=0.00033881558920256793\n",
            "[SELL] ep=3030/4000 reward=-0.0127 eps=0.383 loss=0.0003168334951624274\n",
            "[SELL] ep=3040/4000 reward=0.0563 eps=0.380 loss=0.00013532646698877215\n",
            "target sync @ 39000 loss 0.00012130688992328942\n",
            "[SELL] ep=3050/4000 reward=0.0000 eps=0.378 loss=0.00013388186926022172\n",
            "[SELL] ep=3060/4000 reward=-0.0352 eps=0.376 loss=0.00015378009993582964\n",
            "[SELL] ep=3070/4000 reward=-0.0540 eps=0.374 loss=0.00010713079973356798\n",
            "[SELL] ep=3080/4000 reward=0.0608 eps=0.372 loss=0.000226631760597229\n",
            "target sync @ 39500 loss 0.0001337106805294752\n",
            "[SELL] ep=3090/4000 reward=0.0084 eps=0.370 loss=0.00016831337416078895\n",
            "[SELL] ep=3100/4000 reward=-0.0352 eps=0.367 loss=0.00014442336396314204\n",
            "[SELL] ep=3110/4000 reward=0.0000 eps=0.365 loss=0.00014709908282384276\n",
            "[SELL] ep=3120/4000 reward=0.1396 eps=0.363 loss=0.00016435037832707167\n",
            "target sync @ 40000 loss 7.707507029408589e-05\n",
            "[SELL] ep=3130/4000 reward=-0.0186 eps=0.360 loss=9.184358350466937e-05\n",
            "[SELL] ep=3140/4000 reward=-0.1486 eps=0.358 loss=8.12379366834648e-05\n",
            "[SELL] ep=3150/4000 reward=0.1827 eps=0.356 loss=0.0001514817849965766\n",
            "target sync @ 40500 loss 9.843980660662055e-05\n",
            "[SELL] ep=3160/4000 reward=0.0784 eps=0.354 loss=0.00014927523443475366\n",
            "[SELL] ep=3170/4000 reward=-0.1168 eps=0.352 loss=0.00011588574852794409\n",
            "[SELL] ep=3180/4000 reward=-0.0259 eps=0.350 loss=7.776886923238635e-05\n",
            "target sync @ 41000 loss 6.904968176968396e-05\n",
            "[SELL] ep=3190/4000 reward=0.0046 eps=0.347 loss=0.0002709381515160203\n",
            "[SELL] ep=3200/4000 reward=-0.0256 eps=0.345 loss=0.00019720467389561236\n",
            "[SELL] ep=3210/4000 reward=0.0000 eps=0.343 loss=8.15347520983778e-05\n",
            "[SELL] ep=3220/4000 reward=0.1148 eps=0.341 loss=6.840976129751652e-05\n",
            "target sync @ 41500 loss 6.77674324833788e-05\n",
            "[SELL] ep=3230/4000 reward=-0.0127 eps=0.338 loss=9.422712901141495e-05\n",
            "[SELL] ep=3240/4000 reward=-0.0210 eps=0.336 loss=8.634566620457917e-05\n",
            "[SELL] ep=3250/4000 reward=-0.0129 eps=0.334 loss=0.00010781738092191517\n",
            "[SELL] ep=3260/4000 reward=0.0106 eps=0.332 loss=0.0001220426638610661\n",
            "target sync @ 42000 loss 7.029900734778494e-05\n",
            "[SELL] ep=3270/4000 reward=0.0430 eps=0.329 loss=0.00028698783717118204\n",
            "[SELL] ep=3280/4000 reward=0.0003 eps=0.327 loss=0.00012570488615892828\n",
            "[SELL] ep=3290/4000 reward=-0.0697 eps=0.325 loss=8.718173921806738e-05\n",
            "target sync @ 42500 loss 0.0001139836895163171\n",
            "[SELL] ep=3300/4000 reward=-0.0100 eps=0.322 loss=0.00010956246114801615\n",
            "[SELL] ep=3310/4000 reward=-0.0387 eps=0.320 loss=0.00010877359454752877\n",
            "[SELL] ep=3320/4000 reward=0.0000 eps=0.317 loss=4.5987173507455736e-05\n",
            "target sync @ 43000 loss 5.551268986891955e-05\n",
            "[SELL] ep=3330/4000 reward=0.0000 eps=0.315 loss=6.430430221371353e-05\n",
            "[SELL] ep=3340/4000 reward=0.1396 eps=0.313 loss=0.00012453396630007774\n",
            "[SELL] ep=3350/4000 reward=0.0099 eps=0.311 loss=8.564967720303684e-05\n",
            "[SELL] ep=3360/4000 reward=-0.0441 eps=0.309 loss=7.889838889241219e-05\n",
            "target sync @ 43500 loss 9.510546806268394e-05\n",
            "[SELL] ep=3370/4000 reward=-0.0610 eps=0.307 loss=7.192077464424074e-05\n",
            "[SELL] ep=3380/4000 reward=-0.0248 eps=0.304 loss=0.00014416842896025628\n",
            "[SELL] ep=3390/4000 reward=0.0021 eps=0.302 loss=8.886076102498919e-05\n",
            "target sync @ 44000 loss 3.250538065913133e-05\n",
            "[SELL] ep=3400/4000 reward=0.0525 eps=0.299 loss=0.00011388146231183782\n",
            "[SELL] ep=3410/4000 reward=-0.0340 eps=0.297 loss=2.9035378247499466e-05\n",
            "[SELL] ep=3420/4000 reward=0.0491 eps=0.295 loss=6.554944411618635e-05\n",
            "[SELL] ep=3430/4000 reward=-0.0132 eps=0.293 loss=0.00011133639782201499\n",
            "target sync @ 44500 loss 9.160843910649419e-05\n",
            "[SELL] ep=3440/4000 reward=-0.0286 eps=0.291 loss=0.00012669608986470848\n",
            "[SELL] ep=3450/4000 reward=-0.2166 eps=0.289 loss=4.828739110962488e-05\n",
            "[SELL] ep=3460/4000 reward=-0.0109 eps=0.287 loss=4.90890015498735e-05\n",
            "[SELL] ep=3470/4000 reward=-0.0005 eps=0.285 loss=4.194317079964094e-05\n",
            "target sync @ 45000 loss 5.3449537517735735e-05\n",
            "[SELL] ep=3480/4000 reward=0.0201 eps=0.282 loss=9.83430290943943e-05\n",
            "[SELL] ep=3490/4000 reward=0.0181 eps=0.281 loss=5.7952267525251955e-05\n",
            "[SELL] ep=3500/4000 reward=0.0505 eps=0.278 loss=6.723833212163299e-05\n",
            "[SELL] ep=3510/4000 reward=0.0032 eps=0.276 loss=4.1771403630264103e-05\n",
            "target sync @ 45500 loss 4.111425005248748e-05\n",
            "[SELL] ep=3520/4000 reward=-0.0324 eps=0.274 loss=7.647173333680257e-05\n",
            "[SELL] ep=3530/4000 reward=-0.0602 eps=0.272 loss=9.287951979786158e-05\n",
            "[SELL] ep=3540/4000 reward=0.0584 eps=0.270 loss=8.24711169116199e-05\n",
            "target sync @ 46000 loss 6.306626892182976e-05\n",
            "[SELL] ep=3550/4000 reward=-0.0330 eps=0.268 loss=0.00013041614147368819\n",
            "[SELL] ep=3560/4000 reward=0.0084 eps=0.266 loss=6.189694977365434e-05\n",
            "[SELL] ep=3570/4000 reward=-0.0319 eps=0.264 loss=9.353125642519444e-05\n",
            "[SELL] ep=3580/4000 reward=0.1399 eps=0.261 loss=5.9490761486813426e-05\n",
            "target sync @ 46500 loss 0.00010164087871089578\n",
            "[SELL] ep=3590/4000 reward=-0.0148 eps=0.259 loss=6.549076351802796e-05\n",
            "[SELL] ep=3600/4000 reward=-0.0370 eps=0.257 loss=3.211829607607797e-05\n",
            "[SELL] ep=3610/4000 reward=-0.0101 eps=0.255 loss=6.784219294786453e-05\n",
            "[SELL] ep=3620/4000 reward=-0.0466 eps=0.252 loss=6.332409975584596e-05\n",
            "target sync @ 47000 loss 0.00010073282464873046\n",
            "[SELL] ep=3630/4000 reward=-0.0300 eps=0.250 loss=5.706253068638034e-05\n",
            "[SELL] ep=3640/4000 reward=-0.0288 eps=0.248 loss=5.9185505961067975e-05\n",
            "[SELL] ep=3650/4000 reward=0.0499 eps=0.246 loss=5.681185211869888e-05\n",
            "target sync @ 47500 loss 4.0522714698454365e-05\n",
            "[SELL] ep=3660/4000 reward=-0.0154 eps=0.244 loss=4.0522714698454365e-05\n",
            "[SELL] ep=3670/4000 reward=-0.0143 eps=0.242 loss=6.387504981830716e-05\n",
            "[SELL] ep=3680/4000 reward=0.0033 eps=0.240 loss=6.930284143891186e-05\n",
            "[SELL] ep=3690/4000 reward=-0.0185 eps=0.238 loss=9.07033754629083e-05\n",
            "target sync @ 48000 loss 6.030560325598344e-05\n",
            "[SELL] ep=3700/4000 reward=-0.0272 eps=0.236 loss=7.907150575192645e-05\n",
            "[SELL] ep=3710/4000 reward=0.0000 eps=0.234 loss=5.4133128287503496e-05\n",
            "[SELL] ep=3720/4000 reward=-0.0399 eps=0.232 loss=9.106127254199237e-05\n",
            "[SELL] ep=3730/4000 reward=-0.0280 eps=0.230 loss=3.6156347050564364e-05\n",
            "target sync @ 48500 loss 4.482958684093319e-05\n",
            "[SELL] ep=3740/4000 reward=-0.0026 eps=0.228 loss=4.4883865484735e-05\n",
            "[SELL] ep=3750/4000 reward=-0.0295 eps=0.225 loss=6.141617632238194e-05\n",
            "[SELL] ep=3760/4000 reward=0.1026 eps=0.223 loss=0.00014290957187768072\n",
            "[SELL] ep=3770/4000 reward=0.0000 eps=0.221 loss=3.064078919123858e-05\n",
            "target sync @ 49000 loss 4.63506075902842e-05\n",
            "[SELL] ep=3780/4000 reward=0.0427 eps=0.218 loss=8.787195110926405e-05\n",
            "[SELL] ep=3790/4000 reward=-0.0315 eps=0.216 loss=0.00011697546142386273\n",
            "[SELL] ep=3800/4000 reward=-0.0554 eps=0.214 loss=3.2118448871187866e-05\n",
            "target sync @ 49500 loss 5.16276340931654e-05\n",
            "[SELL] ep=3810/4000 reward=-0.1186 eps=0.211 loss=9.315422357758507e-05\n",
            "[SELL] ep=3820/4000 reward=0.0123 eps=0.209 loss=4.5959881390444934e-05\n",
            "[SELL] ep=3830/4000 reward=0.0262 eps=0.207 loss=6.647598638664931e-05\n",
            "[SELL] ep=3840/4000 reward=-0.0632 eps=0.205 loss=6.0303947975626215e-05\n",
            "target sync @ 50000 loss 0.00011217600695090368\n",
            "[SELL] ep=3850/4000 reward=0.0115 eps=0.203 loss=5.107549441163428e-05\n",
            "[SELL] ep=3860/4000 reward=-0.0477 eps=0.200 loss=9.032004163600504e-05\n",
            "[SELL] ep=3870/4000 reward=0.0181 eps=0.198 loss=2.8373015084071085e-05\n",
            "target sync @ 50500 loss 6.838318950030953e-05\n",
            "[SELL] ep=3880/4000 reward=-0.0251 eps=0.196 loss=3.7915160646662116e-05\n",
            "[SELL] ep=3890/4000 reward=0.0584 eps=0.194 loss=6.272199971135706e-05\n",
            "[SELL] ep=3900/4000 reward=0.0000 eps=0.191 loss=0.00011256719881203026\n",
            "[SELL] ep=3910/4000 reward=-0.0470 eps=0.189 loss=8.81483792909421e-05\n",
            "target sync @ 51000 loss 7.482006913051009e-05\n",
            "[SELL] ep=3920/4000 reward=0.1463 eps=0.187 loss=7.967544661369175e-05\n",
            "[SELL] ep=3930/4000 reward=0.0499 eps=0.185 loss=6.865456089144573e-05\n",
            "[SELL] ep=3940/4000 reward=0.0013 eps=0.183 loss=0.00010979010403389111\n",
            "target sync @ 51500 loss 5.264891660772264e-05\n",
            "[SELL] ep=3950/4000 reward=-0.0282 eps=0.181 loss=6.569705146830529e-05\n",
            "[SELL] ep=3960/4000 reward=0.1492 eps=0.179 loss=4.2545008909655735e-05\n",
            "[SELL] ep=3970/4000 reward=0.0194 eps=0.176 loss=2.7139452868141234e-05\n",
            "[SELL] ep=3980/4000 reward=0.0421 eps=0.174 loss=5.053171844338067e-05\n",
            "target sync @ 52000 loss 7.610057946294546e-05\n",
            "[SELL] ep=3990/4000 reward=-0.1425 eps=0.172 loss=3.5339395253686234e-05\n",
            "[SELL] ep=4000/4000 reward=0.0000 eps=0.170 loss=3.505103086354211e-05\n",
            "Saved: runs/sell_agent.pt\n",
            "\n",
            "=== SELL EVAL (TEST entries) ===\n",
            "n_entries: 104\n",
            "\n",
            "SellAgent:\n",
            "mean: -0.00789098573250663 median: 0.0 win_rate: 0.2980769230769231 min/max: -0.27628554112090287 0.13531363579888112\n",
            "\n",
            "Fixed horizon baseline (hold->forced exit):\n",
            "mean: 0.0 median: 0.0 win_rate: 0.0 min/max: 0.0 0.0\n",
            "\n",
            "Delta (agent - baseline):\n",
            "mean delta: -0.00789098573250663 median delta: 0.0 better %: 0.2980769230769231\n",
            "\n",
            "Per-entry delta: [-0.0037  0.0261  0.      0.0214 -0.0234  0.004   0.0058 -0.0425  0.\n",
            "  0.      0.0146  0.      0.0005 -0.0139 -0.0065 -0.0773  0.      0.\n",
            " -0.0533  0.0209  0.0501  0.1028  0.      0.0178  0.02   -0.0444  0.\n",
            "  0.      0.      0.      0.     -0.0541 -0.0049  0.0369  0.      0.0141\n",
            "  0.     -0.0407  0.     -0.0271 -0.0094  0.      0.1058  0.0361 -0.0946\n",
            " -0.0413 -0.2241  0.      0.0052 -0.11    0.      0.      0.     -0.2763\n",
            "  0.     -0.0375  0.0258 -0.0043  0.      0.0066 -0.0585 -0.0684  0.\n",
            "  0.0067 -0.1632  0.051   0.0509  0.0572  0.0103  0.0527  0.      0.\n",
            " -0.0318  0.      0.      0.      0.      0.      0.      0.0365 -0.0247\n",
            "  0.      0.      0.     -0.0211  0.0323 -0.0778  0.0334 -0.0139  0.1353\n",
            " -0.0857  0.0349 -0.0332 -0.0576  0.      0.      0.      0.      0.0573\n",
            "  0.      0.     -0.0442  0.002  -0.0265]\n",
            "Better count: 31 / 104\n",
            "\n",
            "SellAgent exit stats:\n",
            "avg hold bars: 15.634615384615385 min/max hold bars: 10.0 20.0\n",
            "exit reasons: {'limit': 40, 'sell': 64}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "from agents.ddqn_agent import DDQNAgent\n",
        "from envs.sell_env import SellEnv  # <- make sure this is the TM-consistent SellEnv v1\n",
        "\n",
        "# Load entry indices saved by your TM cell\n",
        "entries_train = np.load(os.path.join(out_dir, \"entry_indices_train.npy\"))\n",
        "entries_test  = np.load(os.path.join(out_dir, \"entry_indices_test.npy\"))\n",
        "\n",
        "print(\"entries_train:\", entries_train.shape, \"entries_test:\", entries_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# Create SellEnv (TRAIN)\n",
        "# -----------------------\n",
        "sell_env_train = SellEnv(\n",
        "    features=X_train,\n",
        "    prices=p_train,\n",
        "    entry_indices=entries_train,\n",
        "    transaction_cost=cfg.reward.transaction_cost,\n",
        "    sell_horizon=cfg.trade_manager.sell_horizon,\n",
        "    min_hold_bars=cfg.trade_manager.min_hold_bars,\n",
        "    segment_len=SEG_TRAIN,\n",
        "    include_pos_features=True,\n",
        ")\n",
        "\n",
        "print(\"sell include_pos:\", sell_env_train.include_pos)\n",
        "print(\"sell feat_dim:\", sell_env_train.feat_dim, \"state_dim:\", sell_env_train.state_dim)\n",
        "\n",
        "# -----------------------\n",
        "# Create SellAgent config\n",
        "# # -----------------------\n",
        "# sell_cfg = deepcopy(cfg.agent)\n",
        "# sell_cfg.state_dim = int(sell_env_train.state_dim)\n",
        "# sell_cfg.n_actions = 2\n",
        "\n",
        "# # faster decay is fine, but DON'T double-count total_steps\n",
        "# sell_cfg.epsilon_start = 1.0\n",
        "# sell_cfg.epsilon_end = 0.05\n",
        "# sell_cfg.epsilon_decay_steps = 40000\n",
        "\n",
        "# sell_agent = DDQNAgent(sell_cfg)\n",
        "# -----------------------\n",
        "# Create SellAgent config\n",
        "# -----------------------\n",
        "sell_cfg = deepcopy(cfg.agent)\n",
        "sell_cfg.state_dim = int(sell_env_train.state_dim)\n",
        "sell_cfg.n_actions = 2\n",
        "\n",
        "# OPTIMIZED HYPERPARAMS FOR SELL AGENT\n",
        "sell_cfg.lr = 0.0005                 # Slightly lower LR for stability\n",
        "sell_cfg.epsilon_start = 1.0\n",
        "sell_cfg.epsilon_end = 0.05\n",
        "\n",
        "# We want decay to finish at ~80% of training\n",
        "EPISODES = 5000\n",
        "avg_steps_per_ep = 15  # horizon is 20, exits often happen around 10-20\n",
        "total_estimated_steps = EPISODES * avg_steps_per_ep\n",
        "\n",
        "sell_cfg.epsilon_decay_steps = int(total_estimated_steps * 0.8)\n",
        "\n",
        "sell_agent = DDQNAgent(sell_cfg)\n",
        "print(f\"Sell Decay Steps: {sell_cfg.epsilon_decay_steps} / Est Total: {total_estimated_steps}\")\n",
        "\n",
        "# ... training loop follows ...\n",
        "print(\"SELL state_dim:\", sell_cfg.state_dim, \"n_actions:\", sell_cfg.n_actions)\n",
        "\n",
        "# -----------------------\n",
        "# Train loop (episode-based)\n",
        "# -----------------------\n",
        "# EPISODES = 800\n",
        "EPISODES = 4000\n",
        "MAX_STEPS = 200         # safety cap (horizon is small anyway)\n",
        "UPDATES_PER_STEP = 1\n",
        "\n",
        "for ep in range(EPISODES):\n",
        "    s = sell_env_train.reset()\n",
        "    done = False\n",
        "    ep_reward = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    while (not done) and (steps < MAX_STEPS):\n",
        "        a = sell_agent.select_action(s, greedy=False)  # <- this increments total_steps internally\n",
        "        ns, r, done, info = sell_env_train.step(a)\n",
        "\n",
        "        sell_agent.push(s, a, r, ns, done)\n",
        "\n",
        "        # update after warmup (based on agent.total_steps, which is now correct)\n",
        "        if sell_agent.total_steps >= int(cfg.training.warmup_steps):\n",
        "            for _ in range(UPDATES_PER_STEP):\n",
        "                sell_agent.update()\n",
        "\n",
        "        s = ns\n",
        "        ep_reward += float(r)\n",
        "        steps += 1\n",
        "\n",
        "    if (ep + 1) % 10 == 0:\n",
        "        loss = sell_agent.loss_history[-1] if sell_agent.loss_history else None\n",
        "        print(f\"[SELL] ep={ep+1}/{EPISODES} reward={ep_reward:.4f} eps={sell_agent.eps:.3f} loss={loss}\")\n",
        "\n",
        "# Save model\n",
        "sell_path = os.path.join(out_dir, \"sell_agent.pt\")\n",
        "sell_agent.save(sell_path)\n",
        "print(\"Saved:\", sell_path)\n",
        "\n",
        "# -----------------------\n",
        "# Evaluation helpers\n",
        "# -----------------------\n",
        "def eval_sell_agent(env, agent, entry_indices, greedy=True):\n",
        "    rets, holds, exits, reasons = [], [], [], []\n",
        "    for e in entry_indices:\n",
        "        s = env.reset(int(e))\n",
        "        done = False\n",
        "        total_r = 0.0\n",
        "        steps = 0\n",
        "        last_info = None\n",
        "\n",
        "        while (not done) and (steps < 500):\n",
        "            a = agent.select_action(s, greedy=greedy)  # greedy=True => no eps update, no step increment\n",
        "            ns, r, done, info = env.step(a)\n",
        "            s = ns\n",
        "            total_r += float(r)\n",
        "            steps += 1\n",
        "            last_info = info\n",
        "\n",
        "        rets.append(total_r)\n",
        "        if last_info:\n",
        "            exits.append(last_info.get(\"exit_idx\", np.nan))\n",
        "            holds.append(last_info.get(\"bars_held\", np.nan))\n",
        "            reasons.append(last_info.get(\"reason\", \"\"))\n",
        "        else:\n",
        "            exits.append(np.nan); holds.append(np.nan); reasons.append(\"\")\n",
        "\n",
        "    return np.array(rets, float), np.array(holds, float), np.array(exits, float), reasons\n",
        "\n",
        "\n",
        "def eval_fixed_horizon(env, entry_indices):\n",
        "    # Just HOLD until forced exit\n",
        "    rets = []\n",
        "    for e in entry_indices:\n",
        "        s = env.reset(int(e))\n",
        "        done = False\n",
        "        total_r = 0.0\n",
        "        steps = 0\n",
        "        while (not done) and (steps < 500):\n",
        "            ns, r, done, info = env.step(0)  # HOLD\n",
        "            s = ns\n",
        "            total_r += float(r)\n",
        "            steps += 1\n",
        "        rets.append(total_r)\n",
        "    return np.array(rets, dtype=float)\n",
        "\n",
        "# -----------------------\n",
        "# Evaluate on TEST subset\n",
        "# -----------------------\n",
        "sell_env_test = SellEnv(\n",
        "    features=X_test,\n",
        "    prices=p_test,\n",
        "    entry_indices=entries_test,\n",
        "    transaction_cost=cfg.reward.transaction_cost,\n",
        "    sell_horizon=cfg.trade_manager.sell_horizon,\n",
        "    min_hold_bars=cfg.trade_manager.min_hold_bars,\n",
        "    segment_len=SEG_TEST,\n",
        "    include_pos_features=True,\n",
        ")\n",
        "\n",
        "rets_agent, holds_agent, exits_agent, reasons_agent = eval_sell_agent(\n",
        "    sell_env_test, sell_agent, entries_test, greedy=True\n",
        ")\n",
        "rets_base = eval_fixed_horizon(sell_env_test, entries_test)\n",
        "\n",
        "print(\"\\n=== SELL EVAL (TEST entries) ===\")\n",
        "print(\"n_entries:\", len(entries_test))\n",
        "\n",
        "print(\"\\nSellAgent:\")\n",
        "print(\"mean:\", float(rets_agent.mean()),\n",
        "      \"median:\", float(np.median(rets_agent)),\n",
        "      \"win_rate:\", float((rets_agent > 0).mean()),\n",
        "      \"min/max:\", float(rets_agent.min()), float(rets_agent.max()))\n",
        "\n",
        "print(\"\\nFixed horizon baseline (hold->forced exit):\")\n",
        "print(\"mean:\", float(rets_base.mean()),\n",
        "      \"median:\", float(np.median(rets_base)),\n",
        "      \"win_rate:\", float((rets_base > 0).mean()),\n",
        "      \"min/max:\", float(rets_base.min()), float(rets_base.max()))\n",
        "\n",
        "delta = rets_agent - rets_base\n",
        "print(\"\\nDelta (agent - baseline):\")\n",
        "print(\"mean delta:\", float(delta.mean()),\n",
        "      \"median delta:\", float(np.median(delta)),\n",
        "      \"better %:\", float((delta > 0).mean()))\n",
        "\n",
        "print(\"\\nPer-entry delta:\", np.round(delta, 4))\n",
        "print(\"Better count:\", int((delta > 0).sum()), \"/\", len(delta))\n",
        "\n",
        "# Optional: inspect exit behavior\n",
        "if len(exits_agent) > 0:\n",
        "    print(\"\\nSellAgent exit stats:\")\n",
        "    print(\"avg hold bars:\", float(np.nanmean(holds_agent)),\n",
        "          \"min/max hold bars:\", float(np.nanmin(holds_agent)), float(np.nanmax(holds_agent)))\n",
        "    # quick breakdown of reasons\n",
        "    unique, counts = np.unique(np.array(reasons_agent, dtype=str), return_counts=True)\n",
        "    print(\"exit reasons:\", dict(zip(unique.tolist(), counts.tolist())))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d010456a",
      "metadata": {},
      "source": [
        "# TradeManager with Sell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "51ee24b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA SPLIT ===\n",
            "features: (6195, 12) prices: (6195,)\n",
            "SEG_LEN: 1239 N_SEGS: 5 TRAIN_FRAC: 0.7\n",
            "train_len per seg: 867 test_len per seg: 372\n",
            "X_train: (4335, 12) p_train: (4335,)\n",
            "X_test : (1860, 12) p_test : (1860,)\n",
            "Sell seen: 1363 Sell actions: 154\n",
            "Exit reasons: {'time': 128, 'sell_agent': 16}\n",
            "Non-time exits: [{'entry_idx': 550, 'exit_idx': 563, 'entry_price': 124.69061279296875, 'exit_price': 131.3394775390625, 'gross_return': 0.05332289734699759, 'net_return': 0.05121730487520115, 'hold_bars': 13, 'forced_exit': False, 'meta': {'buy_conf': 0.48883790487071793, 'reason': 'sell_agent', 'sell_conf': 0.7755460419385347, 'sell_q0': 0.016807276755571365, 'sell_q1': 0.04160521179437637, 'sell_margin': 0.024797935038805008, 'sell_delta_vs_hold': 0.0516557459554432, 'sell_baseline_net': -0.00043844108024204687, 'sell_net_now': 0.05121730487520115}}, {'entry_idx': 648, 'exit_idx': 661, 'entry_price': 145.09996032714844, 'exit_price': 144.86520385742188, 'gross_return': -0.0016178947892009697, 'net_return': -0.0036136606175173336, 'hold_bars': 13, 'forced_exit': False, 'meta': {'buy_conf': 0.48902296104828574, 'reason': 'sell_agent', 'sell_conf': 0.7790949907305198, 'sell_q0': 0.015714339911937714, 'sell_q1': 0.040922343730926514, 'sell_margin': 0.0252080038189888, 'sell_delta_vs_hold': 0.008677607009306976, 'sell_baseline_net': -0.01229126762682431, 'sell_net_now': -0.0036136606175173336}}, {'entry_idx': 764, 'exit_idx': 775, 'entry_price': 172.8780517578125, 'exit_price': 161.6686248779297, 'gross_return': -0.06484008100453495, 'net_return': -0.06670946568260694, 'hold_bars': 11, 'forced_exit': False, 'meta': {'buy_conf': 0.4856012648926856, 'reason': 'sell_agent', 'sell_conf': 0.8672575183637965, 'sell_q0': 0.09610368311405182, 'sell_q1': 0.13364218175411224, 'sell_margin': 0.037538498640060425, 'sell_delta_vs_hold': 0.03583699857057543, 'sell_baseline_net': -0.10254646425318237, 'sell_net_now': -0.06670946568260694}}]\n",
            "ENTRY DEBUG: {'checked': 972, 'blocked_trend': 804, 'blocked_latest_entry': 17, 'blocked_conf': 0, 'opened': 144, 'conf_min': 0.44328194593797604, 'conf_max': 0.5630047978079805, 'blocked_sentiment': 7, 'blocked_sentiment_samples': [{'t': 1661, 'sent': -0.05763888359069824, 'mass': 1.602344036102295, 'conf': 0.48802138636402786}, {'t': 2227, 'sent': -0.12670600414276123, 'mass': 0.5854560136795044, 'conf': 0.48546075294912333}, {'t': 3369, 'sent': 0.04114900156855583, 'mass': 0.7033069729804993, 'conf': 0.4853470423161222}, {'t': 3447, 'sent': -0.11711200326681137, 'mass': 0.7399730086326599, 'conf': 0.4853470423161222}, {'t': 3574, 'sent': -0.02473428286612034, 'mass': 1.302852988243103, 'conf': 0.4853470423161222}]}\n",
            "SELL DEBUG: {'seen': 1363, 'sell_actions': 154}\n",
            "EXIT REASONS: {'time': 128, 'sell_agent': 16}\n",
            "\n",
            "=== TRADE MANAGER (TRAIN) ===\n",
            "n_steps: 4335\n",
            "segment_len: 867\n",
            "n_trades: 144\n",
            "final_equity: 32.54980832909313\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.028074234637256976\n",
            "win rate: 0.6180555555555556\n",
            "min/median/max net: -0.1999990361111923 0.027394169071003782 0.3930759906905483\n",
            "median hold bars: 20.0\n",
            "top 5 net: [0.19186452 0.21719871 0.22808475 0.29425715 0.39307599]\n",
            "bottom 5 net: [-0.19999904 -0.17966912 -0.16360321 -0.14640543 -0.11881024]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 49, 'entry_price': 43.37914276123047, 'exit_price': 47.893470764160156, 'gross_return': 0.10406678683757217, 'net_return': 0.10185975733068386, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4853470423161222, 'reason': 'time'}}\n",
            "{'entry_idx': 54, 'exit_idx': 74, 'entry_price': 48.492679595947266, 'exit_price': 45.55203628540039, 'gross_return': -0.060640973752099606, 'net_return': -0.06251875244556915, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5274239285153728, 'reason': 'time'}}\n",
            "{'entry_idx': 95, 'exit_idx': 115, 'entry_price': 46.465087890625, 'exit_price': 49.00773620605469, 'gross_return': 0.05472169387508354, 'net_return': 0.052613305209027406, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4878637120666459, 'reason': 'time'}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 4206, 'exit_idx': 4226, 'entry_price': 143.90419006347656, 'exit_price': 136.61180114746094, 'gross_return': -0.050675306346527366, 'net_return': -0.052573006409140643, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4854948106007548, 'reason': 'time'}}\n",
            "{'entry_idx': 4232, 'exit_idx': 4252, 'entry_price': 140.53018188476562, 'exit_price': 131.43643188476562, 'gross_return': -0.06471029837175324, 'net_return': -0.06657994248530807, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.49170733296234753, 'reason': 'time'}}\n",
            "{'entry_idx': 4261, 'exit_idx': 4279, 'entry_price': 137.23855590820312, 'exit_price': 129.03404235839844, 'gross_return': -0.059782861277646406, 'net_return': -0.06166235533795239, 'hold_bars': 18, 'forced_exit': False, 'meta': {'buy_conf': 0.49034636493254063, 'reason': 'sell_agent', 'sell_conf': 0.7905600182925134, 'sell_q0': 0.0022044284269213676, 'sell_q1': 0.02877051569521427, 'sell_margin': 0.026566087268292904, 'sell_delta_vs_hold': 0.037491118240959964, 'sell_baseline_net': -0.09915347357891235, 'sell_net_now': -0.06166235533795239}}\n",
            "Sell seen: 536 Sell actions: 93\n",
            "Exit reasons: {'sell_agent': 12, 'time': 48}\n",
            "Non-time exits: [{'entry_idx': 29, 'exit_idx': 46, 'entry_price': 168.69100952148438, 'exit_price': 152.7483673095703, 'gross_return': -0.09450795425990692, 'net_return': -0.09631803285934126, 'hold_bars': 17, 'forced_exit': False, 'meta': {'buy_conf': 0.49320325400894033, 'reason': 'sell_agent', 'sell_conf': 0.7502216248541628, 'sell_q0': -0.0015152357518672943, 'sell_q1': 0.020480656996369362, 'sell_margin': 0.021995892748236656, 'sell_delta_vs_hold': 0.0048292581206501595, 'sell_baseline_net': -0.10114729097999142, 'sell_net_now': -0.09631803285934126}}, {'entry_idx': 78, 'exit_idx': 92, 'entry_price': 153.17132568359375, 'exit_price': 148.47999572753906, 'gross_return': -0.03062799081432216, 'net_return': -0.03256576546068424, 'hold_bars': 14, 'forced_exit': False, 'meta': {'buy_conf': 0.4853470423161222, 'reason': 'sell_agent', 'sell_conf': 0.8135203032376596, 'sell_q0': 0.011175725609064102, 'sell_q1': 0.04063669592142105, 'sell_margin': 0.02946097031235695, 'sell_delta_vs_hold': 0.041721784151318775, 'sell_baseline_net': -0.07428754961200301, 'sell_net_now': -0.03256576546068424}}, {'entry_idx': 372, 'exit_idx': 386, 'entry_price': 257.0619812011719, 'exit_price': 272.83489990234375, 'gross_return': 0.06135842658439725, 'net_return': 0.05923677108965508, 'hold_bars': 14, 'forced_exit': False, 'meta': {'buy_conf': 0.45850428787891295, 'reason': 'sell_agent', 'sell_conf': 0.7522700309484602, 'sell_q0': 0.037275686860084534, 'sell_q1': 0.059490807354450226, 'sell_margin': 0.022215120494365692, 'sell_delta_vs_hold': 0.0015849001926417916, 'sell_baseline_net': 0.05765187089701329, 'sell_net_now': 0.05923677108965508}}]\n",
            "ENTRY DEBUG: {'checked': 496, 'blocked_trend': 407, 'blocked_latest_entry': 25, 'blocked_conf': 1, 'opened': 60, 'conf_min': 0.3882185785515504, 'conf_max': 0.5665328630788287, 'blocked_sentiment': 3, 'blocked_sentiment_samples': [{'t': 344, 'sent': -0.017996761947870255, 'mass': 1.6865910291671753, 'conf': 0.4868276282150827}, {'t': 510, 'sent': -0.10528286546468735, 'mass': 1.8700300455093384, 'conf': 0.4853470423161222}, {'t': 847, 'sent': -0.11386299878358841, 'mass': 0.6134979724884033, 'conf': 0.4853470423161222}]}\n",
            "SELL DEBUG: {'seen': 536, 'sell_actions': 93}\n",
            "EXIT REASONS: {'sell_agent': 12, 'time': 48}\n",
            "\n",
            "=== TRADE MANAGER (TEST) ===\n",
            "n_steps: 1860\n",
            "segment_len: 372\n",
            "n_trades: 60\n",
            "final_equity: 2.964662596172741\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.022941439017150938\n",
            "win rate: 0.6333333333333333\n",
            "min/median/max net: -0.22780488291434964 0.019898337218204554 0.33333847188677823\n",
            "median hold bars: 20.0\n",
            "top 5 net: [0.17135644 0.18209916 0.24502303 0.30975334 0.33333847]\n",
            "bottom 5 net: [-0.22780488 -0.11254164 -0.11149548 -0.10685014 -0.09749147]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 46, 'entry_price': 168.69100952148438, 'exit_price': 152.7483673095703, 'gross_return': -0.09450795425990692, 'net_return': -0.09631803285934126, 'hold_bars': 17, 'forced_exit': False, 'meta': {'buy_conf': 0.49320325400894033, 'reason': 'sell_agent', 'sell_conf': 0.7502216248541628, 'sell_q0': -0.0015152357518672943, 'sell_q1': 0.020480656996369362, 'sell_margin': 0.021995892748236656, 'sell_delta_vs_hold': 0.0048292581206501595, 'sell_baseline_net': -0.10114729097999142, 'sell_net_now': -0.09631803285934126}}\n",
            "{'entry_idx': 78, 'exit_idx': 92, 'entry_price': 153.17132568359375, 'exit_price': 148.47999572753906, 'gross_return': -0.03062799081432216, 'net_return': -0.03256576546068424, 'hold_bars': 14, 'forced_exit': False, 'meta': {'buy_conf': 0.4853470423161222, 'reason': 'sell_agent', 'sell_conf': 0.8135203032376596, 'sell_q0': 0.011175725609064102, 'sell_q1': 0.04063669592142105, 'sell_margin': 0.02946097031235695, 'sell_delta_vs_hold': 0.041721784151318775, 'sell_baseline_net': -0.07428754961200301, 'sell_net_now': -0.03256576546068424}}\n",
            "{'entry_idx': 97, 'exit_idx': 117, 'entry_price': 145.90878295898438, 'exit_price': 129.90028381347656, 'gross_return': -0.10971580202959931, 'net_return': -0.11149548014134214, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4185868257992568, 'reason': 'time'}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 1773, 'exit_idx': 1793, 'entry_price': 130.02755737304688, 'exit_price': 127.6058578491211, 'gross_return': -0.01862450985661407, 'net_return': -0.02058627946141056, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5112639305770627, 'reason': 'time'}}\n",
            "{'entry_idx': 1804, 'exit_idx': 1824, 'entry_price': 139.4960174560547, 'exit_price': 130.851318359375, 'gross_return': -0.0619709383416839, 'net_return': -0.06384605843593882, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4951475892049439, 'reason': 'time'}}\n",
            "{'entry_idx': 1832, 'exit_idx': 1852, 'entry_price': 135.22824096679688, 'exit_price': 135.62522888183594, 'gross_return': 0.002935687931757787, 'net_return': 0.0009308194915820245, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.48688253826930367, 'reason': 'time'}}\n",
            "SELL Î” TRAIN: {'count': 16, 'mean_delta': 0.033161282539367676, 'median_delta': 0.033132173120975494, 'win_rate': 1.0}\n",
            "SELL Î” TEST : {'count': 12, 'mean_delta': 0.03792108967900276, 'median_delta': 0.02243000641465187, 'win_rate': 1.0}\n",
            "\n",
            "=== SAVED ===\n",
            " - runs/entry_indices_train_sell.npy (HARVESTED)\n",
            " - runs/entry_indices_test_sell.npy (HARVESTED)\n",
            " - runs/trades_buy_only_train.json\n",
            " - runs/trades_buy_only_test.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from trade.trade_manager import TradeManager\n",
        "\n",
        "# -----------------------\n",
        "# SETTINGS\n",
        "# -----------------------\n",
        "SEG_LEN = 1239          # rows per ticker segment from your build_features validation\n",
        "N_SEGS  = 5             # AAPL, MSFT, NVDA, AMZN, GOOGL\n",
        "TRAIN_FRAC = 0.70       # time-based split within each segment\n",
        "\n",
        "# NEW: entry harvesting (for SellAgent training)\n",
        "TOPK_PER_SEG_TRAIN = 80     # try 50â€“150\n",
        "TOPK_PER_SEG_TEST  = 40     # fewer is fine for eval\n",
        "MIN_GAP_TRAIN = None        # None => defaults inside TradeManager\n",
        "MIN_GAP_TEST  = None\n",
        "USE_CONF_SCORE = False      # False => uses q1-q0 margin (recommended)\n",
        "\n",
        "# -----------------------\n",
        "# BUILD TRAIN/TEST INDEX (per segment, no leakage)\n",
        "# -----------------------\n",
        "train_len = int(SEG_LEN * TRAIN_FRAC)\n",
        "\n",
        "train_idx = []\n",
        "test_idx = []\n",
        "\n",
        "for seg in range(N_SEGS):\n",
        "    start = seg * SEG_LEN\n",
        "    train_idx.extend(range(start, start + train_len))\n",
        "    test_idx.extend(range(start + train_len, start + SEG_LEN))\n",
        "\n",
        "train_idx = np.array(train_idx, dtype=np.int32)\n",
        "test_idx  = np.array(test_idx, dtype=np.int32)\n",
        "\n",
        "X_train = features[train_idx]\n",
        "p_train = prices[train_idx]\n",
        "X_test  = features[test_idx]\n",
        "p_test  = prices[test_idx]\n",
        "\n",
        "# Segment length inside each split subset (since we concatenated segments in order)\n",
        "SEG_TRAIN = train_len\n",
        "SEG_TEST  = SEG_LEN - train_len\n",
        "\n",
        "print(\"=== DATA SPLIT ===\")\n",
        "print(\"features:\", features.shape, \"prices:\", prices.shape)\n",
        "print(\"SEG_LEN:\", SEG_LEN, \"N_SEGS:\", N_SEGS, \"TRAIN_FRAC:\", TRAIN_FRAC)\n",
        "print(\"train_len per seg:\", SEG_TRAIN, \"test_len per seg:\", SEG_TEST)\n",
        "print(\"X_train:\", X_train.shape, \"p_train:\", p_train.shape)\n",
        "print(\"X_test :\", X_test.shape,  \"p_test :\", p_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# HELPER: run TM + debug logs (unchanged backtest)\n",
        "# -----------------------\n",
        "def run_tm(name: str, X: np.ndarray, p: np.ndarray, seg_len: int, sell_agent=None):\n",
        "    tm = TradeManager(\n",
        "        buy_agent=agent,            # trained buy agent\n",
        "        sell_agent=sell_agent,      # optional\n",
        "        state=X,\n",
        "        prices=p,\n",
        "        reward=cfg.reward,\n",
        "        trade=cfg.trade_manager,\n",
        "        segment_len=seg_len,        # IMPORTANT for boundary correctness\n",
        "    )\n",
        "\n",
        "    res = tm.run()\n",
        "    trades = res[\"trades\"]\n",
        "\n",
        "    print(\"Sell seen:\", tm._sell_debug[\"seen\"], \"Sell actions:\", tm._sell_debug[\"sell_actions\"])\n",
        "\n",
        "    from collections import Counter\n",
        "    reasons = Counter([t[\"meta\"].get(\"reason\", \"none\") for t in res[\"trades\"]])\n",
        "    print(\"Exit reasons:\", dict(reasons))\n",
        "    print(\"Non-time exits:\", [t for t in res[\"trades\"] if t[\"meta\"].get(\"reason\") != \"time\"][:3])\n",
        "\n",
        "    print(\"ENTRY DEBUG:\", res[\"entry_debug\"])\n",
        "    print(\"SELL DEBUG:\", res[\"sell_debug\"])\n",
        "    print(\"EXIT REASONS:\", res.get(\"exit_reasons\"))\n",
        "\n",
        "\n",
        "    print(f\"\\n=== TRADE MANAGER ({name}) ===\")\n",
        "    print(\"n_steps:\", len(p))\n",
        "    print(\"segment_len:\", seg_len)\n",
        "    print(\"n_trades:\", res[\"n_trades\"])\n",
        "    print(\"final_equity:\", res[\"final_equity\"])\n",
        "\n",
        "    # Boundary-crossing check (must be 0)\n",
        "    if trades:\n",
        "        cross = sum((t[\"entry_idx\"] // seg_len) != (t[\"exit_idx\"] // seg_len) for t in trades)\n",
        "    else:\n",
        "        cross = 0\n",
        "    print(\"Trades crossing segment boundary:\", cross)\n",
        "\n",
        "    # Return stats\n",
        "    if trades:\n",
        "        net = np.array([t[\"net_return\"] for t in trades], dtype=float)\n",
        "        hold = np.array([t[\"hold_bars\"] for t in trades], dtype=float)\n",
        "\n",
        "        print(\"avg net return:\", float(net.mean()))\n",
        "        print(\"win rate:\", float((net > 0).mean()))\n",
        "        print(\"min/median/max net:\", float(net.min()), float(np.median(net)), float(net.max()))\n",
        "        print(\"median hold bars:\", float(np.median(hold)))\n",
        "        print(\"top 5 net:\", np.sort(net)[-5:])\n",
        "        print(\"bottom 5 net:\", np.sort(net)[:5])\n",
        "\n",
        "        # A few sample trades (head + tail)\n",
        "        print(\"\\nSample trades (first 3):\")\n",
        "        for t in trades[:3]:\n",
        "            print(t)\n",
        "        print(\"\\nSample trades (last 3):\")\n",
        "        for t in trades[-3:]:\n",
        "            print(t)\n",
        "    else:\n",
        "        print(\"No trades produced. Try lowering buy_min_confidence or disabling trend filter.\")\n",
        "\n",
        "    return tm, res\n",
        "\n",
        "# -----------------------\n",
        "# NEW: Harvest entry indices for SellAgent training (no trade execution)\n",
        "# -----------------------\n",
        "# def harvest_entries(name: str, tm: TradeManager, topk_per_seg: int, min_gap=None, use_confidence_score=False):\n",
        "#     entries = tm.collect_entry_indices_topk(\n",
        "#         topk_per_segment=topk_per_seg,\n",
        "#         min_gap=min_gap,\n",
        "#         use_confidence_score=use_confidence_score,\n",
        "#     )\n",
        "#     entries = np.array(entries, dtype=np.int32)\n",
        "\n",
        "#     # Quick sanity: segment boundary + horizon feasibility check (should hold by construction)\n",
        "#     horizon = int(cfg.trade_manager.sell_horizon)\n",
        "#     if len(entries) > 0:\n",
        "#         seg_ok = np.all((entries % tm.segment_len) <= (tm.segment_len - 1 - horizon))\n",
        "#     else:\n",
        "#         seg_ok = True\n",
        "\n",
        "#     print(f\"\\n=== ENTRY HARVEST ({name}) ===\")\n",
        "#     print(\"topk_per_segment:\", topk_per_seg, \"min_gap:\", min_gap, \"use_conf_score:\", use_confidence_score)\n",
        "#     print(\"n_entries:\", len(entries))\n",
        "#     print(\"horizon:\", horizon, \"segment_len:\", tm.segment_len, \"feasible_in_segment:\", bool(seg_ok))\n",
        "#     if len(entries) > 0:\n",
        "#         print(\"first 10:\", entries[:10].tolist())\n",
        "#         print(\"last 10 :\", entries[-10:].tolist())\n",
        "\n",
        "#     return entries\n",
        "\n",
        "# -----------------------\n",
        "# RUN TRAIN + TEST (backtest as before)\n",
        "# -----------------------\n",
        "tm_train_sell, res_train_sell = run_tm(\"TRAIN\", X_train, p_train, seg_len=SEG_TRAIN, sell_agent=sell_agent)\n",
        "tm_test_sell,  res_test_sell  = run_tm(\"TEST\",  X_test,  p_test,  seg_len=SEG_TEST,  sell_agent=sell_agent)\n",
        "\n",
        "stats_train = compute_mean_delta(\n",
        "    trades=res_train_sell[\"trades\"],\n",
        "    prices=p_train,\n",
        "    horizon=cfg.trade_manager.sell_horizon,\n",
        "    tc=cfg.reward.transaction_cost,\n",
        ")\n",
        "\n",
        "stats_test = compute_mean_delta(\n",
        "    trades=res_test_sell[\"trades\"],\n",
        "    prices=p_test,\n",
        "    horizon=cfg.trade_manager.sell_horizon,\n",
        "    tc=cfg.reward.transaction_cost,\n",
        ")\n",
        "\n",
        "print(\"SELL Î” TRAIN:\", stats_train)\n",
        "print(\"SELL Î” TEST :\", stats_test)\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# HARVEST ENTRIES (NEW LOGIC) â€” use these for SellEnv training\n",
        "# -----------------------\n",
        "# train_entries_sell = harvest_entries(\n",
        "#     \"TRAIN\",\n",
        "#     tm_train_sell,\n",
        "#     topk_per_seg=TOPK_PER_SEG_TRAIN,\n",
        "#     min_gap=MIN_GAP_TRAIN,\n",
        "#     use_confidence_score=USE_CONF_SCORE,\n",
        "# )\n",
        "\n",
        "# test_entries_sell = harvest_entries(\n",
        "#     \"TEST\",\n",
        "#     tm_test_sell,\n",
        "#     topk_per_seg=TOPK_PER_SEG_TEST,\n",
        "#     min_gap=MIN_GAP_TEST,\n",
        "#     use_confidence_score=USE_CONF_SCORE,\n",
        "# )\n",
        "\n",
        "# -----------------------\n",
        "# SAVE ARTIFACTS (into out_dir)\n",
        "# -----------------------\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "train_entries_path = os.path.join(out_dir, \"entry_indices_train_sell.npy\")\n",
        "test_entries_path  = os.path.join(out_dir, \"entry_indices_test_sell.npy\")\n",
        "\n",
        "# NEW: save harvested entries (not trade entries)\n",
        "np.save(train_entries_path, train_entries)\n",
        "np.save(test_entries_path,  test_entries)\n",
        "\n",
        "train_trades_sell_json = os.path.join(out_dir, \"trades_buy_sell_train.json\")\n",
        "test_trades_sell_json  = os.path.join(out_dir, \"trades_buy_sell_test.json\")\n",
        "\n",
        "with open(train_trades_json, \"w\") as f:\n",
        "    json.dump(res_train[\"trades\"], f, indent=2)\n",
        "\n",
        "with open(test_trades_json, \"w\") as f:\n",
        "    json.dump(res_test[\"trades\"], f, indent=2)\n",
        "\n",
        "print(\"\\n=== SAVED ===\")\n",
        "print(\" -\", train_entries_path, \"(HARVESTED)\")\n",
        "print(\" -\", test_entries_path,  \"(HARVESTED)\")\n",
        "print(\" -\", train_trades_json)\n",
        "print(\" -\", test_trades_json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50bd130b",
      "metadata": {},
      "source": [
        "## How to \"pass args\" in a notebook\n",
        "\n",
        "Instead of CLI args, edit the **Parameters** cell.\n",
        "\n",
        "If you really want args-style overrides, you can do:\n",
        "\n",
        "```python\n",
        "import os\n",
        "config_path = os.getenv(\"CFG\", config_path)\n",
        "features_npy = os.getenv(\"FEAT\", features_npy)\n",
        "prices_npy = os.getenv(\"PRICES\", prices_npy)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263d95cb",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3.10.14",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
