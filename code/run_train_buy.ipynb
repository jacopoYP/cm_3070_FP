{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Buy DDQN training (notebook version)\n",
        "\n",
        "Converted from your `run_train_buy.py`-style script.\n",
        "\n",
        "1. Edit paths in **Parameters**.\n",
        "2. Run cells top-to-bottom.\n",
        "3. Model + diagnostics saved under `out_dir/<run_id>/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters (edit these)\n",
        "config_path = \"config.yaml\"                 # path to your YAML config\n",
        "features_npy = \"data/features.npy\"      # (n_steps, state_dim)\n",
        "prices_npy = \"data/prices.npy\"          # (n_steps,)\n",
        "out_dir = \"runs\"                            # output root folder\n",
        "run_id = None                               # set to a string to override, or leave None for timestamp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# If you run this notebook from outside your project root, you may need:\n",
        "# import sys\n",
        "# sys.path.append(\"/absolute/path/to/clean_trading_rl\")\n",
        "\n",
        "from core.config import load_config\n",
        "from agents.ddqn_agent import DDQNAgent\n",
        "from envs.buy_env import BuyEnv\n",
        "from diagnostics.q_gap import compute_q_gap, plot_q_gap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features: (6195, 10) prices: (6195,)\n",
            "state_dim: 10 n_actions: 2\n"
          ]
        }
      ],
      "source": [
        "# Load config + data\n",
        "cfg = load_config(config_path)\n",
        "\n",
        "features = np.load(features_npy)\n",
        "prices = np.load(prices_npy)\n",
        "\n",
        "cfg.agent.state_dim = int(features.shape[1])\n",
        "cfg.agent.n_actions = 2\n",
        "\n",
        "env = BuyEnv(features, prices, cfg.reward, cfg.trade_manager)\n",
        "agent = DDQNAgent(cfg.agent)\n",
        "\n",
        "print(\"features:\", features.shape, \"prices:\", prices.shape)\n",
        "print(\"state_dim:\", cfg.agent.state_dim, \"n_actions:\", cfg.agent.n_actions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_path: runs/20260114-152909\n"
          ]
        }
      ],
      "source": [
        "# Output folder\n",
        "if run_id is None:\n",
        "    run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "run_path = os.path.join(out_dir, run_id)\n",
        "os.makedirs(run_path, exist_ok=True)\n",
        "\n",
        "print(\"run_path:\", run_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ced7936b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.is_inference_mode_enabled(): False\n",
            "torch.is_grad_enabled(): True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"torch.is_inference_mode_enabled():\", torch.is_inference_mode_enabled())\n",
        "print(\"torch.is_grad_enabled():\", torch.is_grad_enabled())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BUY] ep=1/200 reward=0.7628 eps=0.976 total_steps=500 learn_steps=263 loss=0.0027448192704468966\n",
            "target sync @ 500 loss 0.0023499273229390383\n",
            "[BUY] ep=2/200 reward=0.6765 eps=0.953 total_steps=1000 learn_steps=763 loss=0.00035602442221716046\n",
            "target sync @ 1000 loss 0.00032906795968301594\n",
            "[BUY] ep=3/200 reward=0.9156 eps=0.929 total_steps=1500 learn_steps=1263 loss=0.00016380901797674596\n",
            "target sync @ 1500 loss 0.000389173801522702\n",
            "[BUY] ep=4/200 reward=1.0683 eps=0.905 total_steps=2000 learn_steps=1763 loss=0.0005383103853091598\n",
            "target sync @ 2000 loss 0.0001436112797819078\n",
            "[BUY] ep=5/200 reward=0.8210 eps=0.881 total_steps=2500 learn_steps=2263 loss=0.00032686020131222904\n",
            "target sync @ 2500 loss 0.0003605523088481277\n",
            "[BUY] ep=6/200 reward=1.2608 eps=0.858 total_steps=3000 learn_steps=2763 loss=0.00031028559897094965\n",
            "target sync @ 3000 loss 0.00012002426228718832\n",
            "[BUY] ep=7/200 reward=0.8094 eps=0.834 total_steps=3500 learn_steps=3263 loss=0.00011967222235398367\n",
            "target sync @ 3500 loss 0.00021786718571092933\n",
            "[BUY] ep=8/200 reward=0.8152 eps=0.810 total_steps=4000 learn_steps=3763 loss=0.0001118234868044965\n",
            "target sync @ 4000 loss 0.000289497256744653\n",
            "[BUY] ep=9/200 reward=1.0275 eps=0.786 total_steps=4500 learn_steps=4263 loss=9.292588947573677e-05\n",
            "target sync @ 4500 loss 0.00013639534881804138\n",
            "[BUY] ep=10/200 reward=0.7406 eps=0.762 total_steps=5000 learn_steps=4763 loss=0.0003549862012732774\n",
            "target sync @ 5000 loss 0.00013412926637101918\n",
            "[BUY] ep=11/200 reward=0.7703 eps=0.739 total_steps=5500 learn_steps=5263 loss=0.00018628142424859107\n",
            "target sync @ 5500 loss 0.00036185869248583913\n",
            "[BUY] ep=12/200 reward=1.0528 eps=0.715 total_steps=6000 learn_steps=5763 loss=5.5580523621756583e-05\n",
            "target sync @ 6000 loss 0.00016378058353438973\n",
            "[BUY] ep=13/200 reward=1.1360 eps=0.691 total_steps=6500 learn_steps=6263 loss=7.434214785462245e-05\n",
            "target sync @ 6500 loss 0.00026684903423301876\n",
            "[BUY] ep=14/200 reward=0.8883 eps=0.667 total_steps=7000 learn_steps=6763 loss=8.561391587136313e-05\n",
            "target sync @ 7000 loss 0.0003076904104091227\n",
            "[BUY] ep=15/200 reward=0.8715 eps=0.644 total_steps=7500 learn_steps=7263 loss=0.0006553158164024353\n",
            "target sync @ 7500 loss 0.00038276921259239316\n",
            "[BUY] ep=16/200 reward=0.6675 eps=0.620 total_steps=8000 learn_steps=7763 loss=0.00023409980349242687\n",
            "target sync @ 8000 loss 0.00026862777303904295\n",
            "[BUY] ep=17/200 reward=1.1560 eps=0.596 total_steps=8500 learn_steps=8263 loss=0.0001040157803799957\n",
            "target sync @ 8500 loss 0.0003013696114066988\n",
            "[BUY] ep=18/200 reward=1.0745 eps=0.573 total_steps=9000 learn_steps=8763 loss=0.00020334229338914156\n",
            "target sync @ 9000 loss 0.0006841247668489814\n",
            "[BUY] ep=19/200 reward=0.6113 eps=0.549 total_steps=9500 learn_steps=9263 loss=0.0002995719842147082\n",
            "target sync @ 9500 loss 0.00012342560512479395\n",
            "[BUY] ep=20/200 reward=0.7540 eps=0.525 total_steps=10000 learn_steps=9763 loss=0.00023032906756270677\n",
            "target sync @ 10000 loss 0.00031907905940897763\n",
            "[BUY] ep=21/200 reward=0.7542 eps=0.501 total_steps=10500 learn_steps=10263 loss=0.0004731002263724804\n",
            "target sync @ 10500 loss 8.807270205579698e-05\n",
            "[BUY] ep=22/200 reward=1.0935 eps=0.478 total_steps=11000 learn_steps=10763 loss=0.0005106214666739106\n",
            "target sync @ 11000 loss 0.00010497818584553897\n",
            "[BUY] ep=23/200 reward=0.9655 eps=0.454 total_steps=11500 learn_steps=11263 loss=0.00017690284585114568\n",
            "target sync @ 11500 loss 0.00013326099724508822\n",
            "[BUY] ep=24/200 reward=0.5729 eps=0.430 total_steps=12000 learn_steps=11763 loss=0.0001028047117870301\n",
            "target sync @ 12000 loss 5.941735435044393e-05\n",
            "[BUY] ep=25/200 reward=0.9153 eps=0.406 total_steps=12500 learn_steps=12263 loss=0.00010208415915258229\n",
            "target sync @ 12500 loss 0.0002372544549871236\n",
            "[BUY] ep=26/200 reward=1.0292 eps=0.383 total_steps=13000 learn_steps=12763 loss=0.00032433628803119063\n",
            "target sync @ 13000 loss 0.00010123186802957207\n",
            "[BUY] ep=27/200 reward=0.9271 eps=0.359 total_steps=13500 learn_steps=13263 loss=0.00012061734014423564\n",
            "target sync @ 13500 loss 0.00018824671860784292\n",
            "[BUY] ep=28/200 reward=0.9968 eps=0.335 total_steps=14000 learn_steps=13763 loss=0.0003304473066236824\n",
            "target sync @ 14000 loss 9.290515299653634e-05\n",
            "[BUY] ep=29/200 reward=0.7398 eps=0.311 total_steps=14500 learn_steps=14263 loss=0.00010506631224416196\n",
            "target sync @ 14500 loss 0.00020901497919112444\n",
            "[BUY] ep=30/200 reward=1.0898 eps=0.288 total_steps=15000 learn_steps=14763 loss=5.0623741117306054e-05\n",
            "target sync @ 15000 loss 0.00012260388757567853\n",
            "[BUY] ep=31/200 reward=0.8339 eps=0.264 total_steps=15500 learn_steps=15263 loss=5.8834779338212684e-05\n",
            "target sync @ 15500 loss 0.0001893279404612258\n",
            "[BUY] ep=32/200 reward=0.9535 eps=0.240 total_steps=16000 learn_steps=15763 loss=0.0003918494621757418\n",
            "target sync @ 16000 loss 0.00024667149409651756\n",
            "[BUY] ep=33/200 reward=0.5873 eps=0.216 total_steps=16500 learn_steps=16263 loss=7.48928141547367e-05\n",
            "target sync @ 16500 loss 0.00016399085870943964\n",
            "[BUY] ep=34/200 reward=0.8828 eps=0.193 total_steps=17000 learn_steps=16763 loss=0.00016025238437578082\n",
            "target sync @ 17000 loss 4.874299338553101e-05\n",
            "[BUY] ep=35/200 reward=0.9752 eps=0.169 total_steps=17500 learn_steps=17263 loss=2.0723608031403273e-05\n",
            "target sync @ 17500 loss 0.0001225220039486885\n",
            "[BUY] ep=36/200 reward=0.9585 eps=0.145 total_steps=18000 learn_steps=17763 loss=4.546970740193501e-05\n",
            "target sync @ 18000 loss 0.0002887689333874732\n",
            "[BUY] ep=37/200 reward=0.7135 eps=0.121 total_steps=18500 learn_steps=18263 loss=0.0004473754088394344\n",
            "target sync @ 18500 loss 7.317575364140794e-05\n",
            "[BUY] ep=38/200 reward=0.7233 eps=0.098 total_steps=19000 learn_steps=18763 loss=0.00022746303875464946\n",
            "target sync @ 19000 loss 5.0522179662948474e-05\n",
            "[BUY] ep=39/200 reward=0.7621 eps=0.074 total_steps=19500 learn_steps=19263 loss=0.0003040212905034423\n",
            "target sync @ 19500 loss 9.128293459070846e-05\n",
            "[BUY] ep=40/200 reward=0.4518 eps=0.050 total_steps=20000 learn_steps=19763 loss=7.692797225899994e-05\n",
            "target sync @ 20000 loss 0.00020921223040204495\n",
            "[BUY] ep=41/200 reward=0.9225 eps=0.050 total_steps=20500 learn_steps=20263 loss=0.0002594405086711049\n",
            "target sync @ 20500 loss 0.0002895682118833065\n",
            "[BUY] ep=42/200 reward=0.6506 eps=0.050 total_steps=21000 learn_steps=20763 loss=0.0001427643874194473\n",
            "target sync @ 21000 loss 7.877710595494136e-05\n",
            "[BUY] ep=43/200 reward=0.7396 eps=0.050 total_steps=21500 learn_steps=21263 loss=8.681395411258563e-05\n",
            "target sync @ 21500 loss 0.00023031675664242357\n",
            "[BUY] ep=44/200 reward=0.7319 eps=0.050 total_steps=22000 learn_steps=21763 loss=3.832738002529368e-05\n",
            "target sync @ 22000 loss 7.705191092099994e-05\n",
            "[BUY] ep=45/200 reward=1.1119 eps=0.050 total_steps=22500 learn_steps=22263 loss=0.0002569315256550908\n",
            "target sync @ 22500 loss 0.0003844703605864197\n",
            "[BUY] ep=46/200 reward=0.7661 eps=0.050 total_steps=23000 learn_steps=22763 loss=5.161469380254857e-05\n",
            "target sync @ 23000 loss 0.0005262517952360213\n",
            "[BUY] ep=47/200 reward=0.4397 eps=0.050 total_steps=23500 learn_steps=23263 loss=0.0009989681420847774\n",
            "target sync @ 23500 loss 0.00013738921552430838\n",
            "[BUY] ep=48/200 reward=0.8626 eps=0.050 total_steps=24000 learn_steps=23763 loss=0.00030922130099497736\n",
            "target sync @ 24000 loss 0.0003202894586138427\n",
            "[BUY] ep=49/200 reward=0.7085 eps=0.050 total_steps=24500 learn_steps=24263 loss=0.00010331088560633361\n",
            "target sync @ 24500 loss 0.0001564250560477376\n",
            "[BUY] ep=50/200 reward=0.4969 eps=0.050 total_steps=25000 learn_steps=24763 loss=4.839914618059993e-05\n",
            "target sync @ 25000 loss 0.0008950969204306602\n",
            "[BUY] ep=51/200 reward=1.1137 eps=0.050 total_steps=25500 learn_steps=25263 loss=0.00014855159679427743\n",
            "target sync @ 25500 loss 0.00019822505419142544\n",
            "[BUY] ep=52/200 reward=0.6280 eps=0.050 total_steps=26000 learn_steps=25763 loss=0.0002324309025425464\n",
            "target sync @ 26000 loss 0.0008427486172877252\n",
            "[BUY] ep=53/200 reward=1.0495 eps=0.050 total_steps=26500 learn_steps=26263 loss=0.00011754172737710178\n",
            "target sync @ 26500 loss 8.829086436890066e-05\n",
            "[BUY] ep=54/200 reward=1.1592 eps=0.050 total_steps=27000 learn_steps=26763 loss=8.420724770985544e-05\n",
            "target sync @ 27000 loss 0.0002881964319385588\n",
            "[BUY] ep=55/200 reward=0.6962 eps=0.050 total_steps=27500 learn_steps=27263 loss=0.0009183055954053998\n",
            "target sync @ 27500 loss 0.00026357159367762506\n",
            "[BUY] ep=56/200 reward=0.7118 eps=0.050 total_steps=28000 learn_steps=27763 loss=4.31907938036602e-05\n",
            "target sync @ 28000 loss 7.414454012177885e-05\n",
            "[BUY] ep=57/200 reward=0.9089 eps=0.050 total_steps=28500 learn_steps=28263 loss=0.0002697797317523509\n",
            "target sync @ 28500 loss 0.00043729020399041474\n",
            "[BUY] ep=58/200 reward=0.4804 eps=0.050 total_steps=29000 learn_steps=28763 loss=3.0062534278840758e-05\n",
            "target sync @ 29000 loss 0.00017177559493575245\n",
            "[BUY] ep=59/200 reward=0.6090 eps=0.050 total_steps=29500 learn_steps=29263 loss=3.0979004804976285e-05\n",
            "target sync @ 29500 loss 0.00016567630518693477\n",
            "[BUY] ep=60/200 reward=0.4901 eps=0.050 total_steps=30000 learn_steps=29763 loss=8.707550296094269e-05\n",
            "target sync @ 30000 loss 0.00042110023787245154\n",
            "[BUY] ep=61/200 reward=0.9434 eps=0.050 total_steps=30500 learn_steps=30263 loss=8.610249642515555e-05\n",
            "target sync @ 30500 loss 0.00020392221631482244\n",
            "[BUY] ep=62/200 reward=0.7762 eps=0.050 total_steps=31000 learn_steps=30763 loss=0.0002788907731883228\n",
            "target sync @ 31000 loss 0.00016196261276490986\n",
            "[BUY] ep=63/200 reward=0.9763 eps=0.050 total_steps=31500 learn_steps=31263 loss=0.00010236090020043775\n",
            "target sync @ 31500 loss 0.00030628577223978937\n",
            "[BUY] ep=64/200 reward=0.6087 eps=0.050 total_steps=32000 learn_steps=31763 loss=0.0001300834264839068\n",
            "target sync @ 32000 loss 0.000339697755407542\n",
            "[BUY] ep=65/200 reward=0.6697 eps=0.050 total_steps=32500 learn_steps=32263 loss=6.230077269719914e-05\n",
            "target sync @ 32500 loss 0.00020775283337570727\n",
            "[BUY] ep=66/200 reward=0.8732 eps=0.050 total_steps=33000 learn_steps=32763 loss=2.1204725271672942e-05\n",
            "target sync @ 33000 loss 6.775549991289154e-05\n",
            "[BUY] ep=67/200 reward=0.4155 eps=0.050 total_steps=33500 learn_steps=33263 loss=0.00015258233179338276\n",
            "target sync @ 33500 loss 0.000184019620064646\n",
            "[BUY] ep=68/200 reward=0.8998 eps=0.050 total_steps=34000 learn_steps=33763 loss=7.098916103132069e-05\n",
            "target sync @ 34000 loss 0.00023367541143670678\n",
            "[BUY] ep=69/200 reward=0.6353 eps=0.050 total_steps=34500 learn_steps=34263 loss=2.7136800781590864e-05\n",
            "target sync @ 34500 loss 0.0003438741259742528\n",
            "[BUY] ep=70/200 reward=0.5260 eps=0.050 total_steps=35000 learn_steps=34763 loss=3.383153671165928e-05\n",
            "target sync @ 35000 loss 0.00010932335862889886\n",
            "[BUY] ep=71/200 reward=0.5849 eps=0.050 total_steps=35500 learn_steps=35263 loss=2.0530382244032808e-05\n",
            "target sync @ 35500 loss 0.00046414232929237187\n",
            "[BUY] ep=72/200 reward=0.6743 eps=0.050 total_steps=36000 learn_steps=35763 loss=0.0001311160158365965\n",
            "target sync @ 36000 loss 0.0001357432483928278\n",
            "[BUY] ep=73/200 reward=0.8041 eps=0.050 total_steps=36500 learn_steps=36263 loss=0.0002501810959074646\n",
            "target sync @ 36500 loss 0.00029683986213058233\n",
            "[BUY] ep=74/200 reward=0.9012 eps=0.050 total_steps=37000 learn_steps=36763 loss=0.00024271855363622308\n",
            "target sync @ 37000 loss 7.85672600613907e-05\n",
            "[BUY] ep=75/200 reward=0.6789 eps=0.050 total_steps=37500 learn_steps=37263 loss=0.0008918779785744846\n",
            "target sync @ 37500 loss 1.3926673091191333e-05\n",
            "[BUY] ep=76/200 reward=0.9805 eps=0.050 total_steps=38000 learn_steps=37763 loss=0.0013486697571352124\n",
            "target sync @ 38000 loss 2.9382101274677552e-05\n",
            "[BUY] ep=77/200 reward=0.6268 eps=0.050 total_steps=38500 learn_steps=38263 loss=0.00026438641361892223\n",
            "target sync @ 38500 loss 0.00033197077573277056\n",
            "[BUY] ep=78/200 reward=0.7006 eps=0.050 total_steps=39000 learn_steps=38763 loss=0.00031419427250511944\n",
            "target sync @ 39000 loss 0.00010667172318790108\n",
            "[BUY] ep=79/200 reward=0.6297 eps=0.050 total_steps=39500 learn_steps=39263 loss=0.0002407114952802658\n",
            "target sync @ 39500 loss 0.00026833973242901266\n",
            "[BUY] ep=80/200 reward=0.5078 eps=0.050 total_steps=40000 learn_steps=39763 loss=5.169719224795699e-05\n",
            "target sync @ 40000 loss 8.964511653175578e-05\n",
            "[BUY] ep=81/200 reward=1.0713 eps=0.050 total_steps=40500 learn_steps=40263 loss=0.00014773683506064117\n",
            "target sync @ 40500 loss 3.1838746508583426e-05\n",
            "[BUY] ep=82/200 reward=0.6959 eps=0.050 total_steps=41000 learn_steps=40763 loss=5.3852512792218477e-05\n",
            "target sync @ 41000 loss 0.0001886159006971866\n",
            "[BUY] ep=83/200 reward=0.7090 eps=0.050 total_steps=41500 learn_steps=41263 loss=7.597531657665968e-05\n",
            "target sync @ 41500 loss 8.093438373180106e-05\n",
            "[BUY] ep=84/200 reward=0.7285 eps=0.050 total_steps=42000 learn_steps=41763 loss=0.0001503224193584174\n",
            "target sync @ 42000 loss 0.0013493086444213986\n",
            "[BUY] ep=85/200 reward=0.6044 eps=0.050 total_steps=42500 learn_steps=42263 loss=0.0001663023722358048\n",
            "target sync @ 42500 loss 2.5075200028368272e-05\n",
            "[BUY] ep=86/200 reward=0.4687 eps=0.050 total_steps=43000 learn_steps=42763 loss=0.00014624159666709602\n",
            "target sync @ 43000 loss 6.982168997637928e-05\n",
            "[BUY] ep=87/200 reward=1.1262 eps=0.050 total_steps=43500 learn_steps=43263 loss=2.1145118807908148e-05\n",
            "target sync @ 43500 loss 0.00012875429820269346\n",
            "[BUY] ep=88/200 reward=0.9697 eps=0.050 total_steps=44000 learn_steps=43763 loss=6.553724233526736e-05\n",
            "target sync @ 44000 loss 0.00036539504071697593\n",
            "[BUY] ep=89/200 reward=0.8363 eps=0.050 total_steps=44500 learn_steps=44263 loss=4.0335278754355386e-05\n",
            "target sync @ 44500 loss 0.0004087681882083416\n",
            "[BUY] ep=90/200 reward=0.9604 eps=0.050 total_steps=45000 learn_steps=44763 loss=0.0006896961713209748\n",
            "target sync @ 45000 loss 0.00014369108248502016\n",
            "[BUY] ep=91/200 reward=0.7911 eps=0.050 total_steps=45500 learn_steps=45263 loss=1.6505669918842614e-05\n",
            "target sync @ 45500 loss 4.2809835576917976e-05\n",
            "[BUY] ep=92/200 reward=0.6042 eps=0.050 total_steps=46000 learn_steps=45763 loss=8.645144407637417e-05\n",
            "target sync @ 46000 loss 0.00014582925359718502\n",
            "[BUY] ep=93/200 reward=0.7674 eps=0.050 total_steps=46500 learn_steps=46263 loss=6.024915637681261e-05\n",
            "target sync @ 46500 loss 2.7226944439462386e-05\n",
            "[BUY] ep=94/200 reward=0.6679 eps=0.050 total_steps=47000 learn_steps=46763 loss=0.00013130158185958862\n",
            "target sync @ 47000 loss 0.00015943081234581769\n",
            "[BUY] ep=95/200 reward=0.8318 eps=0.050 total_steps=47500 learn_steps=47263 loss=0.00036876180092804134\n",
            "target sync @ 47500 loss 0.0011895967181771994\n",
            "[BUY] ep=96/200 reward=1.0152 eps=0.050 total_steps=48000 learn_steps=47763 loss=0.0003285353013779968\n",
            "target sync @ 48000 loss 0.00010681283310987055\n",
            "[BUY] ep=97/200 reward=0.3352 eps=0.050 total_steps=48500 learn_steps=48263 loss=0.00012524178600870073\n",
            "target sync @ 48500 loss 0.00013402686454355717\n",
            "[BUY] ep=98/200 reward=0.4976 eps=0.050 total_steps=49000 learn_steps=48763 loss=0.00038127167499624193\n",
            "target sync @ 49000 loss 0.00037854822585359216\n",
            "[BUY] ep=99/200 reward=0.6255 eps=0.050 total_steps=49500 learn_steps=49263 loss=0.00019098006305284798\n",
            "target sync @ 49500 loss 3.457525963312946e-05\n",
            "[BUY] ep=100/200 reward=0.7475 eps=0.050 total_steps=50000 learn_steps=49763 loss=4.312154487706721e-05\n",
            "target sync @ 50000 loss 0.0006479615694843233\n",
            "[BUY] ep=101/200 reward=0.6124 eps=0.050 total_steps=50500 learn_steps=50263 loss=0.00016357278218492866\n",
            "target sync @ 50500 loss 0.0004419205361045897\n",
            "[BUY] ep=102/200 reward=0.7583 eps=0.050 total_steps=51000 learn_steps=50763 loss=0.00015619721671100706\n",
            "target sync @ 51000 loss 0.0001235517265740782\n",
            "[BUY] ep=103/200 reward=0.4836 eps=0.050 total_steps=51500 learn_steps=51263 loss=0.0001805650390451774\n",
            "target sync @ 51500 loss 0.0004722537414636463\n",
            "[BUY] ep=104/200 reward=0.5735 eps=0.050 total_steps=52000 learn_steps=51763 loss=0.0003087257209699601\n",
            "target sync @ 52000 loss 2.21255213546101e-05\n",
            "[BUY] ep=105/200 reward=0.1224 eps=0.050 total_steps=52500 learn_steps=52263 loss=0.0002087165485136211\n",
            "target sync @ 52500 loss 0.00017096582450903952\n",
            "[BUY] ep=106/200 reward=0.9116 eps=0.050 total_steps=53000 learn_steps=52763 loss=0.00013808367657475173\n",
            "target sync @ 53000 loss 3.130530967609957e-05\n",
            "[BUY] ep=107/200 reward=0.7734 eps=0.050 total_steps=53500 learn_steps=53263 loss=0.00023905855778139085\n",
            "target sync @ 53500 loss 2.3105336367734708e-05\n",
            "[BUY] ep=108/200 reward=0.6782 eps=0.050 total_steps=54000 learn_steps=53763 loss=0.0003510960377752781\n",
            "target sync @ 54000 loss 1.5739075024612248e-05\n",
            "[BUY] ep=109/200 reward=0.8661 eps=0.050 total_steps=54500 learn_steps=54263 loss=3.5821354686049744e-05\n",
            "target sync @ 54500 loss 0.00023586188035551459\n",
            "[BUY] ep=110/200 reward=0.5261 eps=0.050 total_steps=55000 learn_steps=54763 loss=0.0003479995939414948\n",
            "target sync @ 55000 loss 4.791365427081473e-05\n",
            "[BUY] ep=111/200 reward=0.4975 eps=0.050 total_steps=55500 learn_steps=55263 loss=0.00014793641457799822\n",
            "target sync @ 55500 loss 0.00017696633585728705\n",
            "[BUY] ep=112/200 reward=0.9935 eps=0.050 total_steps=56000 learn_steps=55763 loss=0.00010034387378254905\n",
            "target sync @ 56000 loss 9.560747275827453e-05\n",
            "[BUY] ep=113/200 reward=0.6725 eps=0.050 total_steps=56500 learn_steps=56263 loss=7.628292951267213e-05\n",
            "target sync @ 56500 loss 0.00025450572138652205\n",
            "[BUY] ep=114/200 reward=0.9773 eps=0.050 total_steps=57000 learn_steps=56763 loss=1.039895687426906e-05\n",
            "target sync @ 57000 loss 2.09909412660636e-05\n",
            "[BUY] ep=115/200 reward=0.6037 eps=0.050 total_steps=57500 learn_steps=57263 loss=2.2364196411217563e-05\n",
            "target sync @ 57500 loss 0.0003538409364409745\n",
            "[BUY] ep=116/200 reward=1.0123 eps=0.050 total_steps=58000 learn_steps=57763 loss=0.00023172033252194524\n",
            "target sync @ 58000 loss 0.00013389252126216888\n",
            "[BUY] ep=117/200 reward=0.5017 eps=0.050 total_steps=58500 learn_steps=58263 loss=0.0001738950377330184\n",
            "target sync @ 58500 loss 0.00017870769079308957\n",
            "[BUY] ep=118/200 reward=0.9987 eps=0.050 total_steps=59000 learn_steps=58763 loss=0.0003200021747034043\n",
            "target sync @ 59000 loss 0.00020544667495414615\n",
            "[BUY] ep=119/200 reward=0.7465 eps=0.050 total_steps=59500 learn_steps=59263 loss=0.00010902535723289475\n",
            "target sync @ 59500 loss 2.915373261203058e-05\n",
            "[BUY] ep=120/200 reward=0.7506 eps=0.050 total_steps=60000 learn_steps=59763 loss=0.00020371472055558115\n",
            "target sync @ 60000 loss 0.0001215469601447694\n",
            "[BUY] ep=121/200 reward=0.6830 eps=0.050 total_steps=60500 learn_steps=60263 loss=0.0011877798242494464\n",
            "target sync @ 60500 loss 0.00012102147593395784\n",
            "[BUY] ep=122/200 reward=0.9125 eps=0.050 total_steps=61000 learn_steps=60763 loss=5.197071732254699e-05\n",
            "target sync @ 61000 loss 0.00031744487932883203\n",
            "[BUY] ep=123/200 reward=1.1971 eps=0.050 total_steps=61500 learn_steps=61263 loss=4.83326475659851e-05\n",
            "target sync @ 61500 loss 0.00019566570699680597\n",
            "[BUY] ep=124/200 reward=0.8292 eps=0.050 total_steps=62000 learn_steps=61763 loss=8.955858356785029e-05\n",
            "target sync @ 62000 loss 0.00012053825776092708\n",
            "[BUY] ep=125/200 reward=0.8262 eps=0.050 total_steps=62500 learn_steps=62263 loss=0.00015424347657244653\n",
            "target sync @ 62500 loss 0.0001272454101126641\n",
            "[BUY] ep=126/200 reward=0.9349 eps=0.050 total_steps=63000 learn_steps=62763 loss=0.0002145814651157707\n",
            "target sync @ 63000 loss 5.597339622909203e-05\n",
            "[BUY] ep=127/200 reward=0.7478 eps=0.050 total_steps=63500 learn_steps=63263 loss=0.00019718219118658453\n",
            "target sync @ 63500 loss 0.00025026226649060845\n",
            "[BUY] ep=128/200 reward=0.9728 eps=0.050 total_steps=64000 learn_steps=63763 loss=0.00015348145097959787\n",
            "target sync @ 64000 loss 0.00011625829938566312\n",
            "[BUY] ep=129/200 reward=1.0125 eps=0.050 total_steps=64500 learn_steps=64263 loss=0.00020555220544338226\n",
            "target sync @ 64500 loss 0.0001171208787127398\n",
            "[BUY] ep=130/200 reward=0.8960 eps=0.050 total_steps=65000 learn_steps=64763 loss=3.5703080357052386e-05\n",
            "target sync @ 65000 loss 5.0882426876341924e-05\n",
            "[BUY] ep=131/200 reward=0.8178 eps=0.050 total_steps=65500 learn_steps=65263 loss=5.846302883583121e-05\n",
            "target sync @ 65500 loss 7.284387538675219e-05\n",
            "[BUY] ep=132/200 reward=0.7080 eps=0.050 total_steps=66000 learn_steps=65763 loss=0.0009524704655632377\n",
            "target sync @ 66000 loss 0.00010057142935693264\n",
            "[BUY] ep=133/200 reward=0.8501 eps=0.050 total_steps=66500 learn_steps=66263 loss=0.00010681223648134619\n",
            "target sync @ 66500 loss 0.0004755629925057292\n",
            "[BUY] ep=134/200 reward=0.7662 eps=0.050 total_steps=67000 learn_steps=66763 loss=0.0001259483688045293\n",
            "target sync @ 67000 loss 6.322650733636692e-05\n",
            "[BUY] ep=135/200 reward=0.8860 eps=0.050 total_steps=67500 learn_steps=67263 loss=0.00014406013360712677\n",
            "target sync @ 67500 loss 0.0003542254853527993\n",
            "[BUY] ep=136/200 reward=0.9184 eps=0.050 total_steps=68000 learn_steps=67763 loss=6.577859312528744e-05\n",
            "target sync @ 68000 loss 6.492144166259095e-05\n",
            "[BUY] ep=137/200 reward=0.6072 eps=0.050 total_steps=68500 learn_steps=68263 loss=0.00024969244259409606\n",
            "target sync @ 68500 loss 0.00014008469588588923\n",
            "[BUY] ep=138/200 reward=0.5724 eps=0.050 total_steps=69000 learn_steps=68763 loss=0.0004419514734763652\n",
            "target sync @ 69000 loss 0.00019185361452400684\n",
            "[BUY] ep=139/200 reward=1.3340 eps=0.050 total_steps=69500 learn_steps=69263 loss=0.00010401830513728783\n",
            "target sync @ 69500 loss 8.023896225495264e-05\n",
            "[BUY] ep=140/200 reward=0.6346 eps=0.050 total_steps=70000 learn_steps=69763 loss=0.00051803735550493\n",
            "target sync @ 70000 loss 8.052306657191366e-05\n",
            "[BUY] ep=141/200 reward=0.7017 eps=0.050 total_steps=70500 learn_steps=70263 loss=0.0003969126264564693\n",
            "target sync @ 70500 loss 0.0004216719535179436\n",
            "[BUY] ep=142/200 reward=0.7856 eps=0.050 total_steps=71000 learn_steps=70763 loss=2.8947393730049953e-05\n",
            "target sync @ 71000 loss 0.000125277423649095\n",
            "[BUY] ep=143/200 reward=1.0343 eps=0.050 total_steps=71500 learn_steps=71263 loss=0.0002540507703088224\n",
            "target sync @ 71500 loss 8.834026812110096e-05\n",
            "[BUY] ep=144/200 reward=0.3101 eps=0.050 total_steps=72000 learn_steps=71763 loss=0.0006406625616364181\n",
            "target sync @ 72000 loss 0.00021706237748730928\n",
            "[BUY] ep=145/200 reward=0.5246 eps=0.050 total_steps=72500 learn_steps=72263 loss=0.00017759494949132204\n",
            "target sync @ 72500 loss 0.00024277865304611623\n",
            "[BUY] ep=146/200 reward=0.8478 eps=0.050 total_steps=73000 learn_steps=72763 loss=0.0002810716978274286\n",
            "target sync @ 73000 loss 0.0002182158932555467\n",
            "[BUY] ep=147/200 reward=0.5579 eps=0.050 total_steps=73500 learn_steps=73263 loss=6.466144986916333e-05\n",
            "target sync @ 73500 loss 7.506644760724157e-05\n",
            "[BUY] ep=148/200 reward=0.9583 eps=0.050 total_steps=74000 learn_steps=73763 loss=7.706480391789228e-05\n",
            "target sync @ 74000 loss 0.000494611740577966\n",
            "[BUY] ep=149/200 reward=0.6976 eps=0.050 total_steps=74500 learn_steps=74263 loss=0.0003475471748970449\n",
            "target sync @ 74500 loss 0.0005780772189609706\n",
            "[BUY] ep=150/200 reward=0.9036 eps=0.050 total_steps=75000 learn_steps=74763 loss=2.4069504434010014e-05\n",
            "target sync @ 75000 loss 3.267697320552543e-05\n",
            "[BUY] ep=151/200 reward=0.2640 eps=0.050 total_steps=75500 learn_steps=75263 loss=0.00026706469361670315\n",
            "target sync @ 75500 loss 0.000258668907918036\n",
            "[BUY] ep=152/200 reward=0.6965 eps=0.050 total_steps=76000 learn_steps=75763 loss=0.0009720718371681869\n",
            "target sync @ 76000 loss 0.00023142409918364137\n",
            "[BUY] ep=153/200 reward=0.5757 eps=0.050 total_steps=76500 learn_steps=76263 loss=0.0003961572365369648\n",
            "target sync @ 76500 loss 0.00017719619791023433\n",
            "[BUY] ep=154/200 reward=1.1295 eps=0.050 total_steps=77000 learn_steps=76763 loss=0.00010887967800954357\n",
            "target sync @ 77000 loss 0.00015356685617007315\n",
            "[BUY] ep=155/200 reward=0.7281 eps=0.050 total_steps=77500 learn_steps=77263 loss=0.00020355591550469398\n",
            "target sync @ 77500 loss 0.00013933533045928925\n",
            "[BUY] ep=156/200 reward=1.0339 eps=0.050 total_steps=78000 learn_steps=77763 loss=0.00040897566941566765\n",
            "target sync @ 78000 loss 0.00012236321344971657\n",
            "[BUY] ep=157/200 reward=0.5521 eps=0.050 total_steps=78500 learn_steps=78263 loss=6.0758313338737935e-05\n",
            "target sync @ 78500 loss 4.2269733967259526e-05\n",
            "[BUY] ep=158/200 reward=0.7769 eps=0.050 total_steps=79000 learn_steps=78763 loss=0.00010155012569157407\n",
            "target sync @ 79000 loss 0.00015074133989401162\n",
            "[BUY] ep=159/200 reward=0.9986 eps=0.050 total_steps=79500 learn_steps=79263 loss=0.00015561142936348915\n",
            "target sync @ 79500 loss 0.00013001439219806343\n",
            "[BUY] ep=160/200 reward=0.6693 eps=0.050 total_steps=80000 learn_steps=79763 loss=0.0004504684475250542\n",
            "target sync @ 80000 loss 0.00013690892956219614\n",
            "[BUY] ep=161/200 reward=0.4836 eps=0.050 total_steps=80500 learn_steps=80263 loss=9.879169738269411e-06\n",
            "target sync @ 80500 loss 0.00013477099128067493\n",
            "[BUY] ep=162/200 reward=0.6887 eps=0.050 total_steps=81000 learn_steps=80763 loss=0.0006720368983224034\n",
            "target sync @ 81000 loss 0.0003489913360681385\n",
            "[BUY] ep=163/200 reward=0.5561 eps=0.050 total_steps=81500 learn_steps=81263 loss=0.0005232928670011461\n",
            "target sync @ 81500 loss 7.922136865090579e-05\n",
            "[BUY] ep=164/200 reward=0.5931 eps=0.050 total_steps=82000 learn_steps=81763 loss=0.00016333151143044233\n",
            "target sync @ 82000 loss 3.5268854844616726e-05\n",
            "[BUY] ep=165/200 reward=1.0888 eps=0.050 total_steps=82500 learn_steps=82263 loss=4.409610482980497e-05\n",
            "target sync @ 82500 loss 0.0003959287132602185\n",
            "[BUY] ep=166/200 reward=1.0196 eps=0.050 total_steps=83000 learn_steps=82763 loss=0.0001098053326131776\n",
            "target sync @ 83000 loss 0.0003374709340278059\n",
            "[BUY] ep=167/200 reward=0.8397 eps=0.050 total_steps=83500 learn_steps=83263 loss=4.041915599373169e-05\n",
            "target sync @ 83500 loss 3.020351505256258e-05\n",
            "[BUY] ep=168/200 reward=0.8480 eps=0.050 total_steps=84000 learn_steps=83763 loss=1.7493848645244725e-05\n",
            "target sync @ 84000 loss 0.00023383222287520766\n",
            "[BUY] ep=169/200 reward=0.7371 eps=0.050 total_steps=84500 learn_steps=84263 loss=7.953708700370044e-05\n",
            "target sync @ 84500 loss 0.0003770579642150551\n",
            "[BUY] ep=170/200 reward=0.3015 eps=0.050 total_steps=85000 learn_steps=84763 loss=0.00014707836089655757\n",
            "target sync @ 85000 loss 0.0002450819592922926\n",
            "[BUY] ep=171/200 reward=0.6807 eps=0.050 total_steps=85500 learn_steps=85263 loss=4.678550249082036e-05\n",
            "target sync @ 85500 loss 0.0005943294381722808\n",
            "[BUY] ep=172/200 reward=1.1006 eps=0.050 total_steps=86000 learn_steps=85763 loss=2.785406104521826e-05\n",
            "target sync @ 86000 loss 0.00030086535844020545\n",
            "[BUY] ep=173/200 reward=0.8711 eps=0.050 total_steps=86500 learn_steps=86263 loss=6.967103399802e-05\n",
            "target sync @ 86500 loss 9.648259037930984e-06\n",
            "[BUY] ep=174/200 reward=1.1342 eps=0.050 total_steps=87000 learn_steps=86763 loss=0.00011332629946991801\n",
            "target sync @ 87000 loss 0.000381841033231467\n",
            "[BUY] ep=175/200 reward=0.9556 eps=0.050 total_steps=87500 learn_steps=87263 loss=0.0002275128645123914\n",
            "target sync @ 87500 loss 7.333259418373927e-05\n",
            "[BUY] ep=176/200 reward=0.7120 eps=0.050 total_steps=88000 learn_steps=87763 loss=5.9438913012854755e-05\n",
            "target sync @ 88000 loss 1.630651058803778e-05\n",
            "[BUY] ep=177/200 reward=0.9288 eps=0.050 total_steps=88500 learn_steps=88263 loss=0.0010704376036301255\n",
            "target sync @ 88500 loss 0.0001645698066568002\n",
            "[BUY] ep=178/200 reward=0.6598 eps=0.050 total_steps=89000 learn_steps=88763 loss=0.0003538668679539114\n",
            "target sync @ 89000 loss 3.384671435924247e-05\n",
            "[BUY] ep=179/200 reward=0.3472 eps=0.050 total_steps=89500 learn_steps=89263 loss=1.5717347196186893e-05\n",
            "target sync @ 89500 loss 0.00034449572558514774\n",
            "[BUY] ep=180/200 reward=0.5540 eps=0.050 total_steps=90000 learn_steps=89763 loss=2.2439275198848918e-05\n",
            "target sync @ 90000 loss 4.0981871279655024e-05\n",
            "[BUY] ep=181/200 reward=0.5257 eps=0.050 total_steps=90500 learn_steps=90263 loss=0.00011131212522741407\n",
            "target sync @ 90500 loss 1.2821745258406736e-05\n",
            "[BUY] ep=182/200 reward=0.7655 eps=0.050 total_steps=91000 learn_steps=90763 loss=8.863829134497792e-05\n",
            "target sync @ 91000 loss 1.2350672477623448e-05\n",
            "[BUY] ep=183/200 reward=0.6673 eps=0.050 total_steps=91500 learn_steps=91263 loss=0.00013902214413974434\n",
            "target sync @ 91500 loss 6.261760427150875e-05\n",
            "[BUY] ep=184/200 reward=0.5044 eps=0.050 total_steps=92000 learn_steps=91763 loss=0.0002265586517751217\n",
            "target sync @ 92000 loss 4.278487176634371e-05\n",
            "[BUY] ep=185/200 reward=0.6656 eps=0.050 total_steps=92500 learn_steps=92263 loss=0.0009152504499070346\n",
            "target sync @ 92500 loss 3.544965511537157e-05\n",
            "[BUY] ep=186/200 reward=0.5133 eps=0.050 total_steps=93000 learn_steps=92763 loss=0.00013794208643957973\n",
            "target sync @ 93000 loss 1.2463608982216101e-05\n",
            "[BUY] ep=187/200 reward=0.7600 eps=0.050 total_steps=93500 learn_steps=93263 loss=1.4943661881261505e-05\n",
            "target sync @ 93500 loss 1.911831714096479e-05\n",
            "[BUY] ep=188/200 reward=0.3602 eps=0.050 total_steps=94000 learn_steps=93763 loss=4.037460166728124e-05\n",
            "target sync @ 94000 loss 0.0001493497402407229\n",
            "[BUY] ep=189/200 reward=0.6645 eps=0.050 total_steps=94500 learn_steps=94263 loss=0.00031230892636813223\n",
            "target sync @ 94500 loss 0.00010350065713282675\n",
            "[BUY] ep=190/200 reward=1.0151 eps=0.050 total_steps=95000 learn_steps=94763 loss=5.4068645113147795e-05\n",
            "target sync @ 95000 loss 0.0005712621496059\n",
            "[BUY] ep=191/200 reward=0.7537 eps=0.050 total_steps=95500 learn_steps=95263 loss=0.00020839856006205082\n",
            "target sync @ 95500 loss 0.00021165791258681566\n",
            "[BUY] ep=192/200 reward=0.5997 eps=0.050 total_steps=96000 learn_steps=95763 loss=2.0789502741536126e-05\n",
            "target sync @ 96000 loss 6.020741420798004e-05\n",
            "[BUY] ep=193/200 reward=0.9530 eps=0.050 total_steps=96500 learn_steps=96263 loss=0.0001770599774317816\n",
            "target sync @ 96500 loss 8.528778562322259e-05\n",
            "[BUY] ep=194/200 reward=0.5953 eps=0.050 total_steps=97000 learn_steps=96763 loss=0.0004651009221561253\n",
            "target sync @ 97000 loss 0.00024167643277905881\n",
            "[BUY] ep=195/200 reward=0.5131 eps=0.050 total_steps=97500 learn_steps=97263 loss=4.999929296900518e-05\n",
            "target sync @ 97500 loss 0.00036444584839046\n",
            "[BUY] ep=196/200 reward=0.8267 eps=0.050 total_steps=98000 learn_steps=97763 loss=1.7290754840360023e-05\n",
            "target sync @ 98000 loss 7.67369638197124e-05\n",
            "[BUY] ep=197/200 reward=0.6108 eps=0.050 total_steps=98500 learn_steps=98263 loss=0.0010640968102961779\n",
            "target sync @ 98500 loss 2.5164314138237387e-05\n",
            "[BUY] ep=198/200 reward=0.5358 eps=0.050 total_steps=99000 learn_steps=98763 loss=0.00013955411850474775\n",
            "target sync @ 99000 loss 0.00023635583056602627\n",
            "[BUY] ep=199/200 reward=0.9083 eps=0.050 total_steps=99500 learn_steps=99263 loss=1.9401324607315473e-05\n",
            "target sync @ 99500 loss 0.0002480388793628663\n",
            "[BUY] ep=200/200 reward=0.8673 eps=0.050 total_steps=100000 learn_steps=99763 loss=1.7238104192074388e-05\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for ep in range(int(cfg.training.episodes)):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    ep_reward = 0.0\n",
        "    steps = 0\n",
        "    loss = None\n",
        "\n",
        "    while not done:\n",
        "        a = agent.select_action(s, greedy=False)\n",
        "        ns, r, done, info = env.step(a)\n",
        "\n",
        "        agent.push(s, a, r, ns, done)\n",
        "\n",
        "        # âœ… warmup based on env steps (or buffer size), then train\n",
        "        if agent.total_steps >= int(cfg.training.warmup_steps):\n",
        "            loss = agent.update()\n",
        "\n",
        "        s = ns\n",
        "        ep_reward += float(r)\n",
        "        steps += 1\n",
        "\n",
        "        if cfg.training.steps_per_episode is not None and steps >= int(cfg.training.steps_per_episode):\n",
        "            break\n",
        "\n",
        "    if (ep + 1) % int(cfg.training.log_every) == 0:\n",
        "        last_loss = agent.loss_history[-1] if agent.loss_history else None\n",
        "        print(\n",
        "            f\"[BUY] ep={ep+1}/{cfg.training.episodes} \"\n",
        "            f\"reward={ep_reward:.4f} eps={agent.eps:.3f} \"\n",
        "            f\"total_steps={agent.total_steps} learn_steps={agent.learn_steps} \"\n",
        "            f\"loss={last_loss}\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model: runs/20260114-152909/buy_agent.pt\n",
            "Diagnostics: {'line': 'runs/20260114-152909/q_gap_buy.png', 'hist': 'runs/20260114-152909/q_gap_buy_hist.png'}\n"
          ]
        }
      ],
      "source": [
        "# Save model + diagnostics\n",
        "model_path = os.path.join(run_path, \"buy_agent.pt\")\n",
        "agent.save(model_path)\n",
        "\n",
        "gaps = compute_q_gap(agent, features, max_points=2000)\n",
        "paths = plot_q_gap(gaps, run_path, tag=\"buy\")\n",
        "\n",
        "print(\"Saved model:\", model_path)\n",
        "print(\"Diagnostics:\", paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87c7a72d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mean_delta(trades, prices, horizon, tc):\n",
        "    deltas = []\n",
        "\n",
        "    for tr in trades:\n",
        "        if tr[\"meta\"].get(\"reason\") != \"sell_agent\":\n",
        "            continue\n",
        "\n",
        "        entry = tr[\"entry_idx\"]\n",
        "        # exit_sell = tr[\"exit_idx\"]\n",
        "\n",
        "        horizon_exit = min(entry + horizon, len(prices) - 1)\n",
        "\n",
        "        entry_price = prices[entry]\n",
        "        horizon_price = prices[horizon_exit]\n",
        "\n",
        "        gross_horizon = (horizon_price - entry_price) / (entry_price + 1e-12)\n",
        "        net_horizon = ((1 - tc) ** 2) * (1 + gross_horizon) - 1\n",
        "\n",
        "        delta = tr[\"net_return\"] - net_horizon\n",
        "        deltas.append(delta)\n",
        "\n",
        "    return {\n",
        "        \"count\": len(deltas),\n",
        "        \"mean_delta\": float(np.mean(deltas)) if deltas else 0.0,\n",
        "        \"median_delta\": float(np.median(deltas)) if deltas else 0.0,\n",
        "        \"win_rate\": float(np.mean(np.array(deltas) > 0)) if deltas else 0.0,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f948f3",
      "metadata": {},
      "source": [
        "# TradeManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "63d0a92c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA SPLIT ===\n",
            "features: (6195, 10) prices: (6195,)\n",
            "SEG_LEN: 1239 N_SEGS: 5 TRAIN_FRAC: 0.7\n",
            "train_len per seg: 867 test_len per seg: 372\n",
            "X_train: (4335, 10) p_train: (4335,)\n",
            "X_test : (1860, 10) p_test : (1860,)\n",
            "Exit reasons: {'time': 141, 'segment_end': 1}\n",
            "Non-time exits: [{'entry_idx': 3447, 'exit_idx': 3467, 'entry_price': 121.18000030517578, 'exit_price': 115.54000091552734, 'gross_return': -0.046542328564489185, 'net_return': -0.04844829044968868, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5080829894395992, 'reason': 'segment_end'}}]\n",
            "ENTRY DEBUG: {'checked': 927, 'blocked_trend': 771, 'blocked_latest_entry': 11, 'blocked_conf': 3, 'opened': 142, 'conf_min': 0.2442312077388621, 'conf_max': 0.6760742955395784}\n",
            "SELL DEBUG: {'seen': 0, 'sell_actions': 0}\n",
            "EXIT REASONS: {'time': 141, 'segment_end': 1}\n",
            "\n",
            "=== TRADE MANAGER (TRAIN) ===\n",
            "n_steps: 4335\n",
            "segment_len: 867\n",
            "n_trades: 142\n",
            "final_equity: 21.46589428271698\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.025832535051075977\n",
            "win rate: 0.6267605633802817\n",
            "min/median/max net: -0.2827455067578124 0.029751960236301223 0.39307612553170723\n",
            "median hold bars: 20.0\n",
            "top 5 net: [0.18033615 0.22808477 0.24166434 0.29425715 0.39307613]\n",
            "bottom 5 net: [-0.28274551 -0.16360307 -0.14640532 -0.14082351 -0.12815472]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 49, 'entry_price': 43.37913513183594, 'exit_price': 47.89346694946289, 'gross_return': 0.10406689307905961, 'net_return': 0.10185986335979447, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5133428701243621, 'reason': 'time'}}\n",
            "{'entry_idx': 54, 'exit_idx': 74, 'entry_price': 48.49269104003906, 'exit_price': 45.552024841308594, 'gross_return': -0.06064143143349957, 'net_return': -0.062519209212064, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5191797409353899, 'reason': 'time'}}\n",
            "{'entry_idx': 95, 'exit_idx': 115, 'entry_price': 46.465084075927734, 'exit_price': 49.00773620605469, 'gross_return': 0.05472178046577937, 'net_return': 0.05261339162662826, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5125973828544741, 'reason': 'time'}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 4206, 'exit_idx': 4226, 'entry_price': 143.90420532226562, 'exit_price': 136.61178588867188, 'gross_return': -0.05067551304190676, 'net_return': -0.05257321269133597, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5218399510929117, 'reason': 'time'}}\n",
            "{'entry_idx': 4232, 'exit_idx': 4252, 'entry_price': 140.53018188476562, 'exit_price': 131.43641662597656, 'gross_return': -0.06471040695190917, 'net_return': -0.06658005084841234, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5167151070362404, 'reason': 'time'}}\n",
            "{'entry_idx': 4261, 'exit_idx': 4281, 'entry_price': 137.2385711669922, 'exit_price': 123.87850952148438, 'gross_return': -0.09734917473930244, 'net_return': -0.09915357373899858, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5327772971346836, 'reason': 'time'}}\n",
            "Exit reasons: {'time': 58}\n",
            "Non-time exits: []\n",
            "ENTRY DEBUG: {'checked': 468, 'blocked_trend': 384, 'blocked_latest_entry': 26, 'blocked_conf': 0, 'opened': 58, 'conf_min': 0.4275718854314328, 'conf_max': 0.5992145617495692}\n",
            "SELL DEBUG: {'seen': 0, 'sell_actions': 0}\n",
            "EXIT REASONS: {'time': 58}\n",
            "\n",
            "=== TRADE MANAGER (TEST) ===\n",
            "n_steps: 1860\n",
            "segment_len: 372\n",
            "n_trades: 58\n",
            "final_equity: 1.6027123880655472\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.013725886669135455\n",
            "win rate: 0.5862068965517241\n",
            "min/median/max net: -0.2515424823319645 0.012225928964981758 0.32334161900060887\n",
            "median hold bars: 20.0\n",
            "top 5 net: [0.1778045  0.18209916 0.24502303 0.30975361 0.32334162]\n",
            "bottom 5 net: [-0.25154248 -0.14124071 -0.13996183 -0.13894734 -0.12592149]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 49, 'entry_price': 168.69102478027344, 'exit_price': 151.9320831298828, 'gross_return': -0.09934696687165007, 'net_return': -0.10114737228487358, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5119705157768533, 'reason': 'time'}}\n",
            "{'entry_idx': 78, 'exit_idx': 98, 'entry_price': 153.17129516601562, 'exit_price': 142.07659912109375, 'gross_return': -0.07243325867876713, 'net_return': -0.07428746459466828, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5992145617495692, 'reason': 'time'}}\n",
            "{'entry_idx': 103, 'exit_idx': 123, 'entry_price': 144.45079040527344, 'exit_price': 124.4820327758789, 'gross_return': -0.13823917178555914, 'net_return': -0.13996183168115972, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5084835344301067, 'reason': 'time'}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 1773, 'exit_idx': 1793, 'entry_price': 130.0275421142578, 'exit_price': 127.60585021972656, 'gross_return': -0.018624453366989296, 'net_return': -0.020586223084708588, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5119936923913668, 'reason': 'time'}}\n",
            "{'entry_idx': 1804, 'exit_idx': 1824, 'entry_price': 139.49600219726562, 'exit_price': 130.85133361816406, 'gross_return': -0.061970726350112935, 'net_return': -0.06384584686813899, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4275718854314328, 'reason': 'time'}}\n",
            "{'entry_idx': 1832, 'exit_idx': 1852, 'entry_price': 135.22824096679688, 'exit_price': 135.62527465820312, 'gross_return': 0.0029360264436459863, 'net_return': 0.0009311573267851703, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5048217731500013, 'reason': 'time'}}\n",
            "\n",
            "=== ENTRY HARVEST (TRAIN) ===\n",
            "topk_per_segment: 80 min_gap: None use_conf_score: False\n",
            "n_entries: 249\n",
            "horizon: 20 segment_len: 867 feasible_in_segment: True\n",
            "first 10: [30, 46, 56, 66, 98, 112, 122, 132, 147, 157]\n",
            "last 10 : [4110, 4124, 4134, 4160, 4173, 4184, 4206, 4239, 4262, 4272]\n",
            "\n",
            "=== ENTRY HARVEST (TEST) ===\n",
            "topk_per_segment: 40 min_gap: None use_conf_score: False\n",
            "n_entries: 98\n",
            "horizon: 20 segment_len: 372 feasible_in_segment: True\n",
            "first 10: [31, 79, 93, 103, 142, 153, 171, 190, 202, 213]\n",
            "last 10 : [1705, 1717, 1727, 1748, 1759, 1771, 1784, 1794, 1809, 1839]\n",
            "\n",
            "=== SAVED ===\n",
            " - runs/entry_indices_train.npy (HARVESTED)\n",
            " - runs/entry_indices_test.npy (HARVESTED)\n",
            " - runs/trades_buy_only_train.json\n",
            " - runs/trades_buy_only_test.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from trade.trade_manager import TradeManager\n",
        "\n",
        "# -----------------------\n",
        "# SETTINGS\n",
        "# -----------------------\n",
        "SEG_LEN = 1239          # rows per ticker segment from your build_features validation\n",
        "N_SEGS  = 5             # AAPL, MSFT, NVDA, AMZN, GOOGL\n",
        "TRAIN_FRAC = 0.70       # time-based split within each segment\n",
        "\n",
        "# NEW: entry harvesting (for SellAgent training)\n",
        "TOPK_PER_SEG_TRAIN = 80     # try 50â€“150\n",
        "TOPK_PER_SEG_TEST  = 40     # fewer is fine for eval\n",
        "MIN_GAP_TRAIN = None        # None => defaults inside TradeManager\n",
        "MIN_GAP_TEST  = None\n",
        "USE_CONF_SCORE = False      # False => uses q1-q0 margin (recommended)\n",
        "\n",
        "# -----------------------\n",
        "# BUILD TRAIN/TEST INDEX (per segment, no leakage)\n",
        "# -----------------------\n",
        "train_len = int(SEG_LEN * TRAIN_FRAC)\n",
        "\n",
        "train_idx = []\n",
        "test_idx = []\n",
        "\n",
        "for seg in range(N_SEGS):\n",
        "    start = seg * SEG_LEN\n",
        "    train_idx.extend(range(start, start + train_len))\n",
        "    test_idx.extend(range(start + train_len, start + SEG_LEN))\n",
        "\n",
        "train_idx = np.array(train_idx, dtype=np.int32)\n",
        "test_idx  = np.array(test_idx, dtype=np.int32)\n",
        "\n",
        "X_train = features[train_idx]\n",
        "p_train = prices[train_idx]\n",
        "X_test  = features[test_idx]\n",
        "p_test  = prices[test_idx]\n",
        "\n",
        "# Segment length inside each split subset (since we concatenated segments in order)\n",
        "SEG_TRAIN = train_len\n",
        "SEG_TEST  = SEG_LEN - train_len\n",
        "\n",
        "print(\"=== DATA SPLIT ===\")\n",
        "print(\"features:\", features.shape, \"prices:\", prices.shape)\n",
        "print(\"SEG_LEN:\", SEG_LEN, \"N_SEGS:\", N_SEGS, \"TRAIN_FRAC:\", TRAIN_FRAC)\n",
        "print(\"train_len per seg:\", SEG_TRAIN, \"test_len per seg:\", SEG_TEST)\n",
        "print(\"X_train:\", X_train.shape, \"p_train:\", p_train.shape)\n",
        "print(\"X_test :\", X_test.shape,  \"p_test :\", p_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# HELPER: run TM + debug logs (unchanged backtest)\n",
        "# -----------------------\n",
        "def run_tm(name: str, X: np.ndarray, p: np.ndarray, seg_len: int, sell_agent=None):\n",
        "    tm = TradeManager(\n",
        "        buy_agent=agent,            # trained buy agent\n",
        "        sell_agent=None,      # optional\n",
        "        state=X,\n",
        "        prices=p,\n",
        "        reward=cfg.reward,\n",
        "        trade=cfg.trade_manager,\n",
        "        segment_len=seg_len,        # IMPORTANT for boundary correctness\n",
        "    )\n",
        "\n",
        "    res = tm.run()\n",
        "    trades = res[\"trades\"]\n",
        "\n",
        "    from collections import Counter\n",
        "    reasons = Counter([t[\"meta\"].get(\"reason\", \"none\") for t in res[\"trades\"]])\n",
        "    print(\"Exit reasons:\", dict(reasons))\n",
        "    print(\"Non-time exits:\", [t for t in res[\"trades\"] if t[\"meta\"].get(\"reason\") != \"time\"][:3])\n",
        "\n",
        "    print(\"ENTRY DEBUG:\", res[\"entry_debug\"])\n",
        "    print(\"SELL DEBUG:\", res[\"sell_debug\"])\n",
        "    print(\"EXIT REASONS:\", res.get(\"exit_reasons\"))\n",
        "\n",
        "\n",
        "    print(f\"\\n=== TRADE MANAGER ({name}) ===\")\n",
        "    print(\"n_steps:\", len(p))\n",
        "    print(\"segment_len:\", seg_len)\n",
        "    print(\"n_trades:\", res[\"n_trades\"])\n",
        "    print(\"final_equity:\", res[\"final_equity\"])\n",
        "\n",
        "    # Boundary-crossing check (must be 0)\n",
        "    if trades:\n",
        "        cross = sum((t[\"entry_idx\"] // seg_len) != (t[\"exit_idx\"] // seg_len) for t in trades)\n",
        "    else:\n",
        "        cross = 0\n",
        "    print(\"Trades crossing segment boundary:\", cross)\n",
        "\n",
        "    # Return stats\n",
        "    if trades:\n",
        "        net = np.array([t[\"net_return\"] for t in trades], dtype=float)\n",
        "        hold = np.array([t[\"hold_bars\"] for t in trades], dtype=float)\n",
        "\n",
        "        print(\"avg net return:\", float(net.mean()))\n",
        "        print(\"win rate:\", float((net > 0).mean()))\n",
        "        print(\"min/median/max net:\", float(net.min()), float(np.median(net)), float(net.max()))\n",
        "        print(\"median hold bars:\", float(np.median(hold)))\n",
        "        print(\"top 5 net:\", np.sort(net)[-5:])\n",
        "        print(\"bottom 5 net:\", np.sort(net)[:5])\n",
        "\n",
        "        # A few sample trades (head + tail)\n",
        "        print(\"\\nSample trades (first 3):\")\n",
        "        for t in trades[:3]:\n",
        "            print(t)\n",
        "        print(\"\\nSample trades (last 3):\")\n",
        "        for t in trades[-3:]:\n",
        "            print(t)\n",
        "    else:\n",
        "        print(\"No trades produced. Try lowering buy_min_confidence or disabling trend filter.\")\n",
        "\n",
        "    return tm, res\n",
        "\n",
        "# -----------------------\n",
        "# NEW: Harvest entry indices for SellAgent training (no trade execution)\n",
        "# -----------------------\n",
        "def harvest_entries(name: str, tm: TradeManager, topk_per_seg: int, min_gap=None, use_confidence_score=False):\n",
        "    entries = tm.collect_entry_indices_topk(\n",
        "        topk_per_segment=topk_per_seg,\n",
        "        min_gap=min_gap,\n",
        "        use_confidence_score=use_confidence_score,\n",
        "    )\n",
        "    entries = np.array(entries, dtype=np.int32)\n",
        "\n",
        "    # Quick sanity: segment boundary + horizon feasibility check (should hold by construction)\n",
        "    horizon = int(cfg.trade_manager.sell_horizon)\n",
        "    if len(entries) > 0:\n",
        "        seg_ok = np.all((entries % tm.segment_len) <= (tm.segment_len - 1 - horizon))\n",
        "    else:\n",
        "        seg_ok = True\n",
        "\n",
        "    print(f\"\\n=== ENTRY HARVEST ({name}) ===\")\n",
        "    print(\"topk_per_segment:\", topk_per_seg, \"min_gap:\", min_gap, \"use_conf_score:\", use_confidence_score)\n",
        "    print(\"n_entries:\", len(entries))\n",
        "    print(\"horizon:\", horizon, \"segment_len:\", tm.segment_len, \"feasible_in_segment:\", bool(seg_ok))\n",
        "    if len(entries) > 0:\n",
        "        print(\"first 10:\", entries[:10].tolist())\n",
        "        print(\"last 10 :\", entries[-10:].tolist())\n",
        "\n",
        "    return entries\n",
        "\n",
        "# -----------------------\n",
        "# RUN TRAIN + TEST (backtest as before)\n",
        "# -----------------------\n",
        "tm_train, res_train = run_tm(\"TRAIN\", X_train, p_train, seg_len=SEG_TRAIN, sell_agent=None)\n",
        "tm_test,  res_test  = run_tm(\"TEST\",  X_test,  p_test,  seg_len=SEG_TEST,  sell_agent=None)\n",
        "\n",
        "# -----------------------\n",
        "# HARVEST ENTRIES (NEW LOGIC) â€” use these for SellEnv training\n",
        "# -----------------------\n",
        "train_entries = harvest_entries(\n",
        "    \"TRAIN\",\n",
        "    tm_train,\n",
        "    topk_per_seg=TOPK_PER_SEG_TRAIN,\n",
        "    min_gap=MIN_GAP_TRAIN,\n",
        "    use_confidence_score=USE_CONF_SCORE,\n",
        ")\n",
        "\n",
        "test_entries = harvest_entries(\n",
        "    \"TEST\",\n",
        "    tm_test,\n",
        "    topk_per_seg=TOPK_PER_SEG_TEST,\n",
        "    min_gap=MIN_GAP_TEST,\n",
        "    use_confidence_score=USE_CONF_SCORE,\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# SAVE ARTIFACTS (into out_dir)\n",
        "# -----------------------\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "train_entries_path = os.path.join(out_dir, \"entry_indices_train.npy\")\n",
        "test_entries_path  = os.path.join(out_dir, \"entry_indices_test.npy\")\n",
        "\n",
        "# NEW: save harvested entries (not trade entries)\n",
        "np.save(train_entries_path, train_entries)\n",
        "np.save(test_entries_path,  test_entries)\n",
        "\n",
        "train_trades_json = os.path.join(out_dir, \"trades_buy_only_train.json\")\n",
        "test_trades_json  = os.path.join(out_dir, \"trades_buy_only_test.json\")\n",
        "\n",
        "with open(train_trades_json, \"w\") as f:\n",
        "    json.dump(res_train[\"trades\"], f, indent=2)\n",
        "\n",
        "with open(test_trades_json, \"w\") as f:\n",
        "    json.dump(res_test[\"trades\"], f, indent=2)\n",
        "\n",
        "print(\"\\n=== SAVED ===\")\n",
        "print(\" -\", train_entries_path, \"(HARVESTED)\")\n",
        "print(\" -\", test_entries_path,  \"(HARVESTED)\")\n",
        "print(\" -\", train_trades_json)\n",
        "print(\" -\", test_trades_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d5aa5cee",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST final_equity: 1.6027123880655472\n",
            "TEST n_trades: 58\n",
            "TEST avg net: 0.013725886669135455\n",
            "TEST win rate: 0.5862068965517241\n",
            "TEST min/median/max: (np.float64(-0.2515424823319645), np.float64(0.012225928964981758), np.float64(0.32334161900060887))\n"
          ]
        }
      ],
      "source": [
        "print(\"TEST final_equity:\", res_test[\"final_equity\"])\n",
        "print(\"TEST n_trades:\", res_test[\"n_trades\"])\n",
        "\n",
        "trades = res_test[\"trades\"]\n",
        "net = np.array([t[\"net_return\"] for t in trades], dtype=float) if trades else np.array([])\n",
        "print(\"TEST avg net:\", net.mean() if len(net) else None)\n",
        "print(\"TEST win rate:\", (net > 0).mean() if len(net) else None)\n",
        "print(\"TEST min/median/max:\", (net.min(), np.median(net), net.max()) if len(net) else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2a420c7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# def filter_entries(entries, seg_len, horizon, n):\n",
        "#     kept = []\n",
        "#     drop_oob = 0\n",
        "#     drop_seg = 0\n",
        "#     drop_horizon = 0\n",
        "\n",
        "#     for e in entries:\n",
        "#         e = int(e)\n",
        "#         if e < 0 or e >= n:\n",
        "#             drop_oob += 1\n",
        "#             continue\n",
        "\n",
        "#         # segment end (inclusive)\n",
        "#         seg_end = min(((e // seg_len) + 1) * seg_len - 1, n - 1)\n",
        "\n",
        "#         # need room for at least horizon bars INSIDE segment\n",
        "#         last_allowed = min(e + horizon, seg_end, n - 1)\n",
        "\n",
        "#         # if episode would immediately be at/over last_allowed, it's useless\n",
        "#         # (or you can make this stricter: require at least 1 step)\n",
        "#         if last_allowed <= e:\n",
        "#             drop_horizon += 1\n",
        "#             continue\n",
        "\n",
        "#         kept.append(e)\n",
        "\n",
        "#     kept = np.array(sorted(set(kept)), dtype=np.int32)\n",
        "\n",
        "#     print(\"entries raw:\", len(entries))\n",
        "#     print(\"entries kept:\", len(kept))\n",
        "#     print(\"dropped oob:\", drop_oob)\n",
        "#     print(\"dropped horizon/seg:\", drop_horizon)\n",
        "#     return kept\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3058fa38",
      "metadata": {},
      "source": [
        "# Sell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ec8c30cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "entries_train: (249,) entries_test: (98,)\n",
            "sell include_pos: True\n",
            "sell feat_dim: 10 state_dim: 13\n",
            "Sell Decay Steps: 60000 / Est Total: 75000\n",
            "SELL state_dim: 13 n_actions: 2\n",
            "[SELL] ep=10/4000 reward=0.0050 eps=0.998 loss=None\n",
            "[SELL] ep=20/4000 reward=0.0243 eps=0.996 loss=0.0010292499791830778\n",
            "[SELL] ep=30/4000 reward=0.0110 eps=0.994 loss=0.0007058464689180255\n",
            "[SELL] ep=40/4000 reward=0.0400 eps=0.992 loss=0.0009508313378319144\n",
            "[SELL] ep=50/4000 reward=0.0321 eps=0.990 loss=0.0008290411788038909\n",
            "[SELL] ep=60/4000 reward=0.0312 eps=0.988 loss=0.0008469241438433528\n",
            "target sync @ 500 loss 0.0005994084058329463\n",
            "[SELL] ep=70/4000 reward=-0.0079 eps=0.986 loss=0.00021494048996828496\n",
            "[SELL] ep=80/4000 reward=-0.0344 eps=0.984 loss=0.00026774549041874707\n",
            "[SELL] ep=90/4000 reward=-0.0442 eps=0.982 loss=0.00024250360729638487\n",
            "[SELL] ep=100/4000 reward=0.0110 eps=0.981 loss=0.0003875322872772813\n",
            "target sync @ 1000 loss 0.0005280726472847164\n",
            "[SELL] ep=110/4000 reward=0.0183 eps=0.979 loss=0.00021668136469088495\n",
            "[SELL] ep=120/4000 reward=0.0227 eps=0.977 loss=0.0001386088115395978\n",
            "[SELL] ep=130/4000 reward=-0.0346 eps=0.975 loss=0.0002781145158223808\n",
            "[SELL] ep=140/4000 reward=-0.0551 eps=0.973 loss=0.0003255250630900264\n",
            "target sync @ 1500 loss 0.0003126529336441308\n",
            "[SELL] ep=150/4000 reward=-0.0038 eps=0.971 loss=0.000228513206820935\n",
            "[SELL] ep=160/4000 reward=-0.0732 eps=0.969 loss=0.00017346492677461356\n",
            "[SELL] ep=170/4000 reward=0.0213 eps=0.967 loss=0.00015266332775354385\n",
            "[SELL] ep=180/4000 reward=0.0153 eps=0.965 loss=0.00023185762984212488\n",
            "target sync @ 2000 loss 0.00016046683595050126\n",
            "[SELL] ep=190/4000 reward=0.1014 eps=0.963 loss=0.00015283045649994165\n",
            "[SELL] ep=200/4000 reward=-0.0050 eps=0.961 loss=0.00010278007539454848\n",
            "[SELL] ep=210/4000 reward=0.0321 eps=0.960 loss=0.00011699946480803192\n",
            "[SELL] ep=220/4000 reward=-0.0499 eps=0.958 loss=0.00012542038166429847\n",
            "target sync @ 2500 loss 0.0001610454055480659\n",
            "[SELL] ep=230/4000 reward=-0.0056 eps=0.956 loss=0.00017475287313573062\n",
            "[SELL] ep=240/4000 reward=0.0671 eps=0.954 loss=0.0001473273296141997\n",
            "[SELL] ep=250/4000 reward=-0.0469 eps=0.952 loss=0.0001703617163002491\n",
            "[SELL] ep=260/4000 reward=0.0346 eps=0.950 loss=8.085712033789605e-05\n",
            "target sync @ 3000 loss 0.00018301839008927345\n",
            "[SELL] ep=270/4000 reward=0.0194 eps=0.948 loss=0.00022240853286348283\n",
            "[SELL] ep=280/4000 reward=-0.0670 eps=0.946 loss=0.00010430807014927268\n",
            "[SELL] ep=290/4000 reward=-0.0250 eps=0.944 loss=0.00010372671385994181\n",
            "[SELL] ep=300/4000 reward=0.0178 eps=0.943 loss=0.0001815917348721996\n",
            "target sync @ 3500 loss 0.00015906235785223544\n",
            "[SELL] ep=310/4000 reward=-0.0118 eps=0.940 loss=0.00011072454071836546\n",
            "[SELL] ep=320/4000 reward=-0.0471 eps=0.939 loss=0.00014205776096787304\n",
            "[SELL] ep=330/4000 reward=0.0679 eps=0.937 loss=9.919109288603067e-05\n",
            "[SELL] ep=340/4000 reward=-0.0819 eps=0.935 loss=8.826309931464493e-05\n",
            "target sync @ 4000 loss 9.947256330633536e-05\n",
            "[SELL] ep=350/4000 reward=0.0185 eps=0.933 loss=0.00011542798165464774\n",
            "[SELL] ep=360/4000 reward=0.0149 eps=0.931 loss=7.931656728032976e-05\n",
            "[SELL] ep=370/4000 reward=0.0109 eps=0.929 loss=7.715325045865029e-05\n",
            "[SELL] ep=380/4000 reward=-0.0418 eps=0.927 loss=4.279398126527667e-05\n",
            "target sync @ 4500 loss 9.050249354913831e-05\n",
            "[SELL] ep=390/4000 reward=-0.0018 eps=0.925 loss=0.00011391726729925722\n",
            "[SELL] ep=400/4000 reward=-0.0375 eps=0.923 loss=9.976361616281793e-05\n",
            "[SELL] ep=410/4000 reward=0.0001 eps=0.921 loss=8.083637658273801e-05\n",
            "[SELL] ep=420/4000 reward=0.0049 eps=0.919 loss=6.229825521586463e-05\n",
            "target sync @ 5000 loss 6.8874258431606e-05\n",
            "[SELL] ep=430/4000 reward=-0.0563 eps=0.917 loss=0.00021669629495590925\n",
            "[SELL] ep=440/4000 reward=0.0025 eps=0.915 loss=0.00010286368342349306\n",
            "[SELL] ep=450/4000 reward=-0.0272 eps=0.913 loss=0.00011922956036869437\n",
            "[SELL] ep=460/4000 reward=-0.0594 eps=0.911 loss=6.064974149921909e-05\n",
            "[SELL] ep=470/4000 reward=-0.1169 eps=0.909 loss=9.282846440328285e-05\n",
            "target sync @ 5500 loss 6.543740164488554e-05\n",
            "[SELL] ep=480/4000 reward=-0.0427 eps=0.907 loss=8.734901348361745e-05\n",
            "[SELL] ep=490/4000 reward=-0.0452 eps=0.905 loss=0.00011878617078764364\n",
            "[SELL] ep=500/4000 reward=-0.1063 eps=0.904 loss=0.00017280875181313604\n",
            "[SELL] ep=510/4000 reward=-0.0138 eps=0.902 loss=5.465605863719247e-05\n",
            "target sync @ 6000 loss 6.371044582920149e-05\n",
            "[SELL] ep=520/4000 reward=0.0001 eps=0.900 loss=0.00012103558401577175\n",
            "[SELL] ep=530/4000 reward=-0.0277 eps=0.898 loss=8.510874613421038e-05\n",
            "[SELL] ep=540/4000 reward=0.0178 eps=0.896 loss=8.74534307513386e-05\n",
            "[SELL] ep=550/4000 reward=-0.0932 eps=0.894 loss=5.4041651310399175e-05\n",
            "target sync @ 6500 loss 5.683185736415908e-05\n",
            "[SELL] ep=560/4000 reward=-0.0871 eps=0.892 loss=7.733667735010386e-05\n",
            "[SELL] ep=570/4000 reward=0.0343 eps=0.890 loss=0.00010146259592147544\n",
            "[SELL] ep=580/4000 reward=0.1623 eps=0.888 loss=9.177129686577246e-05\n",
            "[SELL] ep=590/4000 reward=-0.0078 eps=0.887 loss=9.395515371579677e-05\n",
            "target sync @ 7000 loss 0.00013701285934075713\n",
            "[SELL] ep=600/4000 reward=0.0300 eps=0.885 loss=0.0001225267187692225\n",
            "[SELL] ep=610/4000 reward=-0.0293 eps=0.883 loss=6.238622881937772e-05\n",
            "[SELL] ep=620/4000 reward=0.0085 eps=0.881 loss=0.00012993841664865613\n",
            "[SELL] ep=630/4000 reward=0.0183 eps=0.879 loss=6.966362707316875e-05\n",
            "target sync @ 7500 loss 4.9769510951591656e-05\n",
            "[SELL] ep=640/4000 reward=-0.0211 eps=0.877 loss=8.017871004994959e-05\n",
            "[SELL] ep=650/4000 reward=-0.0550 eps=0.875 loss=7.24260025890544e-05\n",
            "[SELL] ep=660/4000 reward=-0.0272 eps=0.873 loss=7.199325773399323e-05\n",
            "[SELL] ep=670/4000 reward=0.0534 eps=0.871 loss=0.00017762489733286202\n",
            "target sync @ 8000 loss 8.625859481981024e-05\n",
            "[SELL] ep=680/4000 reward=-0.0288 eps=0.869 loss=7.662006100872532e-05\n",
            "[SELL] ep=690/4000 reward=0.0380 eps=0.868 loss=8.502853597747162e-05\n",
            "[SELL] ep=700/4000 reward=-0.1678 eps=0.866 loss=5.2843661251245067e-05\n",
            "[SELL] ep=710/4000 reward=-0.0445 eps=0.864 loss=5.476286605698988e-05\n",
            "[SELL] ep=720/4000 reward=-0.0123 eps=0.862 loss=5.556633186643012e-05\n",
            "target sync @ 8500 loss 7.415877189487219e-05\n",
            "[SELL] ep=730/4000 reward=-0.0757 eps=0.860 loss=7.61176852392964e-05\n",
            "[SELL] ep=740/4000 reward=0.0282 eps=0.858 loss=3.76238240278326e-05\n",
            "[SELL] ep=750/4000 reward=-0.0339 eps=0.856 loss=7.178053783718497e-05\n",
            "[SELL] ep=760/4000 reward=0.0008 eps=0.854 loss=8.982849249150604e-05\n",
            "target sync @ 9000 loss 3.7421443266794086e-05\n",
            "[SELL] ep=770/4000 reward=-0.0067 eps=0.852 loss=4.6025226765777916e-05\n",
            "[SELL] ep=780/4000 reward=-0.0603 eps=0.850 loss=9.655237954575568e-05\n",
            "[SELL] ep=790/4000 reward=0.0166 eps=0.848 loss=4.3277024815324694e-05\n",
            "[SELL] ep=800/4000 reward=0.0075 eps=0.846 loss=4.485830868361518e-05\n",
            "target sync @ 9500 loss 5.3525080147664994e-05\n",
            "[SELL] ep=810/4000 reward=0.0538 eps=0.844 loss=0.00014275420107878745\n",
            "[SELL] ep=820/4000 reward=0.0538 eps=0.843 loss=8.632973913336173e-05\n",
            "[SELL] ep=830/4000 reward=-0.0523 eps=0.841 loss=5.89187657169532e-05\n",
            "[SELL] ep=840/4000 reward=-0.0076 eps=0.839 loss=0.00011081848060712218\n",
            "target sync @ 10000 loss 4.3594802264124155e-05\n",
            "[SELL] ep=850/4000 reward=-0.0452 eps=0.837 loss=4.6984256186988205e-05\n",
            "[SELL] ep=860/4000 reward=-0.0670 eps=0.835 loss=0.00010808126535266638\n",
            "[SELL] ep=870/4000 reward=-0.0558 eps=0.833 loss=5.484393477672711e-05\n",
            "[SELL] ep=880/4000 reward=0.0631 eps=0.831 loss=9.149263496510684e-05\n",
            "target sync @ 10500 loss 8.50069773150608e-05\n",
            "[SELL] ep=890/4000 reward=0.0928 eps=0.829 loss=4.341126623330638e-05\n",
            "[SELL] ep=900/4000 reward=0.0838 eps=0.827 loss=3.2730029488448054e-05\n",
            "[SELL] ep=910/4000 reward=0.0362 eps=0.826 loss=7.529766298830509e-05\n",
            "[SELL] ep=920/4000 reward=-0.0346 eps=0.823 loss=5.527245593839325e-05\n",
            "target sync @ 11000 loss 3.587874743971042e-05\n",
            "[SELL] ep=930/4000 reward=0.0774 eps=0.821 loss=5.413527105702087e-05\n",
            "[SELL] ep=940/4000 reward=0.0222 eps=0.820 loss=3.6354438634589314e-05\n",
            "[SELL] ep=950/4000 reward=-0.0312 eps=0.818 loss=4.387659646454267e-05\n",
            "[SELL] ep=960/4000 reward=0.1221 eps=0.816 loss=8.373880700673908e-05\n",
            "target sync @ 11500 loss 3.6129862564848736e-05\n",
            "[SELL] ep=970/4000 reward=0.0052 eps=0.814 loss=5.6141874665627256e-05\n",
            "[SELL] ep=980/4000 reward=-0.0824 eps=0.812 loss=7.686050230404362e-05\n",
            "[SELL] ep=990/4000 reward=-0.0102 eps=0.810 loss=3.916558125638403e-05\n",
            "[SELL] ep=1000/4000 reward=0.0161 eps=0.808 loss=2.9515598725993186e-05\n",
            "target sync @ 12000 loss 4.4041298679076135e-05\n",
            "[SELL] ep=1010/4000 reward=0.0810 eps=0.806 loss=6.265750562306494e-05\n",
            "[SELL] ep=1020/4000 reward=0.0227 eps=0.804 loss=5.488942770170979e-05\n",
            "[SELL] ep=1030/4000 reward=0.0243 eps=0.802 loss=4.045712557854131e-05\n",
            "[SELL] ep=1040/4000 reward=-0.0288 eps=0.800 loss=4.053766679135151e-05\n",
            "target sync @ 12500 loss 6.550635589519516e-05\n",
            "[SELL] ep=1050/4000 reward=-0.0428 eps=0.798 loss=3.698580258060247e-05\n",
            "[SELL] ep=1060/4000 reward=-0.0232 eps=0.796 loss=3.334202119731344e-05\n",
            "[SELL] ep=1070/4000 reward=0.0932 eps=0.794 loss=5.199316728976555e-05\n",
            "[SELL] ep=1080/4000 reward=-0.0170 eps=0.792 loss=6.647134432569146e-05\n",
            "target sync @ 13000 loss 8.542399882571772e-05\n",
            "[SELL] ep=1090/4000 reward=0.0209 eps=0.790 loss=5.0112234021071345e-05\n",
            "[SELL] ep=1100/4000 reward=-0.0629 eps=0.788 loss=8.566962060285732e-05\n",
            "[SELL] ep=1110/4000 reward=0.0115 eps=0.786 loss=4.62055868410971e-05\n",
            "[SELL] ep=1120/4000 reward=0.0234 eps=0.784 loss=3.1841977033764124e-05\n",
            "target sync @ 13500 loss 9.522315667709336e-05\n",
            "[SELL] ep=1130/4000 reward=-0.0670 eps=0.782 loss=6.485873745987192e-05\n",
            "[SELL] ep=1140/4000 reward=-0.1287 eps=0.780 loss=8.45588801894337e-05\n",
            "[SELL] ep=1150/4000 reward=0.0015 eps=0.778 loss=4.158976298640482e-05\n",
            "[SELL] ep=1160/4000 reward=-0.0031 eps=0.776 loss=0.0001262571895495057\n",
            "target sync @ 14000 loss 7.929468847578391e-05\n",
            "[SELL] ep=1170/4000 reward=0.0031 eps=0.774 loss=6.265506090130657e-05\n",
            "[SELL] ep=1180/4000 reward=-0.0169 eps=0.772 loss=6.886050687171519e-05\n",
            "[SELL] ep=1190/4000 reward=-0.0051 eps=0.770 loss=7.086175901349634e-05\n",
            "[SELL] ep=1200/4000 reward=-0.0703 eps=0.768 loss=4.597350562107749e-05\n",
            "target sync @ 14500 loss 3.414876482565887e-05\n",
            "[SELL] ep=1210/4000 reward=0.0020 eps=0.766 loss=6.492876855190843e-05\n",
            "[SELL] ep=1220/4000 reward=0.0339 eps=0.764 loss=4.201794945402071e-05\n",
            "[SELL] ep=1230/4000 reward=-0.0249 eps=0.762 loss=0.00010757108975667506\n",
            "[SELL] ep=1240/4000 reward=-0.0359 eps=0.760 loss=4.8600642912788317e-05\n",
            "target sync @ 15000 loss 2.7543699616217054e-05\n",
            "[SELL] ep=1250/4000 reward=-0.0051 eps=0.758 loss=4.235191590851173e-05\n",
            "[SELL] ep=1260/4000 reward=-0.0100 eps=0.756 loss=0.00010771361121442169\n",
            "[SELL] ep=1270/4000 reward=-0.0766 eps=0.754 loss=3.343974094605073e-05\n",
            "[SELL] ep=1280/4000 reward=0.0072 eps=0.752 loss=4.6312321501318365e-05\n",
            "target sync @ 15500 loss 5.5881802836665884e-05\n",
            "[SELL] ep=1290/4000 reward=-0.1032 eps=0.750 loss=6.235593900782987e-05\n",
            "[SELL] ep=1300/4000 reward=-0.0523 eps=0.748 loss=6.541067705256864e-05\n",
            "[SELL] ep=1310/4000 reward=-0.0120 eps=0.746 loss=2.451207365083974e-05\n",
            "[SELL] ep=1320/4000 reward=0.0480 eps=0.744 loss=4.367039946373552e-05\n",
            "target sync @ 16000 loss 4.526699194684625e-05\n",
            "[SELL] ep=1330/4000 reward=0.0276 eps=0.742 loss=6.327798473648727e-05\n",
            "[SELL] ep=1340/4000 reward=0.0475 eps=0.740 loss=5.3489115089178085e-05\n",
            "[SELL] ep=1350/4000 reward=0.0001 eps=0.738 loss=3.8345286156982183e-05\n",
            "[SELL] ep=1360/4000 reward=-0.0097 eps=0.736 loss=4.715058094006963e-05\n",
            "target sync @ 16500 loss 3.4172389860032126e-05\n",
            "[SELL] ep=1370/4000 reward=-0.0449 eps=0.734 loss=3.4396027331240475e-05\n",
            "[SELL] ep=1380/4000 reward=-0.1287 eps=0.732 loss=4.440068732947111e-05\n",
            "[SELL] ep=1390/4000 reward=0.0111 eps=0.730 loss=0.00011085589358117431\n",
            "[SELL] ep=1400/4000 reward=0.0384 eps=0.728 loss=2.606937414384447e-05\n",
            "target sync @ 17000 loss 7.137726788641885e-05\n",
            "[SELL] ep=1410/4000 reward=0.0011 eps=0.726 loss=5.827833228977397e-05\n",
            "[SELL] ep=1420/4000 reward=-0.0952 eps=0.724 loss=5.3338746511144564e-05\n",
            "[SELL] ep=1430/4000 reward=-0.0118 eps=0.722 loss=5.733585203415714e-05\n",
            "[SELL] ep=1440/4000 reward=-0.0819 eps=0.720 loss=4.105929838260636e-05\n",
            "target sync @ 17500 loss 6.716231291648e-05\n",
            "[SELL] ep=1450/4000 reward=0.1176 eps=0.718 loss=3.937745714210905e-05\n",
            "[SELL] ep=1460/4000 reward=0.0526 eps=0.716 loss=0.00013459721230901778\n",
            "[SELL] ep=1470/4000 reward=0.0039 eps=0.714 loss=6.909180228831246e-05\n",
            "[SELL] ep=1480/4000 reward=0.0631 eps=0.712 loss=3.33128627971746e-05\n",
            "target sync @ 18000 loss 5.6596465583425015e-05\n",
            "[SELL] ep=1490/4000 reward=0.0049 eps=0.710 loss=0.00010686514724511653\n",
            "[SELL] ep=1500/4000 reward=-0.0207 eps=0.708 loss=5.705895819119178e-05\n",
            "[SELL] ep=1510/4000 reward=0.0312 eps=0.706 loss=3.564970029401593e-05\n",
            "[SELL] ep=1520/4000 reward=-0.1579 eps=0.704 loss=4.832665581488982e-05\n",
            "target sync @ 18500 loss 8.79294311744161e-05\n",
            "[SELL] ep=1530/4000 reward=0.0060 eps=0.702 loss=8.771547436481342e-05\n",
            "[SELL] ep=1540/4000 reward=-0.1032 eps=0.700 loss=5.525679807760753e-05\n",
            "[SELL] ep=1550/4000 reward=-0.0051 eps=0.698 loss=5.80513478780631e-05\n",
            "[SELL] ep=1560/4000 reward=-0.0073 eps=0.696 loss=5.1529590564314276e-05\n",
            "target sync @ 19000 loss 6.924750050529838e-05\n",
            "[SELL] ep=1570/4000 reward=-0.0298 eps=0.694 loss=9.576701995683834e-05\n",
            "[SELL] ep=1580/4000 reward=-0.0100 eps=0.692 loss=0.00010169020970351994\n",
            "[SELL] ep=1590/4000 reward=-0.0039 eps=0.689 loss=5.6827007938409224e-05\n",
            "target sync @ 19500 loss 7.770402589812875e-05\n",
            "[SELL] ep=1600/4000 reward=-0.0252 eps=0.687 loss=5.3308824135456234e-05\n",
            "[SELL] ep=1610/4000 reward=-0.0736 eps=0.685 loss=5.871949906577356e-05\n",
            "[SELL] ep=1620/4000 reward=-0.0744 eps=0.683 loss=7.694687519688159e-05\n",
            "[SELL] ep=1630/4000 reward=-0.0016 eps=0.681 loss=7.380553870461881e-05\n",
            "target sync @ 20000 loss 9.212626173393801e-05\n",
            "[SELL] ep=1640/4000 reward=-0.1305 eps=0.679 loss=6.262310489546508e-05\n",
            "[SELL] ep=1650/4000 reward=-0.0170 eps=0.677 loss=3.7776975659653544e-05\n",
            "[SELL] ep=1660/4000 reward=-0.0404 eps=0.675 loss=5.942454663454555e-05\n",
            "[SELL] ep=1670/4000 reward=0.0521 eps=0.673 loss=3.725232818396762e-05\n",
            "target sync @ 20500 loss 6.82051686453633e-05\n",
            "[SELL] ep=1680/4000 reward=0.0526 eps=0.671 loss=0.0001574409834574908\n",
            "[SELL] ep=1690/4000 reward=-0.1376 eps=0.669 loss=6.65952029521577e-05\n",
            "[SELL] ep=1700/4000 reward=0.0057 eps=0.667 loss=5.003636033507064e-05\n",
            "[SELL] ep=1710/4000 reward=0.0303 eps=0.665 loss=4.301925946492702e-05\n",
            "target sync @ 21000 loss 6.952969852136448e-05\n",
            "[SELL] ep=1720/4000 reward=0.0526 eps=0.663 loss=5.110057463753037e-05\n",
            "[SELL] ep=1730/4000 reward=-0.0594 eps=0.661 loss=0.00010720186401158571\n",
            "[SELL] ep=1740/4000 reward=-0.0179 eps=0.659 loss=6.320247484836727e-05\n",
            "[SELL] ep=1750/4000 reward=0.0010 eps=0.657 loss=8.98412472452037e-05\n",
            "target sync @ 21500 loss 9.466198389418423e-05\n",
            "[SELL] ep=1760/4000 reward=-0.1654 eps=0.655 loss=9.770855103852227e-05\n",
            "[SELL] ep=1770/4000 reward=0.0284 eps=0.653 loss=5.892289846087806e-05\n",
            "[SELL] ep=1780/4000 reward=0.0312 eps=0.651 loss=6.865822069812566e-05\n",
            "[SELL] ep=1790/4000 reward=0.0094 eps=0.649 loss=7.300271681742743e-05\n",
            "target sync @ 22000 loss 2.9018057830398902e-05\n",
            "[SELL] ep=1800/4000 reward=0.1125 eps=0.647 loss=3.158287290716544e-05\n",
            "[SELL] ep=1810/4000 reward=0.0000 eps=0.645 loss=4.625615110853687e-05\n",
            "[SELL] ep=1820/4000 reward=-0.0171 eps=0.643 loss=9.856747055891901e-05\n",
            "[SELL] ep=1830/4000 reward=0.0015 eps=0.641 loss=7.944001845316961e-05\n",
            "target sync @ 22500 loss 5.1372742746025324e-05\n",
            "[SELL] ep=1840/4000 reward=0.0220 eps=0.639 loss=3.826574902632274e-05\n",
            "[SELL] ep=1850/4000 reward=0.0117 eps=0.637 loss=4.959610669175163e-05\n",
            "[SELL] ep=1860/4000 reward=0.0238 eps=0.635 loss=5.4795869800727814e-05\n",
            "[SELL] ep=1870/4000 reward=0.0141 eps=0.633 loss=6.382189894793555e-05\n",
            "target sync @ 23000 loss 4.564708797261119e-05\n",
            "[SELL] ep=1880/4000 reward=-0.0931 eps=0.630 loss=5.351772051653825e-05\n",
            "[SELL] ep=1890/4000 reward=0.0038 eps=0.628 loss=9.578583558322862e-05\n",
            "[SELL] ep=1900/4000 reward=-0.0031 eps=0.626 loss=4.1625797166489065e-05\n",
            "[SELL] ep=1910/4000 reward=-0.0386 eps=0.624 loss=6.608047988265753e-05\n",
            "target sync @ 23500 loss 5.074915679870173e-05\n",
            "[SELL] ep=1920/4000 reward=0.0395 eps=0.622 loss=8.493499626638368e-05\n",
            "[SELL] ep=1930/4000 reward=0.0099 eps=0.620 loss=5.660899114445783e-05\n",
            "[SELL] ep=1940/4000 reward=-0.0522 eps=0.618 loss=3.97779731429182e-05\n",
            "target sync @ 24000 loss 6.959481106605381e-05\n",
            "[SELL] ep=1950/4000 reward=-0.0143 eps=0.616 loss=6.577109888894483e-05\n",
            "[SELL] ep=1960/4000 reward=-0.0629 eps=0.614 loss=4.1171555494656786e-05\n",
            "[SELL] ep=1970/4000 reward=0.0105 eps=0.612 loss=2.246521216875408e-05\n",
            "[SELL] ep=1980/4000 reward=0.0639 eps=0.609 loss=4.60072114947252e-05\n",
            "target sync @ 24500 loss 9.13631811272353e-05\n",
            "[SELL] ep=1990/4000 reward=-0.0394 eps=0.607 loss=6.587112147826701e-05\n",
            "[SELL] ep=2000/4000 reward=-0.0679 eps=0.605 loss=6.0287311498541385e-05\n",
            "[SELL] ep=2010/4000 reward=-0.0055 eps=0.604 loss=5.325003439793363e-05\n",
            "[SELL] ep=2020/4000 reward=-0.1617 eps=0.602 loss=3.45749722328037e-05\n",
            "target sync @ 25000 loss 5.600145595963113e-05\n",
            "[SELL] ep=2030/4000 reward=0.0256 eps=0.600 loss=6.478424620581791e-05\n",
            "[SELL] ep=2040/4000 reward=0.0520 eps=0.597 loss=6.330741598503664e-05\n",
            "[SELL] ep=2050/4000 reward=0.1121 eps=0.595 loss=5.998916458338499e-05\n",
            "[SELL] ep=2060/4000 reward=0.0156 eps=0.593 loss=5.936947127338499e-05\n",
            "target sync @ 25500 loss 3.055859633604996e-05\n",
            "[SELL] ep=2070/4000 reward=-0.0725 eps=0.591 loss=3.839816417894326e-05\n",
            "[SELL] ep=2080/4000 reward=-0.0459 eps=0.589 loss=6.0522455896716565e-05\n",
            "[SELL] ep=2090/4000 reward=0.0036 eps=0.587 loss=3.133395512122661e-05\n",
            "[SELL] ep=2100/4000 reward=-0.0257 eps=0.585 loss=7.356789137702435e-05\n",
            "target sync @ 26000 loss 0.0001318646245636046\n",
            "[SELL] ep=2110/4000 reward=0.0152 eps=0.583 loss=6.894378020660952e-05\n",
            "[SELL] ep=2120/4000 reward=-0.0959 eps=0.581 loss=3.700648812809959e-05\n",
            "[SELL] ep=2130/4000 reward=0.0007 eps=0.578 loss=6.799037510063499e-05\n",
            "target sync @ 26500 loss 8.370738942176104e-05\n",
            "[SELL] ep=2140/4000 reward=-0.0424 eps=0.576 loss=8.815994078759104e-05\n",
            "[SELL] ep=2150/4000 reward=0.0009 eps=0.574 loss=6.571852281922475e-05\n",
            "[SELL] ep=2160/4000 reward=0.0294 eps=0.572 loss=4.0001101297093555e-05\n",
            "[SELL] ep=2170/4000 reward=-0.0192 eps=0.570 loss=9.468429198022932e-05\n",
            "target sync @ 27000 loss 4.267858457751572e-05\n",
            "[SELL] ep=2180/4000 reward=0.1330 eps=0.568 loss=7.762632594676688e-05\n",
            "[SELL] ep=2190/4000 reward=0.1278 eps=0.566 loss=6.02309410169255e-05\n",
            "[SELL] ep=2200/4000 reward=0.0129 eps=0.564 loss=5.1902585255447775e-05\n",
            "[SELL] ep=2210/4000 reward=-0.0279 eps=0.563 loss=5.537344259209931e-05\n",
            "target sync @ 27500 loss 3.965523501392454e-05\n",
            "[SELL] ep=2220/4000 reward=-0.0691 eps=0.560 loss=6.331463373498991e-05\n",
            "[SELL] ep=2230/4000 reward=0.0017 eps=0.559 loss=7.997809734661132e-05\n",
            "[SELL] ep=2240/4000 reward=-0.0106 eps=0.556 loss=7.408477540593594e-05\n",
            "[SELL] ep=2250/4000 reward=0.0366 eps=0.554 loss=5.495506411534734e-05\n",
            "target sync @ 28000 loss 4.8081259592436254e-05\n",
            "[SELL] ep=2260/4000 reward=0.1146 eps=0.552 loss=5.234557102085091e-05\n",
            "[SELL] ep=2270/4000 reward=0.0129 eps=0.550 loss=6.112919800216332e-05\n",
            "[SELL] ep=2280/4000 reward=-0.0387 eps=0.548 loss=5.3780113375978544e-05\n",
            "[SELL] ep=2290/4000 reward=0.0901 eps=0.546 loss=7.37602385925129e-05\n",
            "target sync @ 28500 loss 3.533141716616228e-05\n",
            "[SELL] ep=2300/4000 reward=-0.0904 eps=0.544 loss=2.9603108487208374e-05\n",
            "[SELL] ep=2310/4000 reward=-0.0117 eps=0.542 loss=9.562969353282824e-05\n",
            "[SELL] ep=2320/4000 reward=-0.0228 eps=0.540 loss=5.649625381920487e-05\n",
            "[SELL] ep=2330/4000 reward=0.0176 eps=0.538 loss=3.355729495524429e-05\n",
            "target sync @ 29000 loss 2.954391857201699e-05\n",
            "[SELL] ep=2340/4000 reward=-0.1494 eps=0.536 loss=6.862598820589483e-05\n",
            "[SELL] ep=2350/4000 reward=0.0439 eps=0.534 loss=4.643409920390695e-05\n",
            "[SELL] ep=2360/4000 reward=-0.0372 eps=0.532 loss=3.8577352825086564e-05\n",
            "[SELL] ep=2370/4000 reward=-0.0295 eps=0.530 loss=5.220784805715084e-05\n",
            "target sync @ 29500 loss 4.523048846749589e-05\n",
            "[SELL] ep=2380/4000 reward=0.0293 eps=0.528 loss=6.465020123869181e-05\n",
            "[SELL] ep=2390/4000 reward=-0.0336 eps=0.526 loss=0.00010508944978937507\n",
            "[SELL] ep=2400/4000 reward=-0.0215 eps=0.524 loss=5.8900212025037035e-05\n",
            "[SELL] ep=2410/4000 reward=-0.0555 eps=0.522 loss=8.727534441277385e-05\n",
            "target sync @ 30000 loss 0.00013235803635325283\n",
            "[SELL] ep=2420/4000 reward=-0.0952 eps=0.520 loss=6.821281567681581e-05\n",
            "[SELL] ep=2430/4000 reward=0.0928 eps=0.518 loss=5.62358764000237e-05\n",
            "[SELL] ep=2440/4000 reward=0.0739 eps=0.516 loss=5.061434058006853e-05\n",
            "[SELL] ep=2450/4000 reward=-0.0381 eps=0.514 loss=4.1980576497735456e-05\n",
            "target sync @ 30500 loss 2.3529813915956765e-05\n",
            "[SELL] ep=2460/4000 reward=-0.0038 eps=0.512 loss=6.02290965616703e-05\n",
            "[SELL] ep=2470/4000 reward=0.0023 eps=0.510 loss=4.426375380717218e-05\n",
            "[SELL] ep=2480/4000 reward=0.0615 eps=0.508 loss=4.8818663344718516e-05\n",
            "[SELL] ep=2490/4000 reward=-0.0237 eps=0.506 loss=6.712974573019892e-05\n",
            "target sync @ 31000 loss 4.002480272902176e-05\n",
            "[SELL] ep=2500/4000 reward=0.0000 eps=0.504 loss=3.796890814555809e-05\n",
            "[SELL] ep=2510/4000 reward=0.0148 eps=0.502 loss=4.9118716560769826e-05\n",
            "[SELL] ep=2520/4000 reward=0.0213 eps=0.500 loss=5.1037735829595476e-05\n",
            "[SELL] ep=2530/4000 reward=0.0888 eps=0.498 loss=5.83756118430756e-05\n",
            "target sync @ 31500 loss 4.5906730520073324e-05\n",
            "[SELL] ep=2540/4000 reward=0.0234 eps=0.496 loss=4.0112983697326854e-05\n",
            "[SELL] ep=2550/4000 reward=-0.0605 eps=0.494 loss=6.884123285999522e-05\n",
            "[SELL] ep=2560/4000 reward=0.0346 eps=0.493 loss=5.5300268286373466e-05\n",
            "[SELL] ep=2570/4000 reward=-0.0365 eps=0.490 loss=4.919596540275961e-05\n",
            "target sync @ 32000 loss 4.216295201331377e-05\n",
            "[SELL] ep=2580/4000 reward=-0.0106 eps=0.488 loss=5.2574381697922945e-05\n",
            "[SELL] ep=2590/4000 reward=0.0258 eps=0.486 loss=4.852097117691301e-05\n",
            "[SELL] ep=2600/4000 reward=-0.0182 eps=0.484 loss=2.3339078325079754e-05\n",
            "[SELL] ep=2610/4000 reward=0.0000 eps=0.482 loss=3.517703589750454e-05\n",
            "target sync @ 32500 loss 3.20125836879015e-05\n",
            "[SELL] ep=2620/4000 reward=-0.0198 eps=0.480 loss=5.7309342082589865e-05\n",
            "[SELL] ep=2630/4000 reward=-0.0793 eps=0.478 loss=6.154634320409968e-05\n",
            "[SELL] ep=2640/4000 reward=0.0009 eps=0.476 loss=3.649105201475322e-05\n",
            "target sync @ 33000 loss 2.7545156626729295e-05\n",
            "[SELL] ep=2650/4000 reward=-0.0461 eps=0.474 loss=7.196591468527913e-05\n",
            "[SELL] ep=2660/4000 reward=-0.0137 eps=0.472 loss=8.582015288993716e-05\n",
            "[SELL] ep=2670/4000 reward=0.0057 eps=0.470 loss=2.7065871108788997e-05\n",
            "[SELL] ep=2680/4000 reward=0.0012 eps=0.468 loss=5.5938016885193065e-05\n",
            "target sync @ 33500 loss 4.070584327564575e-05\n",
            "[SELL] ep=2690/4000 reward=-0.0124 eps=0.466 loss=4.672430077334866e-05\n",
            "[SELL] ep=2700/4000 reward=-0.0410 eps=0.464 loss=4.808091034647077e-05\n",
            "[SELL] ep=2710/4000 reward=-0.0326 eps=0.461 loss=3.102803020738065e-05\n",
            "[SELL] ep=2720/4000 reward=0.0230 eps=0.459 loss=4.164620622759685e-05\n",
            "target sync @ 34000 loss 3.814031879301183e-05\n",
            "[SELL] ep=2730/4000 reward=-0.1617 eps=0.458 loss=4.330319643486291e-05\n",
            "[SELL] ep=2740/4000 reward=0.0276 eps=0.455 loss=4.153174813836813e-05\n",
            "[SELL] ep=2750/4000 reward=0.0254 eps=0.453 loss=5.4111715144244954e-05\n",
            "[SELL] ep=2760/4000 reward=-0.0556 eps=0.451 loss=6.121134356362745e-05\n",
            "target sync @ 34500 loss 5.188712020753883e-05\n",
            "[SELL] ep=2770/4000 reward=-0.0129 eps=0.449 loss=4.130220986553468e-05\n",
            "[SELL] ep=2780/4000 reward=0.0063 eps=0.447 loss=3.710314922500402e-05\n",
            "[SELL] ep=2790/4000 reward=-0.0442 eps=0.445 loss=3.518080120556988e-05\n",
            "[SELL] ep=2800/4000 reward=0.0524 eps=0.443 loss=3.183853186783381e-05\n",
            "target sync @ 35000 loss 4.6420500439126045e-05\n",
            "[SELL] ep=2810/4000 reward=-0.0626 eps=0.441 loss=4.428403190104291e-05\n",
            "[SELL] ep=2820/4000 reward=0.0059 eps=0.439 loss=5.732843419536948e-05\n",
            "[SELL] ep=2830/4000 reward=-0.0016 eps=0.436 loss=3.784074942814186e-05\n",
            "[SELL] ep=2840/4000 reward=-0.0407 eps=0.434 loss=3.4227668948005885e-05\n",
            "target sync @ 35500 loss 3.554990325937979e-05\n",
            "[SELL] ep=2850/4000 reward=-0.0178 eps=0.432 loss=4.800271199201234e-05\n",
            "[SELL] ep=2860/4000 reward=0.0535 eps=0.430 loss=6.940534512978047e-05\n",
            "[SELL] ep=2870/4000 reward=-0.0372 eps=0.428 loss=3.300610842416063e-05\n",
            "target sync @ 36000 loss 3.8860343920532614e-05\n",
            "[SELL] ep=2880/4000 reward=-0.0135 eps=0.426 loss=5.5695192713756114e-05\n",
            "[SELL] ep=2890/4000 reward=0.0509 eps=0.424 loss=6.434371607610956e-05\n",
            "[SELL] ep=2900/4000 reward=-0.0265 eps=0.422 loss=3.236854536226019e-05\n",
            "[SELL] ep=2910/4000 reward=0.0258 eps=0.420 loss=2.6235702534904703e-05\n",
            "target sync @ 36500 loss 3.949452002416365e-05\n",
            "[SELL] ep=2920/4000 reward=0.0081 eps=0.418 loss=2.8970793209737167e-05\n",
            "[SELL] ep=2930/4000 reward=0.0336 eps=0.416 loss=2.4740027583902702e-05\n",
            "[SELL] ep=2940/4000 reward=-0.0446 eps=0.414 loss=8.171410445356742e-05\n",
            "[SELL] ep=2950/4000 reward=0.1235 eps=0.412 loss=5.9724992752308026e-05\n",
            "target sync @ 37000 loss 3.318220842629671e-05\n",
            "[SELL] ep=2960/4000 reward=-0.0507 eps=0.410 loss=0.00010781726450659335\n",
            "[SELL] ep=2970/4000 reward=-0.0110 eps=0.408 loss=4.979196091881022e-05\n",
            "[SELL] ep=2980/4000 reward=-0.0583 eps=0.405 loss=2.6590165361994877e-05\n",
            "[SELL] ep=2990/4000 reward=-0.0728 eps=0.403 loss=9.418106492375955e-05\n",
            "target sync @ 37500 loss 2.6223795430269092e-05\n",
            "[SELL] ep=3000/4000 reward=0.0841 eps=0.401 loss=4.562115282169543e-05\n",
            "[SELL] ep=3010/4000 reward=0.0667 eps=0.399 loss=3.331324114697054e-05\n",
            "[SELL] ep=3020/4000 reward=-0.0712 eps=0.397 loss=4.113690374651924e-05\n",
            "[SELL] ep=3030/4000 reward=-0.0826 eps=0.395 loss=5.716097803087905e-05\n",
            "target sync @ 38000 loss 5.151053483132273e-05\n",
            "[SELL] ep=3040/4000 reward=0.0987 eps=0.393 loss=4.544440162135288e-05\n",
            "[SELL] ep=3050/4000 reward=-0.0024 eps=0.391 loss=3.087692311964929e-05\n",
            "[SELL] ep=3060/4000 reward=-0.0991 eps=0.388 loss=4.649610127671622e-05\n",
            "target sync @ 38500 loss 2.2972701117396355e-05\n",
            "[SELL] ep=3070/4000 reward=0.2041 eps=0.386 loss=3.232894232496619e-05\n",
            "[SELL] ep=3080/4000 reward=0.0479 eps=0.384 loss=3.259068034822121e-05\n",
            "[SELL] ep=3090/4000 reward=-0.0044 eps=0.382 loss=4.780539165949449e-05\n",
            "[SELL] ep=3100/4000 reward=0.0928 eps=0.380 loss=3.457131242612377e-05\n",
            "target sync @ 39000 loss 3.494885459076613e-05\n",
            "[SELL] ep=3110/4000 reward=0.0000 eps=0.377 loss=3.541865953593515e-05\n",
            "[SELL] ep=3120/4000 reward=0.0784 eps=0.375 loss=2.382589445915073e-05\n",
            "[SELL] ep=3130/4000 reward=0.0639 eps=0.373 loss=5.384671385399997e-05\n",
            "target sync @ 39500 loss 2.8063328500138596e-05\n",
            "[SELL] ep=3140/4000 reward=-0.0920 eps=0.371 loss=2.947401844721753e-05\n",
            "[SELL] ep=3150/4000 reward=-0.0488 eps=0.369 loss=5.7560035202186555e-05\n",
            "[SELL] ep=3160/4000 reward=-0.0336 eps=0.366 loss=3.2259649742627516e-05\n",
            "[SELL] ep=3170/4000 reward=-0.0457 eps=0.364 loss=2.55374179687351e-05\n",
            "target sync @ 40000 loss 4.344258559285663e-05\n",
            "[SELL] ep=3180/4000 reward=-0.0381 eps=0.362 loss=4.162180266575888e-05\n",
            "[SELL] ep=3190/4000 reward=-0.0117 eps=0.360 loss=2.8633174224523827e-05\n",
            "[SELL] ep=3200/4000 reward=-0.0418 eps=0.357 loss=5.103388684801757e-05\n",
            "[SELL] ep=3210/4000 reward=-0.0116 eps=0.355 loss=3.30737057083752e-05\n",
            "target sync @ 40500 loss 3.055240085814148e-05\n",
            "[SELL] ep=3220/4000 reward=-0.0171 eps=0.353 loss=2.1570402168435976e-05\n",
            "[SELL] ep=3230/4000 reward=-0.0319 eps=0.351 loss=9.919399599311873e-05\n",
            "[SELL] ep=3240/4000 reward=-0.0793 eps=0.349 loss=3.09714705508668e-05\n",
            "target sync @ 41000 loss 3.163414658047259e-05\n",
            "[SELL] ep=3250/4000 reward=0.0480 eps=0.347 loss=5.225728818913922e-05\n",
            "[SELL] ep=3260/4000 reward=0.1221 eps=0.345 loss=1.8001253920374438e-05\n",
            "[SELL] ep=3270/4000 reward=-0.0124 eps=0.342 loss=2.8597089112736285e-05\n",
            "[SELL] ep=3280/4000 reward=0.0008 eps=0.340 loss=4.4818858441431075e-05\n",
            "target sync @ 41500 loss 3.7050514947623014e-05\n",
            "[SELL] ep=3290/4000 reward=0.0000 eps=0.338 loss=3.857171395793557e-05\n",
            "[SELL] ep=3300/4000 reward=0.0000 eps=0.336 loss=5.1334402087377384e-05\n",
            "[SELL] ep=3310/4000 reward=-0.0121 eps=0.334 loss=2.5556053515174426e-05\n",
            "[SELL] ep=3320/4000 reward=0.0189 eps=0.331 loss=3.653101884992793e-05\n",
            "target sync @ 42000 loss 4.028892726637423e-05\n",
            "[SELL] ep=3330/4000 reward=0.0000 eps=0.329 loss=2.9840994102414697e-05\n",
            "[SELL] ep=3340/4000 reward=0.0784 eps=0.327 loss=2.2095366148278117e-05\n",
            "[SELL] ep=3350/4000 reward=0.0301 eps=0.324 loss=2.7628124371403828e-05\n",
            "target sync @ 42500 loss 2.0270188542781398e-05\n",
            "[SELL] ep=3360/4000 reward=0.0461 eps=0.322 loss=3.772438503801823e-05\n",
            "[SELL] ep=3370/4000 reward=-0.0506 eps=0.320 loss=2.1024508896516636e-05\n",
            "[SELL] ep=3380/4000 reward=0.0480 eps=0.318 loss=3.1490308174397796e-05\n",
            "[SELL] ep=3390/4000 reward=-0.0791 eps=0.315 loss=2.6119578251382336e-05\n",
            "target sync @ 43000 loss 2.4602784833405167e-05\n",
            "[SELL] ep=3400/4000 reward=-0.0091 eps=0.313 loss=3.501338869682513e-05\n",
            "[SELL] ep=3410/4000 reward=-0.0492 eps=0.311 loss=2.7822439733427018e-05\n",
            "[SELL] ep=3420/4000 reward=-0.0105 eps=0.309 loss=5.115203748573549e-05\n",
            "target sync @ 43500 loss 2.179057992179878e-05\n",
            "[SELL] ep=3430/4000 reward=0.0755 eps=0.307 loss=4.3488224036991596e-05\n",
            "[SELL] ep=3440/4000 reward=0.0356 eps=0.305 loss=2.696494266274385e-05\n",
            "[SELL] ep=3450/4000 reward=-0.0428 eps=0.303 loss=3.206005203537643e-05\n",
            "[SELL] ep=3460/4000 reward=0.0480 eps=0.301 loss=2.1395249859779142e-05\n",
            "target sync @ 44000 loss 2.7165891879121773e-05\n",
            "[SELL] ep=3470/4000 reward=0.0071 eps=0.298 loss=5.7311153796035796e-05\n",
            "[SELL] ep=3480/4000 reward=0.0343 eps=0.296 loss=5.722287460230291e-05\n",
            "[SELL] ep=3490/4000 reward=-0.0038 eps=0.294 loss=3.471626405371353e-05\n",
            "target sync @ 44500 loss 3.396617830730975e-05\n",
            "[SELL] ep=3500/4000 reward=0.0535 eps=0.291 loss=6.99042939231731e-05\n",
            "[SELL] ep=3510/4000 reward=-0.0000 eps=0.289 loss=2.6914674890576862e-05\n",
            "[SELL] ep=3520/4000 reward=-0.0249 eps=0.287 loss=1.9371822418179363e-05\n",
            "[SELL] ep=3530/4000 reward=-0.0467 eps=0.284 loss=1.5445340977748856e-05\n",
            "target sync @ 45000 loss 3.747184018720873e-05\n",
            "[SELL] ep=3540/4000 reward=-0.0293 eps=0.282 loss=3.67852917406708e-05\n",
            "[SELL] ep=3550/4000 reward=-0.0346 eps=0.280 loss=3.5976416256744415e-05\n",
            "[SELL] ep=3560/4000 reward=0.0000 eps=0.277 loss=1.6138406863319688e-05\n",
            "target sync @ 45500 loss 3.7056968722026795e-05\n",
            "[SELL] ep=3570/4000 reward=-0.0335 eps=0.275 loss=2.6654128305381164e-05\n",
            "[SELL] ep=3580/4000 reward=-0.0489 eps=0.273 loss=4.079029167769477e-05\n",
            "[SELL] ep=3590/4000 reward=0.0310 eps=0.271 loss=4.080293365404941e-05\n",
            "[SELL] ep=3600/4000 reward=-0.0438 eps=0.268 loss=4.635355435311794e-05\n",
            "target sync @ 46000 loss 2.554032289481256e-05\n",
            "[SELL] ep=3610/4000 reward=-0.0868 eps=0.266 loss=4.6202629164326936e-05\n",
            "[SELL] ep=3620/4000 reward=-0.0560 eps=0.264 loss=2.1374999050749466e-05\n",
            "[SELL] ep=3630/4000 reward=0.0356 eps=0.262 loss=2.8869524612673558e-05\n",
            "target sync @ 46500 loss 5.421260357252322e-05\n",
            "[SELL] ep=3640/4000 reward=-0.0119 eps=0.260 loss=2.1418127289507538e-05\n",
            "[SELL] ep=3650/4000 reward=0.0222 eps=0.258 loss=1.793198498489801e-05\n",
            "[SELL] ep=3660/4000 reward=0.1273 eps=0.256 loss=3.7023983168182895e-05\n",
            "[SELL] ep=3670/4000 reward=-0.0330 eps=0.253 loss=3.399407796678133e-05\n",
            "target sync @ 47000 loss 2.599788422230631e-05\n",
            "[SELL] ep=3680/4000 reward=-0.0184 eps=0.251 loss=3.427801129873842e-05\n",
            "[SELL] ep=3690/4000 reward=0.0129 eps=0.249 loss=2.9139475373085588e-05\n",
            "[SELL] ep=3700/4000 reward=0.0649 eps=0.247 loss=0.00011220297164982185\n",
            "[SELL] ep=3710/4000 reward=-0.0361 eps=0.245 loss=3.9866979932412505e-05\n",
            "target sync @ 47500 loss 1.6601152310613543e-05\n",
            "[SELL] ep=3720/4000 reward=-0.0397 eps=0.243 loss=2.895174839068204e-05\n",
            "[SELL] ep=3730/4000 reward=0.0300 eps=0.240 loss=2.842841058736667e-05\n",
            "[SELL] ep=3740/4000 reward=-0.0117 eps=0.238 loss=2.5668603484518826e-05\n",
            "target sync @ 48000 loss 2.7132753530167975e-05\n",
            "[SELL] ep=3750/4000 reward=0.0054 eps=0.236 loss=2.989249514939729e-05\n",
            "[SELL] ep=3760/4000 reward=-0.0542 eps=0.234 loss=4.932072260999121e-05\n",
            "[SELL] ep=3770/4000 reward=0.0394 eps=0.231 loss=4.238214751239866e-05\n",
            "[SELL] ep=3780/4000 reward=0.0520 eps=0.229 loss=1.8441078282194212e-05\n",
            "target sync @ 48500 loss 2.6170488126808777e-05\n",
            "[SELL] ep=3790/4000 reward=0.0181 eps=0.227 loss=4.5054926886223257e-05\n",
            "[SELL] ep=3800/4000 reward=-0.0436 eps=0.225 loss=3.6552653909893706e-05\n",
            "[SELL] ep=3810/4000 reward=-0.0605 eps=0.223 loss=2.505849624867551e-05\n",
            "[SELL] ep=3820/4000 reward=-0.0636 eps=0.221 loss=2.132055669790134e-05\n",
            "target sync @ 49000 loss 2.3865031835157424e-05\n",
            "[SELL] ep=3830/4000 reward=-0.0915 eps=0.218 loss=2.6807290851138532e-05\n",
            "[SELL] ep=3840/4000 reward=0.0099 eps=0.216 loss=4.6787012252025306e-05\n",
            "[SELL] ep=3850/4000 reward=0.0058 eps=0.214 loss=4.652382631320506e-05\n",
            "target sync @ 49500 loss 3.182765794917941e-05\n",
            "[SELL] ep=3860/4000 reward=0.0137 eps=0.212 loss=3.5221208236180246e-05\n",
            "[SELL] ep=3870/4000 reward=0.0460 eps=0.210 loss=2.657609547895845e-05\n",
            "[SELL] ep=3880/4000 reward=-0.0359 eps=0.208 loss=2.1652045688824728e-05\n",
            "[SELL] ep=3890/4000 reward=0.0000 eps=0.206 loss=2.0738361854455434e-05\n",
            "target sync @ 50000 loss 2.4431605197605677e-05\n",
            "[SELL] ep=3900/4000 reward=-0.0394 eps=0.204 loss=2.2464437279268168e-05\n",
            "[SELL] ep=3910/4000 reward=-0.0112 eps=0.201 loss=3.2907395507209e-05\n",
            "[SELL] ep=3920/4000 reward=0.0481 eps=0.199 loss=9.167417010758072e-05\n",
            "target sync @ 50500 loss 3.832177753793076e-05\n",
            "[SELL] ep=3930/4000 reward=0.0003 eps=0.196 loss=4.5713204599451274e-05\n",
            "[SELL] ep=3940/4000 reward=-0.0075 eps=0.194 loss=4.2700445192167535e-05\n",
            "[SELL] ep=3950/4000 reward=-0.0954 eps=0.192 loss=4.1305342165287584e-05\n",
            "[SELL] ep=3960/4000 reward=0.0034 eps=0.189 loss=2.342334482818842e-05\n",
            "target sync @ 51000 loss 3.0982770113041624e-05\n",
            "[SELL] ep=3970/4000 reward=0.0000 eps=0.187 loss=4.8811671149451286e-05\n",
            "[SELL] ep=3980/4000 reward=-0.0549 eps=0.184 loss=4.012759018223733e-05\n",
            "[SELL] ep=3990/4000 reward=0.0183 eps=0.182 loss=3.478243888821453e-05\n",
            "target sync @ 51500 loss 2.323672379134223e-05\n",
            "[SELL] ep=4000/4000 reward=0.1501 eps=0.180 loss=3.6905283195665106e-05\n",
            "Saved: runs/sell_agent.pt\n",
            "\n",
            "=== SELL EVAL (TEST entries) ===\n",
            "n_entries: 98\n",
            "\n",
            "SellAgent:\n",
            "mean: -0.004359499293526214 median: 0.0 win_rate: 0.2857142857142857 min/max: -0.1622109133448263 0.10313347590748101\n",
            "\n",
            "Fixed horizon baseline (hold->forced exit):\n",
            "mean: 0.0 median: 0.0 win_rate: 0.0 min/max: 0.0 0.0\n",
            "\n",
            "Delta (agent - baseline):\n",
            "mean delta: -0.004359499293526214 median delta: 0.0 better %: 0.2857142857142857\n",
            "\n",
            "Per-entry delta: [ 0.0134  0.0199  0.      0.      0.0647  0.     -0.0059 -0.0242 -0.0198\n",
            " -0.0187  0.      0.      0.      0.0378 -0.0479  0.     -0.0055 -0.0454\n",
            "  0.0121  0.      0.      0.0515  0.      0.      0.      0.0092  0.\n",
            "  0.      0.      0.     -0.0212  0.03    0.007  -0.0528  0.0313  0.0258\n",
            " -0.0185  0.0019 -0.0245 -0.0051  0.      0.032   0.0381  0.     -0.1622\n",
            "  0.     -0.0446  0.0291 -0.01    0.0422  0.0048 -0.0489 -0.0419  0.0177\n",
            " -0.0738  0.      0.0164 -0.0494 -0.1056 -0.0602 -0.0359  0.      0.\n",
            "  0.      0.      0.      0.      0.     -0.0234 -0.0312  0.      0.0547\n",
            "  0.0398  0.0052  0.1017 -0.0403 -0.001   0.     -0.1059  0.      0.\n",
            "  0.0654 -0.0385  0.0118 -0.0241 -0.0064  0.     -0.0127 -0.0078 -0.0093\n",
            " -0.0066  0.0106  0.      0.     -0.0731  0.1031 -0.0072  0.0051]\n",
            "Better count: 28 / 98\n",
            "\n",
            "SellAgent exit stats:\n",
            "avg hold bars: 15.938775510204081 min/max hold bars: 10.0 20.0\n",
            "exit reasons: {'limit': 34, 'sell': 64}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "from agents.ddqn_agent import DDQNAgent\n",
        "from envs.sell_env import SellEnv  # <- make sure this is the TM-consistent SellEnv v1\n",
        "\n",
        "# Load entry indices saved by your TM cell\n",
        "entries_train = np.load(os.path.join(out_dir, \"entry_indices_train.npy\"))\n",
        "entries_test  = np.load(os.path.join(out_dir, \"entry_indices_test.npy\"))\n",
        "\n",
        "print(\"entries_train:\", entries_train.shape, \"entries_test:\", entries_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# Create SellEnv (TRAIN)\n",
        "# -----------------------\n",
        "sell_env_train = SellEnv(\n",
        "    features=X_train,\n",
        "    prices=p_train,\n",
        "    entry_indices=entries_train,\n",
        "    transaction_cost=cfg.reward.transaction_cost,\n",
        "    sell_horizon=cfg.trade_manager.sell_horizon,\n",
        "    min_hold_bars=cfg.trade_manager.min_hold_bars,\n",
        "    segment_len=SEG_TRAIN,\n",
        "    include_pos_features=True,\n",
        ")\n",
        "\n",
        "print(\"sell include_pos:\", sell_env_train.include_pos)\n",
        "print(\"sell feat_dim:\", sell_env_train.feat_dim, \"state_dim:\", sell_env_train.state_dim)\n",
        "\n",
        "# -----------------------\n",
        "# Create SellAgent config\n",
        "# # -----------------------\n",
        "# sell_cfg = deepcopy(cfg.agent)\n",
        "# sell_cfg.state_dim = int(sell_env_train.state_dim)\n",
        "# sell_cfg.n_actions = 2\n",
        "\n",
        "# # faster decay is fine, but DON'T double-count total_steps\n",
        "# sell_cfg.epsilon_start = 1.0\n",
        "# sell_cfg.epsilon_end = 0.05\n",
        "# sell_cfg.epsilon_decay_steps = 40000\n",
        "\n",
        "# sell_agent = DDQNAgent(sell_cfg)\n",
        "# -----------------------\n",
        "# Create SellAgent config\n",
        "# -----------------------\n",
        "sell_cfg = deepcopy(cfg.agent)\n",
        "sell_cfg.state_dim = int(sell_env_train.state_dim)\n",
        "sell_cfg.n_actions = 2\n",
        "\n",
        "# OPTIMIZED HYPERPARAMS FOR SELL AGENT\n",
        "sell_cfg.lr = 0.0005                 # Slightly lower LR for stability\n",
        "sell_cfg.epsilon_start = 1.0\n",
        "sell_cfg.epsilon_end = 0.05\n",
        "\n",
        "# We want decay to finish at ~80% of training\n",
        "EPISODES = 5000\n",
        "avg_steps_per_ep = 15  # horizon is 20, exits often happen around 10-20\n",
        "total_estimated_steps = EPISODES * avg_steps_per_ep\n",
        "\n",
        "sell_cfg.epsilon_decay_steps = int(total_estimated_steps * 0.8)\n",
        "\n",
        "sell_agent = DDQNAgent(sell_cfg)\n",
        "print(f\"Sell Decay Steps: {sell_cfg.epsilon_decay_steps} / Est Total: {total_estimated_steps}\")\n",
        "\n",
        "# ... training loop follows ...\n",
        "print(\"SELL state_dim:\", sell_cfg.state_dim, \"n_actions:\", sell_cfg.n_actions)\n",
        "\n",
        "# -----------------------\n",
        "# Train loop (episode-based)\n",
        "# -----------------------\n",
        "# EPISODES = 800\n",
        "EPISODES = 4000\n",
        "MAX_STEPS = 200         # safety cap (horizon is small anyway)\n",
        "UPDATES_PER_STEP = 1\n",
        "\n",
        "for ep in range(EPISODES):\n",
        "    s = sell_env_train.reset()\n",
        "    done = False\n",
        "    ep_reward = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    while (not done) and (steps < MAX_STEPS):\n",
        "        a = sell_agent.select_action(s, greedy=False)  # <- this increments total_steps internally\n",
        "        ns, r, done, info = sell_env_train.step(a)\n",
        "\n",
        "        sell_agent.push(s, a, r, ns, done)\n",
        "\n",
        "        # update after warmup (based on agent.total_steps, which is now correct)\n",
        "        if sell_agent.total_steps >= int(cfg.training.warmup_steps):\n",
        "            for _ in range(UPDATES_PER_STEP):\n",
        "                sell_agent.update()\n",
        "\n",
        "        s = ns\n",
        "        ep_reward += float(r)\n",
        "        steps += 1\n",
        "\n",
        "    if (ep + 1) % 10 == 0:\n",
        "        loss = sell_agent.loss_history[-1] if sell_agent.loss_history else None\n",
        "        print(f\"[SELL] ep={ep+1}/{EPISODES} reward={ep_reward:.4f} eps={sell_agent.eps:.3f} loss={loss}\")\n",
        "\n",
        "# Save model\n",
        "sell_path = os.path.join(out_dir, \"sell_agent.pt\")\n",
        "sell_agent.save(sell_path)\n",
        "print(\"Saved:\", sell_path)\n",
        "\n",
        "# -----------------------\n",
        "# Evaluation helpers\n",
        "# -----------------------\n",
        "def eval_sell_agent(env, agent, entry_indices, greedy=True):\n",
        "    rets, holds, exits, reasons = [], [], [], []\n",
        "    for e in entry_indices:\n",
        "        s = env.reset(int(e))\n",
        "        done = False\n",
        "        total_r = 0.0\n",
        "        steps = 0\n",
        "        last_info = None\n",
        "\n",
        "        while (not done) and (steps < 500):\n",
        "            a = agent.select_action(s, greedy=greedy)  # greedy=True => no eps update, no step increment\n",
        "            ns, r, done, info = env.step(a)\n",
        "            s = ns\n",
        "            total_r += float(r)\n",
        "            steps += 1\n",
        "            last_info = info\n",
        "\n",
        "        rets.append(total_r)\n",
        "        if last_info:\n",
        "            exits.append(last_info.get(\"exit_idx\", np.nan))\n",
        "            holds.append(last_info.get(\"bars_held\", np.nan))\n",
        "            reasons.append(last_info.get(\"reason\", \"\"))\n",
        "        else:\n",
        "            exits.append(np.nan); holds.append(np.nan); reasons.append(\"\")\n",
        "\n",
        "    return np.array(rets, float), np.array(holds, float), np.array(exits, float), reasons\n",
        "\n",
        "\n",
        "def eval_fixed_horizon(env, entry_indices):\n",
        "    # Just HOLD until forced exit\n",
        "    rets = []\n",
        "    for e in entry_indices:\n",
        "        s = env.reset(int(e))\n",
        "        done = False\n",
        "        total_r = 0.0\n",
        "        steps = 0\n",
        "        while (not done) and (steps < 500):\n",
        "            ns, r, done, info = env.step(0)  # HOLD\n",
        "            s = ns\n",
        "            total_r += float(r)\n",
        "            steps += 1\n",
        "        rets.append(total_r)\n",
        "    return np.array(rets, dtype=float)\n",
        "\n",
        "# -----------------------\n",
        "# Evaluate on TEST subset\n",
        "# -----------------------\n",
        "sell_env_test = SellEnv(\n",
        "    features=X_test,\n",
        "    prices=p_test,\n",
        "    entry_indices=entries_test,\n",
        "    transaction_cost=cfg.reward.transaction_cost,\n",
        "    sell_horizon=cfg.trade_manager.sell_horizon,\n",
        "    min_hold_bars=cfg.trade_manager.min_hold_bars,\n",
        "    segment_len=SEG_TEST,\n",
        "    include_pos_features=True,\n",
        ")\n",
        "\n",
        "rets_agent, holds_agent, exits_agent, reasons_agent = eval_sell_agent(\n",
        "    sell_env_test, sell_agent, entries_test, greedy=True\n",
        ")\n",
        "rets_base = eval_fixed_horizon(sell_env_test, entries_test)\n",
        "\n",
        "print(\"\\n=== SELL EVAL (TEST entries) ===\")\n",
        "print(\"n_entries:\", len(entries_test))\n",
        "\n",
        "print(\"\\nSellAgent:\")\n",
        "print(\"mean:\", float(rets_agent.mean()),\n",
        "      \"median:\", float(np.median(rets_agent)),\n",
        "      \"win_rate:\", float((rets_agent > 0).mean()),\n",
        "      \"min/max:\", float(rets_agent.min()), float(rets_agent.max()))\n",
        "\n",
        "print(\"\\nFixed horizon baseline (hold->forced exit):\")\n",
        "print(\"mean:\", float(rets_base.mean()),\n",
        "      \"median:\", float(np.median(rets_base)),\n",
        "      \"win_rate:\", float((rets_base > 0).mean()),\n",
        "      \"min/max:\", float(rets_base.min()), float(rets_base.max()))\n",
        "\n",
        "delta = rets_agent - rets_base\n",
        "print(\"\\nDelta (agent - baseline):\")\n",
        "print(\"mean delta:\", float(delta.mean()),\n",
        "      \"median delta:\", float(np.median(delta)),\n",
        "      \"better %:\", float((delta > 0).mean()))\n",
        "\n",
        "print(\"\\nPer-entry delta:\", np.round(delta, 4))\n",
        "print(\"Better count:\", int((delta > 0).sum()), \"/\", len(delta))\n",
        "\n",
        "# Optional: inspect exit behavior\n",
        "if len(exits_agent) > 0:\n",
        "    print(\"\\nSellAgent exit stats:\")\n",
        "    print(\"avg hold bars:\", float(np.nanmean(holds_agent)),\n",
        "          \"min/max hold bars:\", float(np.nanmin(holds_agent)), float(np.nanmax(holds_agent)))\n",
        "    # quick breakdown of reasons\n",
        "    unique, counts = np.unique(np.array(reasons_agent, dtype=str), return_counts=True)\n",
        "    print(\"exit reasons:\", dict(zip(unique.tolist(), counts.tolist())))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d010456a",
      "metadata": {},
      "source": [
        "# TradeManager with Sell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "51ee24b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA SPLIT ===\n",
            "features: (6195, 10) prices: (6195,)\n",
            "SEG_LEN: 1239 N_SEGS: 5 TRAIN_FRAC: 0.7\n",
            "train_len per seg: 867 test_len per seg: 372\n",
            "X_train: (4335, 10) p_train: (4335,)\n",
            "X_test : (1860, 10) p_test : (1860,)\n",
            "Sell seen: 1332 Sell actions: 243\n",
            "Exit reasons: {'time': 110, 'sell_agent': 39, 'segment_end': 1}\n",
            "Non-time exits: [{'entry_idx': 54, 'exit_idx': 69, 'entry_price': 48.49269104003906, 'exit_price': 47.91735076904297, 'gross_return': -0.011864473978584514, 'net_return': -0.013839756895101307, 'hold_bars': 15, 'forced_exit': False, 'meta': {'buy_conf': 0.5191797409353899, 'reason': 'sell_agent', 'sell_conf': 0.7618173928959574, 'sell_q0': 0.0325348861515522, 'sell_q1': 0.05578827112913132, 'sell_margin': 0.023253384977579117, 'sell_delta_vs_hold': 0.04867945231696269, 'sell_baseline_net': -0.062519209212064, 'sell_net_now': -0.013839756895101307}}, {'entry_idx': 170, 'exit_idx': 189, 'entry_price': 52.672393798828125, 'exit_price': 58.525146484375, 'gross_return': 0.11111613244501863, 'net_return': 0.10889501129626122, 'hold_bars': 19, 'forced_exit': False, 'meta': {'buy_conf': 0.5080829894395992, 'reason': 'sell_agent', 'sell_conf': 0.8216745169019861, 'sell_q0': -0.021524934098124504, 'sell_q1': 0.009029744192957878, 'sell_margin': 0.030554678291082382, 'sell_delta_vs_hold': 0.00013660600871179263, 'sell_baseline_net': 0.10875840528754943, 'sell_net_now': 0.10889501129626122}}, {'entry_idx': 194, 'exit_idx': 212, 'entry_price': 61.854469299316406, 'exit_price': 63.736366271972656, 'gross_return': 0.03042459169036986, 'net_return': 0.028364772931580795, 'hold_bars': 18, 'forced_exit': False, 'meta': {'buy_conf': 0.5190233211039669, 'reason': 'sell_agent', 'sell_conf': 0.8558981422946211, 'sell_q0': 0.010388417169451714, 'sell_q1': 0.04602103680372238, 'sell_margin': 0.03563261963427067, 'sell_delta_vs_hold': 0.009420852204266916, 'sell_baseline_net': 0.01894392072731388, 'sell_net_now': 0.028364772931580795}}]\n",
            "ENTRY DEBUG: {'checked': 942, 'blocked_trend': 779, 'blocked_latest_entry': 11, 'blocked_conf': 2, 'opened': 150, 'conf_min': 0.32457127927281265, 'conf_max': 0.6760742955395784}\n",
            "SELL DEBUG: {'seen': 1332, 'sell_actions': 243}\n",
            "EXIT REASONS: {'time': 110, 'sell_agent': 39, 'segment_end': 1}\n",
            "\n",
            "=== TRADE MANAGER (TRAIN) ===\n",
            "n_steps: 4335\n",
            "segment_len: 867\n",
            "n_trades: 150\n",
            "final_equity: 39.59873031845221\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.02826318638439283\n",
            "win rate: 0.6266666666666667\n",
            "min/median/max net: -0.16680681604095793 0.03227206497015933 0.39307612553170723\n",
            "median hold bars: 20.0\n",
            "top 5 net: [0.18739447 0.19005804 0.29425715 0.3383027  0.39307613]\n",
            "bottom 5 net: [-0.16680682 -0.16360307 -0.13863099 -0.13835389 -0.11820936]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 49, 'entry_price': 43.37913513183594, 'exit_price': 47.89346694946289, 'gross_return': 0.10406689307905961, 'net_return': 0.10185986335979447, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5133428701243621, 'reason': 'time'}}\n",
            "{'entry_idx': 54, 'exit_idx': 69, 'entry_price': 48.49269104003906, 'exit_price': 47.91735076904297, 'gross_return': -0.011864473978584514, 'net_return': -0.013839756895101307, 'hold_bars': 15, 'forced_exit': False, 'meta': {'buy_conf': 0.5191797409353899, 'reason': 'sell_agent', 'sell_conf': 0.7618173928959574, 'sell_q0': 0.0325348861515522, 'sell_q1': 0.05578827112913132, 'sell_margin': 0.023253384977579117, 'sell_delta_vs_hold': 0.04867945231696269, 'sell_baseline_net': -0.062519209212064, 'sell_net_now': -0.013839756895101307}}\n",
            "{'entry_idx': 95, 'exit_idx': 115, 'entry_price': 46.465084075927734, 'exit_price': 49.00773620605469, 'gross_return': 0.05472178046577937, 'net_return': 0.05261339162662826, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5125973828544741, 'reason': 'time'}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 4206, 'exit_idx': 4226, 'entry_price': 143.90420532226562, 'exit_price': 136.61178588867188, 'gross_return': -0.05067551304190676, 'net_return': -0.05257321269133597, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5218399510929117, 'reason': 'time'}}\n",
            "{'entry_idx': 4232, 'exit_idx': 4252, 'entry_price': 140.53018188476562, 'exit_price': 131.43641662597656, 'gross_return': -0.06471040695190917, 'net_return': -0.06658005084841234, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5167151070362404, 'reason': 'time'}}\n",
            "{'entry_idx': 4261, 'exit_idx': 4281, 'entry_price': 137.2385711669922, 'exit_price': 123.87850952148438, 'gross_return': -0.09734917473930244, 'net_return': -0.09915357373899858, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5327772971346836, 'reason': 'time'}}\n",
            "Sell seen: 573 Sell actions: 95\n",
            "Exit reasons: {'time': 52, 'sell_agent': 7}\n",
            "Non-time exits: [{'entry_idx': 135, 'exit_idx': 147, 'entry_price': 139.0128173828125, 'exit_price': 149.6621551513672, 'gross_return': 0.0766068767545984, 'net_return': 0.074454739607966, 'hold_bars': 12, 'forced_exit': False, 'meta': {'buy_conf': 0.5080829894395992, 'reason': 'sell_agent', 'sell_conf': 0.8264373094470374, 'sell_q0': 0.0187951922416687, 'sell_q1': 0.05000689625740051, 'sell_margin': 0.03121170401573181, 'sell_delta_vs_hold': 0.02272606758394602, 'sell_baseline_net': 0.051728672024019984, 'sell_net_now': 0.074454739607966}}, {'entry_idx': 252, 'exit_idx': 269, 'entry_price': 187.4940643310547, 'exit_price': 179.807373046875, 'gross_return': -0.04099698468644534, 'net_return': -0.042914031714057055, 'hold_bars': 17, 'forced_exit': False, 'meta': {'buy_conf': 0.5080829894395992, 'reason': 'sell_agent', 'sell_conf': 0.932121145441078, 'sell_q0': 0.02849615179002285, 'sell_q1': 0.08089091628789902, 'sell_margin': 0.05239476449787617, 'sell_delta_vs_hold': 0.01998436816171989, 'sell_baseline_net': -0.06289839987577694, 'sell_net_now': -0.042914031714057055}}, {'entry_idx': 510, 'exit_idx': 529, 'entry_price': 242.20904541015625, 'exit_price': 249.44419860839844, 'gross_return': 0.029871523526258756, 'net_return': 0.0278128103507298, 'hold_bars': 19, 'forced_exit': False, 'meta': {'buy_conf': 0.5414847981865549, 'reason': 'sell_agent', 'sell_conf': 0.8622556262089364, 'sell_q0': -0.026294125244021416, 'sell_q1': 0.010388918220996857, 'sell_margin': 0.03668304346501827, 'sell_delta_vs_hold': 0.022390341164394867, 'sell_baseline_net': 0.005422469186334933, 'sell_net_now': 0.0278128103507298}}]\n",
            "ENTRY DEBUG: {'checked': 468, 'blocked_trend': 383, 'blocked_latest_entry': 26, 'blocked_conf': 0, 'opened': 59, 'conf_min': 0.4275718854314328, 'conf_max': 0.5992145617495692}\n",
            "SELL DEBUG: {'seen': 573, 'sell_actions': 95}\n",
            "EXIT REASONS: {'time': 52, 'sell_agent': 7}\n",
            "\n",
            "=== TRADE MANAGER (TEST) ===\n",
            "n_steps: 1860\n",
            "segment_len: 372\n",
            "n_trades: 59\n",
            "final_equity: 1.9336726529058437\n",
            "Trades crossing segment boundary: 0\n",
            "avg net return: 0.01663657832486006\n",
            "win rate: 0.5932203389830508\n",
            "min/median/max net: -0.2515424823319645 0.021133717415761533 0.32334161900060887\n",
            "median hold bars: 20.0\n",
            "top 5 net: [0.1778045  0.18209916 0.24502303 0.30975361 0.32334162]\n",
            "bottom 5 net: [-0.25154248 -0.14124071 -0.13996183 -0.13894734 -0.12592149]\n",
            "\n",
            "Sample trades (first 3):\n",
            "{'entry_idx': 29, 'exit_idx': 49, 'entry_price': 168.69102478027344, 'exit_price': 151.9320831298828, 'gross_return': -0.09934696687165007, 'net_return': -0.10114737228487358, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5119705157768533, 'reason': 'time'}}\n",
            "{'entry_idx': 78, 'exit_idx': 98, 'entry_price': 153.17129516601562, 'exit_price': 142.07659912109375, 'gross_return': -0.07243325867876713, 'net_return': -0.07428746459466828, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5992145617495692, 'reason': 'time'}}\n",
            "{'entry_idx': 103, 'exit_idx': 123, 'entry_price': 144.45079040527344, 'exit_price': 124.4820327758789, 'gross_return': -0.13823917178555914, 'net_return': -0.13996183168115972, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5084835344301067, 'reason': 'time'}}\n",
            "\n",
            "Sample trades (last 3):\n",
            "{'entry_idx': 1773, 'exit_idx': 1792, 'entry_price': 130.0275421142578, 'exit_price': 130.12681579589844, 'gross_return': 0.0007634819517959547, 'net_return': -0.001237044248625696, 'hold_bars': 19, 'forced_exit': False, 'meta': {'buy_conf': 0.5119936923913668, 'reason': 'sell_agent', 'sell_conf': 0.8967789291515255, 'sell_q0': 0.028720634058117867, 'sell_q1': 0.07195936143398285, 'sell_margin': 0.04323872737586498, 'sell_delta_vs_hold': 0.019349178836082892, 'sell_baseline_net': -0.020586223084708588, 'sell_net_now': -0.001237044248625696}}\n",
            "{'entry_idx': 1804, 'exit_idx': 1824, 'entry_price': 139.49600219726562, 'exit_price': 130.85133361816406, 'gross_return': -0.061970726350112935, 'net_return': -0.06384584686813899, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.4275718854314328, 'reason': 'time'}}\n",
            "{'entry_idx': 1832, 'exit_idx': 1852, 'entry_price': 135.22824096679688, 'exit_price': 135.62527465820312, 'gross_return': 0.0029360264436459863, 'net_return': 0.0009311573267851703, 'hold_bars': 20, 'forced_exit': True, 'meta': {'buy_conf': 0.5048217731500013, 'reason': 'time'}}\n",
            "SELL Î” TRAIN: {'count': 39, 'mean_delta': 0.04419347271323204, 'median_delta': 0.021525688469409943, 'win_rate': 1.0}\n",
            "SELL Î” TEST : {'count': 7, 'mean_delta': 0.020891370251774788, 'median_delta': 0.01998436450958252, 'win_rate': 1.0}\n",
            "\n",
            "=== SAVED ===\n",
            " - runs/entry_indices_train_sell.npy (HARVESTED)\n",
            " - runs/entry_indices_test_sell.npy (HARVESTED)\n",
            " - runs/trades_buy_only_train.json\n",
            " - runs/trades_buy_only_test.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from trade.trade_manager import TradeManager\n",
        "\n",
        "# -----------------------\n",
        "# SETTINGS\n",
        "# -----------------------\n",
        "SEG_LEN = 1239          # rows per ticker segment from your build_features validation\n",
        "N_SEGS  = 5             # AAPL, MSFT, NVDA, AMZN, GOOGL\n",
        "TRAIN_FRAC = 0.70       # time-based split within each segment\n",
        "\n",
        "# NEW: entry harvesting (for SellAgent training)\n",
        "TOPK_PER_SEG_TRAIN = 80     # try 50â€“150\n",
        "TOPK_PER_SEG_TEST  = 40     # fewer is fine for eval\n",
        "MIN_GAP_TRAIN = None        # None => defaults inside TradeManager\n",
        "MIN_GAP_TEST  = None\n",
        "USE_CONF_SCORE = False      # False => uses q1-q0 margin (recommended)\n",
        "\n",
        "# -----------------------\n",
        "# BUILD TRAIN/TEST INDEX (per segment, no leakage)\n",
        "# -----------------------\n",
        "train_len = int(SEG_LEN * TRAIN_FRAC)\n",
        "\n",
        "train_idx = []\n",
        "test_idx = []\n",
        "\n",
        "for seg in range(N_SEGS):\n",
        "    start = seg * SEG_LEN\n",
        "    train_idx.extend(range(start, start + train_len))\n",
        "    test_idx.extend(range(start + train_len, start + SEG_LEN))\n",
        "\n",
        "train_idx = np.array(train_idx, dtype=np.int32)\n",
        "test_idx  = np.array(test_idx, dtype=np.int32)\n",
        "\n",
        "X_train = features[train_idx]\n",
        "p_train = prices[train_idx]\n",
        "X_test  = features[test_idx]\n",
        "p_test  = prices[test_idx]\n",
        "\n",
        "# Segment length inside each split subset (since we concatenated segments in order)\n",
        "SEG_TRAIN = train_len\n",
        "SEG_TEST  = SEG_LEN - train_len\n",
        "\n",
        "print(\"=== DATA SPLIT ===\")\n",
        "print(\"features:\", features.shape, \"prices:\", prices.shape)\n",
        "print(\"SEG_LEN:\", SEG_LEN, \"N_SEGS:\", N_SEGS, \"TRAIN_FRAC:\", TRAIN_FRAC)\n",
        "print(\"train_len per seg:\", SEG_TRAIN, \"test_len per seg:\", SEG_TEST)\n",
        "print(\"X_train:\", X_train.shape, \"p_train:\", p_train.shape)\n",
        "print(\"X_test :\", X_test.shape,  \"p_test :\", p_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# HELPER: run TM + debug logs (unchanged backtest)\n",
        "# -----------------------\n",
        "def run_tm(name: str, X: np.ndarray, p: np.ndarray, seg_len: int, sell_agent=None):\n",
        "    tm = TradeManager(\n",
        "        buy_agent=agent,            # trained buy agent\n",
        "        sell_agent=sell_agent,      # optional\n",
        "        state=X,\n",
        "        prices=p,\n",
        "        reward=cfg.reward,\n",
        "        trade=cfg.trade_manager,\n",
        "        segment_len=seg_len,        # IMPORTANT for boundary correctness\n",
        "    )\n",
        "\n",
        "    res = tm.run()\n",
        "    trades = res[\"trades\"]\n",
        "\n",
        "    print(\"Sell seen:\", tm._sell_debug[\"seen\"], \"Sell actions:\", tm._sell_debug[\"sell_actions\"])\n",
        "\n",
        "    from collections import Counter\n",
        "    reasons = Counter([t[\"meta\"].get(\"reason\", \"none\") for t in res[\"trades\"]])\n",
        "    print(\"Exit reasons:\", dict(reasons))\n",
        "    print(\"Non-time exits:\", [t for t in res[\"trades\"] if t[\"meta\"].get(\"reason\") != \"time\"][:3])\n",
        "\n",
        "    print(\"ENTRY DEBUG:\", res[\"entry_debug\"])\n",
        "    print(\"SELL DEBUG:\", res[\"sell_debug\"])\n",
        "    print(\"EXIT REASONS:\", res.get(\"exit_reasons\"))\n",
        "\n",
        "\n",
        "    print(f\"\\n=== TRADE MANAGER ({name}) ===\")\n",
        "    print(\"n_steps:\", len(p))\n",
        "    print(\"segment_len:\", seg_len)\n",
        "    print(\"n_trades:\", res[\"n_trades\"])\n",
        "    print(\"final_equity:\", res[\"final_equity\"])\n",
        "\n",
        "    # Boundary-crossing check (must be 0)\n",
        "    if trades:\n",
        "        cross = sum((t[\"entry_idx\"] // seg_len) != (t[\"exit_idx\"] // seg_len) for t in trades)\n",
        "    else:\n",
        "        cross = 0\n",
        "    print(\"Trades crossing segment boundary:\", cross)\n",
        "\n",
        "    # Return stats\n",
        "    if trades:\n",
        "        net = np.array([t[\"net_return\"] for t in trades], dtype=float)\n",
        "        hold = np.array([t[\"hold_bars\"] for t in trades], dtype=float)\n",
        "\n",
        "        print(\"avg net return:\", float(net.mean()))\n",
        "        print(\"win rate:\", float((net > 0).mean()))\n",
        "        print(\"min/median/max net:\", float(net.min()), float(np.median(net)), float(net.max()))\n",
        "        print(\"median hold bars:\", float(np.median(hold)))\n",
        "        print(\"top 5 net:\", np.sort(net)[-5:])\n",
        "        print(\"bottom 5 net:\", np.sort(net)[:5])\n",
        "\n",
        "        # A few sample trades (head + tail)\n",
        "        print(\"\\nSample trades (first 3):\")\n",
        "        for t in trades[:3]:\n",
        "            print(t)\n",
        "        print(\"\\nSample trades (last 3):\")\n",
        "        for t in trades[-3:]:\n",
        "            print(t)\n",
        "    else:\n",
        "        print(\"No trades produced. Try lowering buy_min_confidence or disabling trend filter.\")\n",
        "\n",
        "    return tm, res\n",
        "\n",
        "# -----------------------\n",
        "# NEW: Harvest entry indices for SellAgent training (no trade execution)\n",
        "# -----------------------\n",
        "# def harvest_entries(name: str, tm: TradeManager, topk_per_seg: int, min_gap=None, use_confidence_score=False):\n",
        "#     entries = tm.collect_entry_indices_topk(\n",
        "#         topk_per_segment=topk_per_seg,\n",
        "#         min_gap=min_gap,\n",
        "#         use_confidence_score=use_confidence_score,\n",
        "#     )\n",
        "#     entries = np.array(entries, dtype=np.int32)\n",
        "\n",
        "#     # Quick sanity: segment boundary + horizon feasibility check (should hold by construction)\n",
        "#     horizon = int(cfg.trade_manager.sell_horizon)\n",
        "#     if len(entries) > 0:\n",
        "#         seg_ok = np.all((entries % tm.segment_len) <= (tm.segment_len - 1 - horizon))\n",
        "#     else:\n",
        "#         seg_ok = True\n",
        "\n",
        "#     print(f\"\\n=== ENTRY HARVEST ({name}) ===\")\n",
        "#     print(\"topk_per_segment:\", topk_per_seg, \"min_gap:\", min_gap, \"use_conf_score:\", use_confidence_score)\n",
        "#     print(\"n_entries:\", len(entries))\n",
        "#     print(\"horizon:\", horizon, \"segment_len:\", tm.segment_len, \"feasible_in_segment:\", bool(seg_ok))\n",
        "#     if len(entries) > 0:\n",
        "#         print(\"first 10:\", entries[:10].tolist())\n",
        "#         print(\"last 10 :\", entries[-10:].tolist())\n",
        "\n",
        "#     return entries\n",
        "\n",
        "# -----------------------\n",
        "# RUN TRAIN + TEST (backtest as before)\n",
        "# -----------------------\n",
        "tm_train_sell, res_train_sell = run_tm(\"TRAIN\", X_train, p_train, seg_len=SEG_TRAIN, sell_agent=sell_agent)\n",
        "tm_test_sell,  res_test_sell  = run_tm(\"TEST\",  X_test,  p_test,  seg_len=SEG_TEST,  sell_agent=sell_agent)\n",
        "\n",
        "stats_train = compute_mean_delta(\n",
        "    trades=res_train_sell[\"trades\"],\n",
        "    prices=p_train,\n",
        "    horizon=cfg.trade_manager.sell_horizon,\n",
        "    tc=cfg.reward.transaction_cost,\n",
        ")\n",
        "\n",
        "stats_test = compute_mean_delta(\n",
        "    trades=res_test_sell[\"trades\"],\n",
        "    prices=p_test,\n",
        "    horizon=cfg.trade_manager.sell_horizon,\n",
        "    tc=cfg.reward.transaction_cost,\n",
        ")\n",
        "\n",
        "print(\"SELL Î” TRAIN:\", stats_train)\n",
        "print(\"SELL Î” TEST :\", stats_test)\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# HARVEST ENTRIES (NEW LOGIC) â€” use these for SellEnv training\n",
        "# -----------------------\n",
        "# train_entries_sell = harvest_entries(\n",
        "#     \"TRAIN\",\n",
        "#     tm_train_sell,\n",
        "#     topk_per_seg=TOPK_PER_SEG_TRAIN,\n",
        "#     min_gap=MIN_GAP_TRAIN,\n",
        "#     use_confidence_score=USE_CONF_SCORE,\n",
        "# )\n",
        "\n",
        "# test_entries_sell = harvest_entries(\n",
        "#     \"TEST\",\n",
        "#     tm_test_sell,\n",
        "#     topk_per_seg=TOPK_PER_SEG_TEST,\n",
        "#     min_gap=MIN_GAP_TEST,\n",
        "#     use_confidence_score=USE_CONF_SCORE,\n",
        "# )\n",
        "\n",
        "# -----------------------\n",
        "# SAVE ARTIFACTS (into out_dir)\n",
        "# -----------------------\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "train_entries_path = os.path.join(out_dir, \"entry_indices_train_sell.npy\")\n",
        "test_entries_path  = os.path.join(out_dir, \"entry_indices_test_sell.npy\")\n",
        "\n",
        "# NEW: save harvested entries (not trade entries)\n",
        "np.save(train_entries_path, train_entries)\n",
        "np.save(test_entries_path,  test_entries)\n",
        "\n",
        "train_trades_sell_json = os.path.join(out_dir, \"trades_buy_sell_train.json\")\n",
        "test_trades_sell_json  = os.path.join(out_dir, \"trades_buy_sell_test.json\")\n",
        "\n",
        "with open(train_trades_json, \"w\") as f:\n",
        "    json.dump(res_train[\"trades\"], f, indent=2)\n",
        "\n",
        "with open(test_trades_json, \"w\") as f:\n",
        "    json.dump(res_test[\"trades\"], f, indent=2)\n",
        "\n",
        "print(\"\\n=== SAVED ===\")\n",
        "print(\" -\", train_entries_path, \"(HARVESTED)\")\n",
        "print(\" -\", test_entries_path,  \"(HARVESTED)\")\n",
        "print(\" -\", train_trades_json)\n",
        "print(\" -\", test_trades_json)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50bd130b",
      "metadata": {},
      "source": [
        "## How to \"pass args\" in a notebook\n",
        "\n",
        "Instead of CLI args, edit the **Parameters** cell.\n",
        "\n",
        "If you really want args-style overrides, you can do:\n",
        "\n",
        "```python\n",
        "import os\n",
        "config_path = os.getenv(\"CFG\", config_path)\n",
        "features_npy = os.getenv(\"FEAT\", features_npy)\n",
        "prices_npy = os.getenv(\"PRICES\", prices_npy)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263d95cb",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3.10.14",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
